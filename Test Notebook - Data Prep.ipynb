{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528971ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import re]\n",
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9c3cb696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data\n",
    "train_data = pd.read_csv('WikiQA-train.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('WikiQA-test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad5982",
   "metadata": {},
   "source": [
    "Extract the unique questions from the train and test data frames, including the documentID and the DocumentTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "034c4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_documenttag(data):\n",
    "    qd = data[['Question', 'QuestionID', 'DocumentID','DocumentTitle']].drop_duplicates()\n",
    "    return qd\n",
    "train_question_doctag = get_questions_documenttag(train_data)\n",
    "test_question_doctag = get_questions_documenttag(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "368895a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique questions\n",
    "train_questions = train_question_doctag['Question']\n",
    "test_questions = test_question_doctag['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c3f8f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique document ids\n",
    "train_docid = train_question_doctag['DocumentID']\n",
    "test_docid = test_question_doctag['DocumentID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf4306",
   "metadata": {},
   "source": [
    "Extract the answers to those questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e2d6a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(data, questions, documentids): \n",
    "    answers = [] # list of answers\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q] # add the document id\n",
    "        df = data[data['Question'] == question]\n",
    "        index = df.loc[df['Label'] == 1]['Sentence'].index.values\n",
    "        if len(index) == 0: # if no answer found\n",
    "            answers.append([question, doc_id, 'No answer'])\n",
    "        else: # if 1 answer found\n",
    "            answers.append([question, doc_id, df.loc[index[0], \"Sentence\"]])\n",
    "    return answers\n",
    "\n",
    "train_answers = pd.DataFrame(get_answers(train_data, train_questions, train_docid))\n",
    "test_answers = pd.DataFrame(get_answers(test_data, test_questions, test_docid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e5527b",
   "metadata": {},
   "source": [
    "The above get_answers returns train_answers and test_answers which, gives us in the following columns\n",
    "- Question\n",
    "- Related Document ID\n",
    "- Answer (if no answer to that question, return no answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ac4567fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(data, questions, documentids): # (done by Finn, tweaked by Dan)\n",
    "    documents = []\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q] # add the document id\n",
    "        df = data[data['Question'] == question]\n",
    "        sentences = df['Sentence'].tolist()\n",
    "        for i in range(0, len(sentences) - 1):\n",
    "            sentences[i] = sentences[i] + ' '\n",
    "        documents.append([doc_id,''.join(sentences)])\n",
    "    return documents\n",
    "\n",
    "train_documents = pd.DataFrame(get_documents(train_data, train_questions, train_docid)) # return the individual document in list\n",
    "test_documents = pd.DataFrame(get_documents(test_data, test_questions, test_docid)) # return the individual document in list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa7704",
   "metadata": {},
   "source": [
    "The above train_documents and test_documents called from the get_documents gives us in the following columns\n",
    "- Document ID\n",
    "- Full Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0432bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming all the columns for more standardised access\n",
    "train_answers.columns = ['Question','DocumentID','Answer']\n",
    "test_answers.columns = ['Question','DocumentID','Answer']\n",
    "train_documents.columns = ['DocumentID','Document']\n",
    "test_documents.columns = ['DocumentID','Document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "763141d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2117, 2117, 630, 630)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result is 2117, 2117, 630, 630\n",
    "\n",
    "len(train_answers),len(train_documents), len(test_answers),len(test_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5edcdf6",
   "metadata": {},
   "source": [
    "**Prior to tagging, we should maybe clean the document and answers first:** (stopped here)\n",
    "\n",
    "Maybe? \n",
    "- lowercase (might lose context, but we can use on questions)\n",
    "- removing any punctuation or weird symbols (do)\n",
    "- removal of stop words? (probably not)\n",
    "\n",
    "Make sure that the pre-processing is standardised to be the same throughout doc and ans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c5fddce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lower(text):\n",
    "    # Lowercase the text for question, answer and documents\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "train_answers[['Question', 'Answer']] = train_answers[['Question', 'Answer']].applymap(preprocess_lower)\n",
    "train_documents['Document'] = train_documents['Document'].apply(preprocess_lower)\n",
    "test_answers[['Question', 'Answer']] = test_answers[['Question', 'Answer']].applymap(preprocess_lower)\n",
    "test_documents['Document'] = test_documents['Document'].apply(preprocess_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9785d51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>a partly submerged glacier cave on perito more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>in physics , circular motion is a movement of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D5</td>\n",
       "      <td>apollo creed is a fictional character from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D6</td>\n",
       "      <td>in the united states, the title of federal jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D7</td>\n",
       "      <td>the beretta 21a bobcat is a small pocket-sized...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>D2805</td>\n",
       "      <td>blue mountain state is an american comedy seri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>D2806</td>\n",
       "      <td>apple inc., formerly apple computer, inc., is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>D2807</td>\n",
       "      <td>section 8 housing in the south bronx section 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>D2808</td>\n",
       "      <td>restaurants categorized by type and informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>D2810</td>\n",
       "      <td>u.s. federal reserve notes in the mid-1990s th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2117 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DocumentID                                           Document\n",
       "0            D1  a partly submerged glacier cave on perito more...\n",
       "1            D2  in physics , circular motion is a movement of ...\n",
       "2            D5  apollo creed is a fictional character from the...\n",
       "3            D6  in the united states, the title of federal jud...\n",
       "4            D7  the beretta 21a bobcat is a small pocket-sized...\n",
       "...         ...                                                ...\n",
       "2112      D2805  blue mountain state is an american comedy seri...\n",
       "2113      D2806  apple inc., formerly apple computer, inc., is ...\n",
       "2114      D2807  section 8 housing in the south bronx section 8...\n",
       "2115      D2808  restaurants categorized by type and informatio...\n",
       "2116      D2810  u.s. federal reserve notes in the mid-1990s th...\n",
       "\n",
       "[2117 rows x 2 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3328fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelling(documents, answers):\n",
    "    tagged_documents = []\n",
    "    for q in range(len(answers)):\n",
    "        tagged_document = []\n",
    "        qn = answers['Question'].loc[q]\n",
    "        doc_id = answers['DocumentID'].loc[q]\n",
    "        content = documents.loc[documents['DocumentID'] == doc_id,'Document'].values[0]\n",
    "        answer = answers['Answer'].loc[q]\n",
    "\n",
    "        if answer == 'no answer':\n",
    "            tokens = word_tokenize(content)\n",
    "            for j in range(len(tokens)):\n",
    "                tagged_document.append('N') # none \n",
    "        else:\n",
    "            parts = content.partition(answer)\n",
    "            for j in range(len(parts)):\n",
    "                tokens = word_tokenize(parts[j])\n",
    "                if j == 1:\n",
    "                    tagged_document.append('S') # start of answer\n",
    "                    for k in range(len(tokens) - 2):\n",
    "                        tagged_document.append('I') # inside of answer\n",
    "                    tagged_document.append('E') # end of answer\n",
    "                else:\n",
    "                    for k in range(len(tokens)):\n",
    "                        tagged_document.append('N') # outside answer\n",
    "        tagged_documents.append(tagged_document)\n",
    "    return(tagged_documents)\n",
    "\n",
    "train_doc_ans_labels = labelling(train_documents, train_answers)\n",
    "test_doc_ans_labels = labelling(test_documents, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3fc6f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'the']\n",
      "['N', 'big']\n",
      "['N', 'ten']\n",
      "['N', 'conference']\n",
      "['N', ',']\n",
      "['N', 'formerly']\n",
      "['N', 'western']\n",
      "['N', 'conference']\n",
      "['N', 'and']\n",
      "['N', 'big']\n",
      "['N', 'nine']\n",
      "['N', 'conference']\n",
      "['N', ',']\n",
      "['N', 'is']\n",
      "['N', 'the']\n",
      "['N', 'oldest']\n",
      "['N', 'division']\n",
      "['N', 'i']\n",
      "['N', 'college']\n",
      "['N', 'athletic']\n",
      "['N', 'conference']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'united']\n",
      "['N', 'states']\n",
      "['N', '.']\n",
      "['S', 'its']\n",
      "['I', 'twelve']\n",
      "['I', 'member']\n",
      "['I', 'institutions']\n",
      "['I', '(']\n",
      "['I', 'which']\n",
      "['I', 'are']\n",
      "['I', 'primarily']\n",
      "['I', 'flagship']\n",
      "['I', 'research']\n",
      "['I', 'universities']\n",
      "['I', 'in']\n",
      "['I', 'their']\n",
      "['I', 'respective']\n",
      "['I', 'states']\n",
      "['I', ',']\n",
      "['I', 'well-regarded']\n",
      "['I', 'academically']\n",
      "['I', ',']\n",
      "['I', 'and']\n",
      "['I', 'with']\n",
      "['I', 'relatively']\n",
      "['I', 'large']\n",
      "['I', 'student']\n",
      "['I', 'enrollment']\n",
      "['I', ')']\n",
      "['I', 'are']\n",
      "['I', 'located']\n",
      "['I', 'primarily']\n",
      "['I', 'in']\n",
      "['I', 'the']\n",
      "['I', 'midwest']\n",
      "['I', ',']\n",
      "['I', 'stretching']\n",
      "['I', 'from']\n",
      "['I', 'nebraska']\n",
      "['I', 'in']\n",
      "['I', 'the']\n",
      "['I', 'west']\n",
      "['I', 'to']\n",
      "['I', 'penn']\n",
      "['I', 'state']\n",
      "['I', 'in']\n",
      "['I', 'the']\n",
      "['I', 'east']\n",
      "['E', '.']\n",
      "['N', 'the']\n",
      "['N', 'conference']\n",
      "['N', 'competes']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'ncaa']\n",
      "['N', \"'s\"]\n",
      "['N', 'division']\n",
      "['N', 'i']\n",
      "['N', ';']\n",
      "['N', 'its']\n",
      "['N', 'football']\n",
      "['N', 'teams']\n",
      "['N', 'compete']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'football']\n",
      "['N', 'bowl']\n",
      "['N', 'subdivision']\n",
      "['N', '(']\n",
      "['N', 'fbs']\n",
      "['N', ')']\n",
      "['N', ',']\n",
      "['N', 'formerly']\n",
      "['N', 'known']\n",
      "['N', 'as']\n",
      "['N', 'division']\n",
      "['N', 'i-a']\n",
      "['N', ',']\n",
      "['N', 'the']\n",
      "['N', 'highest']\n",
      "['N', 'level']\n",
      "['N', 'of']\n",
      "['N', 'ncaa']\n",
      "['N', 'competition']\n",
      "['N', 'in']\n",
      "['N', 'that']\n",
      "['N', 'sport']\n",
      "['N', '.']\n",
      "['N', 'member']\n",
      "['N', 'schools']\n",
      "['N', 'of']\n",
      "['N', 'the']\n",
      "['N', 'big']\n",
      "['N', 'ten']\n",
      "['N', '(']\n",
      "['N', 'or']\n",
      "['N', ',']\n",
      "['N', 'in']\n",
      "['N', 'two']\n",
      "['N', 'cases']\n",
      "['N', ',']\n",
      "['N', 'their']\n",
      "['N', 'parent']\n",
      "['N', 'university']\n",
      "['N', 'systems']\n",
      "['N', ')']\n",
      "['N', 'also']\n",
      "['N', 'are']\n",
      "['N', 'members']\n",
      "['N', 'of']\n",
      "['N', 'the']\n",
      "['N', 'committee']\n",
      "['N', 'on']\n",
      "['N', 'institutional']\n",
      "['N', 'cooperation']\n",
      "['N', ',']\n",
      "['N', 'a']\n",
      "['N', 'leading']\n",
      "['N', 'educational']\n",
      "['N', 'and']\n",
      "['N', 'research']\n",
      "['N', 'consortium']\n",
      "['N', '.']\n",
      "['N', 'despite']\n",
      "['N', 'the']\n",
      "['N', 'conference']\n",
      "['N', \"'s\"]\n",
      "['N', 'name']\n",
      "['N', ',']\n",
      "['N', 'the']\n",
      "['N', 'big']\n",
      "['N', 'ten']\n",
      "['N', 'actually']\n",
      "['N', 'consists']\n",
      "['N', 'of']\n",
      "['N', '12']\n",
      "['N', 'schools']\n",
      "['N', ',']\n",
      "['N', 'following']\n",
      "['N', 'the']\n",
      "['N', 'addition']\n",
      "['N', 'of']\n",
      "['N', 'the']\n",
      "['N', 'pennsylvania']\n",
      "['N', 'state']\n",
      "['N', 'university']\n",
      "['N', 'in']\n",
      "['N', '1993']\n",
      "['N', 'and']\n",
      "['N', 'the']\n",
      "['N', 'university']\n",
      "['N', 'of']\n",
      "['N', 'nebraska–lincoln']\n",
      "['N', 'in']\n",
      "['N', '2011.']\n",
      "['N', 'in']\n",
      "['N', '2014']\n",
      "['N', ',']\n",
      "['N', 'the']\n",
      "['N', 'conference']\n",
      "['N', 'will']\n",
      "['N', 'expand']\n",
      "['N', 'to']\n",
      "['N', '14']\n",
      "['N', 'members']\n",
      "['N', 'with']\n",
      "['N', 'the']\n",
      "['N', 'additions']\n",
      "['N', 'of']\n",
      "['N', 'the']\n",
      "['N', 'university']\n",
      "['N', 'of']\n",
      "['N', 'maryland']\n",
      "['N', ',']\n",
      "['N', 'college']\n",
      "['N', 'park']\n",
      "['N', 'and']\n",
      "['N', 'rutgers']\n",
      "['N', ',']\n",
      "['N', 'the']\n",
      "['N', 'state']\n",
      "['N', 'university']\n",
      "['N', 'of']\n",
      "['N', 'new']\n",
      "['N', 'jersey']\n",
      "['N', '.']\n",
      "['N', 'it']\n",
      "['N', 'is']\n",
      "['N', 'not']\n",
      "['N', 'to']\n",
      "['N', 'be']\n",
      "['N', 'confused']\n",
      "['N', 'with']\n",
      "['N', 'the']\n",
      "['N', 'big']\n",
      "['N', '12']\n",
      "['N', 'conference']\n",
      "['N', ',']\n",
      "['N', 'which']\n",
      "['N', 'has']\n",
      "['N', 'only']\n",
      "['N', 'ten']\n",
      "['N', 'schools']\n",
      "['N', 'and']\n",
      "['N', 'represents']\n",
      "['N', 'a']\n",
      "['N', 'different']\n",
      "['N', 'region']\n",
      "['N', 'of']\n",
      "['N', 'the']\n",
      "['N', 'country']\n",
      "['N', '.']\n",
      "its twelve member institutions (which are primarily flagship research universities in their respective states, well-regarded academically, and with relatively large student enrollment) are located primarily in the midwest , stretching from nebraska in the west to penn state in the east.\n"
     ]
    }
   ],
   "source": [
    "# check if tags are good\n",
    "def testing_tokens(ind, labels, documents, answers):\n",
    "    for i,j in zip(labels[ind],word_tokenize(documents['Document'][ind])):\n",
    "        print([i,j])\n",
    "    print(answers['Answer'][ind])\n",
    "testing_tokens(100 , train_doc_ans_labels, train_documents, train_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a294cdb",
   "metadata": {},
   "source": [
    "Cleaned Documents: train and test\n",
    "\n",
    "train_answers - contains the ['Question','DocumentID','Answer'] \n",
    "\n",
    "train_documents - contains the ['DocumentID','Document']\n",
    "\n",
    "train_doc_ans_labels - contains a list of list of answer tags for each document, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "83ed3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare the document for word embeddings:\n",
    "train_doc_ques = pd.DataFrame({'Document': train_documents['Document'],\n",
    "                               'Question': train_answers['Question']})\n",
    "test_doc_ques = pd.DataFrame({'Document': test_documents['Document'],\n",
    "                               'Question': test_answers['Question']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a01e5",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "To use the CBOW model, we need the data in sentences. Extract this from the original dataset, don't use sent_tokenise, will mess with some of the fullstops, we want to maintain structure from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "99ba3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokens(data):\n",
    "    sentence_list = []\n",
    "    for i in range(len(data)):\n",
    "        sentence_list.append(word_tokenize(data[i]))\n",
    "    return(sentence_list)\n",
    "train_doc_list = word_tokens(train_doc_ques['Document'])\n",
    "train_ques_list = word_tokens(train_doc_ques['Question'])\n",
    "test_doc_list = word_tokens(test_doc_ques['Document'])\n",
    "test_ques_list = word_tokens(test_doc_ques['Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1c491320",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = train_doc_list + train_ques_list + test_doc_list + test_ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acb5de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "wc_cbow_model = Word2Vec(sentences=combined_text, vector_size=100, window=5, min_count=1, workers=2, epochs=30)\n",
    "wc_cbow_model.save(\"cbow.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8cdd66",
   "metadata": {},
   "source": [
    "To implement QA\n",
    "\n",
    "1. Word Embeddings, using CBOW\n",
    "2. Feature Extraction 1 - POS tags\n",
    "3. Feature Extraction 2 - TF-IDF \n",
    "4. Feature Extraction 3 - NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ee6b4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(doc):\n",
    "    tokenized_doc = word_tokenize(doc)\n",
    "    embeddings = [wc_cbow_model.wv[word] for word in tokenized_doc]\n",
    "    return embeddings\n",
    "\n",
    "train_doc_ques['Doc_Embeddings'] = train_doc_ques['Document'].apply(get_word_embeddings)\n",
    "train_doc_ques['Q_Embeddings'] = train_doc_ques['Question'].apply(get_word_embeddings)\n",
    "test_doc_ques['Doc_Embeddings'] = test_doc_ques['Document'].apply(get_word_embeddings)\n",
    "test_doc_ques['Q_Embeddings'] = test_doc_ques['Question'].apply(get_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7da9a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_ques['Doc_Tokens'] = train_doc_ques['Document'].apply(word_tokenize)\n",
    "train_doc_ques['Q_Tokens'] =  train_doc_ques['Question'].apply(word_tokenize)\n",
    "test_doc_ques['Doc_Tokens'] = test_doc_ques['Document'].apply(word_tokenize)\n",
    "test_doc_ques['Q_Tokens'] = test_doc_ques['Question'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cdf6d45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_count(doc):\n",
    "    count = 0\n",
    "    for i in range(len(doc)):\n",
    "        if len(doc['Doc_Embeddings'][i]) != len(doc['Doc_Tokens'][i]):\n",
    "            count += 1\n",
    "        elif len(doc['Q_Embeddings'][i]) != len(doc['Q_Tokens'][i]):\n",
    "            count += 1\n",
    "        else:\n",
    "            continue\n",
    "    return(count)\n",
    "        \n",
    "check_count(train_doc_ques) # looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46810415",
   "metadata": {},
   "source": [
    "Note, need to convert the POS tags, NER tags into embeddings. After this, pad the questions and answers to the max question/document length in the combined training and test set.\n",
    "\n",
    "### PoS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ea21daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Apply the pos tags to the tokens \n",
    "from nltk.tag import pos_tag\n",
    "# download the dependency and resource as required\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "train_doc_ques['Doc_POS'] = train_doc_ques['Doc_Tokens'].apply(pos_tag)\n",
    "train_doc_ques['Q_POS'] =  train_doc_ques['Q_Tokens'].apply(pos_tag)\n",
    "test_doc_ques['Doc_POS'] = test_doc_ques['Doc_Tokens'].apply(pos_tag)\n",
    "test_doc_ques['Q_POS'] = test_doc_ques['Q_Tokens'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e62df735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how', 'WRB'),\n",
       " ('african', 'JJ'),\n",
       " ('americans', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('immigrated', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('us', 'PRP')]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the POS tags: # looks ok\n",
    "test_doc_ques['Q_POS'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cfdef90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#': 0,\n",
       " '$': 1,\n",
       " \"''\": 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " ',': 5,\n",
       " '.': 6,\n",
       " ':': 7,\n",
       " 'CC': 8,\n",
       " 'CD': 9,\n",
       " 'DT': 10,\n",
       " 'EX': 11,\n",
       " 'FW': 12,\n",
       " 'IN': 13,\n",
       " 'JJ': 14,\n",
       " 'JJR': 15,\n",
       " 'JJS': 16,\n",
       " 'MD': 17,\n",
       " 'NN': 18,\n",
       " 'NNP': 19,\n",
       " 'NNPS': 20,\n",
       " 'NNS': 21,\n",
       " 'PDT': 22,\n",
       " 'POS': 23,\n",
       " 'PRP': 24,\n",
       " 'PRP$': 25,\n",
       " 'RB': 26,\n",
       " 'RBR': 27,\n",
       " 'RBS': 28,\n",
       " 'RP': 29,\n",
       " 'SYM': 30,\n",
       " 'TO': 31,\n",
       " 'UH': 32,\n",
       " 'VB': 33,\n",
       " 'VBD': 34,\n",
       " 'VBG': 35,\n",
       " 'VBN': 36,\n",
       " 'VBP': 37,\n",
       " 'VBZ': 38,\n",
       " 'WDT': 39,\n",
       " 'WP': 40,\n",
       " 'WP$': 41,\n",
       " 'WRB': 42,\n",
       " '``': 43}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all unique POS Tags\n",
    "all_pos_tags = train_doc_ques['Doc_POS'].tolist() + test_doc_ques['Doc_POS'].tolist() + train_doc_ques['Q_POS'].tolist() + test_doc_ques['Q_POS'].tolist()\n",
    "\n",
    "def get_unique_pos(data):\n",
    "    pos_tags = set()\n",
    "    for item in data:\n",
    "        for _,pos_tag in item:\n",
    "            pos_tags.add(pos_tag)\n",
    "\n",
    "    pos_tag_index = {tag: i for i, tag in enumerate(sorted(pos_tags))}\n",
    "    return pos_tag_index\n",
    "\n",
    "pos_iden = get_unique_pos(all_pos_tags) # list of tags\n",
    "pos_iden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c06ede",
   "metadata": {},
   "source": [
    "### NER Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b9a90",
   "metadata": {},
   "source": [
    "### Steps to run this:\n",
    "\n",
    "- pip install spacy \n",
    "- python -m spacy download en_core_web_sm\n",
    "\n",
    "If loaded for the first time, restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "89121aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk using Spacy\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "# loading pre-trained model of NER\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ab395e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tagging(texts):\n",
    "    tagged_texts = []\n",
    "    for text in texts:\n",
    "        doc = spacy.tokens.Doc(nlp.vocab, words=text)\n",
    "        nlp.get_pipe(\"ner\")(doc)\n",
    "        tagged_texts.append([(token.text, token.ent_type_) for token in doc])\n",
    "    return tagged_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "946fb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will take a while...\n",
    "train_doc_ques['Doc_NER'] = ner_tagging(train_doc_ques['Doc_Tokens'])\n",
    "train_doc_ques['Q_NER'] = ner_tagging(train_doc_ques['Q_Tokens'])\n",
    "test_doc_ques['Doc_NER'] = ner_tagging(test_doc_ques['Doc_Tokens'])\n",
    "test_doc_ques['Q_NER'] = ner_tagging(test_doc_ques['Q_Tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "25cba08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'CARDINAL': 1,\n",
       " 'DATE': 2,\n",
       " 'EVENT': 3,\n",
       " 'FAC': 4,\n",
       " 'GPE': 5,\n",
       " 'LANGUAGE': 6,\n",
       " 'LAW': 7,\n",
       " 'LOC': 8,\n",
       " 'MONEY': 9,\n",
       " 'NORP': 10,\n",
       " 'ORDINAL': 11,\n",
       " 'ORG': 12,\n",
       " 'PERCENT': 13,\n",
       " 'PERSON': 14,\n",
       " 'PRODUCT': 15,\n",
       " 'QUANTITY': 16,\n",
       " 'TIME': 17,\n",
       " 'WORK_OF_ART': 18}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar approach to the POS\n",
    "\n",
    "# Extract all unique POS Tags\n",
    "all_ner_tags = train_doc_ques['Doc_NER'].tolist() + test_doc_ques['Doc_NER'].tolist() + train_doc_ques['Q_NER'].tolist() + test_doc_ques['Q_NER'].tolist()\n",
    "\n",
    "def get_unique_ner(data):\n",
    "    ner_tags = set()\n",
    "    for item in data:\n",
    "        for _,ner_tag in item:\n",
    "            ner_tags.add(ner_tag)\n",
    "\n",
    "    ner_tag_index = {tag: i for i, tag in enumerate(sorted(ner_tags))}\n",
    "    return ner_tag_index\n",
    "\n",
    "ner_iden = get_unique_pos(all_ner_tags) # list of tags\n",
    "ner_iden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0ffd8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopped here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
