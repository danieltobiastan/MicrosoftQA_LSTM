{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528971ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3cb696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data\n",
    "train_data = pd.read_csv(\"WikiQA-train.tsv\", sep=\"\\t\")\n",
    "test_data = pd.read_csv(\"WikiQA-test.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad5982",
   "metadata": {},
   "source": [
    "Extract the unique questions from the train and test data frames, including the documentID and the DocumentTitle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034c4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_documenttag(data):\n",
    "    qd = data[\n",
    "        [\"Question\", \"QuestionID\", \"DocumentID\", \"DocumentTitle\"]\n",
    "    ].drop_duplicates()\n",
    "    return qd\n",
    "\n",
    "\n",
    "train_question_doctag = get_questions_documenttag(train_data)\n",
    "test_question_doctag = get_questions_documenttag(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368895a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique questions\n",
    "train_questions = train_question_doctag[\"Question\"]\n",
    "test_questions = test_question_doctag[\"Question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f8f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique document ids\n",
    "train_docid = train_question_doctag[\"DocumentID\"]\n",
    "test_docid = test_question_doctag[\"DocumentID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf4306",
   "metadata": {},
   "source": [
    "Extract the answers to those questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2d6a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(data, questions, documentids):\n",
    "    answers = []  # list of answers\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q]  # add the document id\n",
    "        df = data[data[\"Question\"] == question]\n",
    "        index = df.loc[df[\"Label\"] == 1][\"Sentence\"].index.values\n",
    "        if len(index) == 0:  # if no answer found\n",
    "            answers.append([question, doc_id, \"No answer\"])\n",
    "        else:  # if 1 answer found\n",
    "            answers.append([question, doc_id, df.loc[index[0], \"Sentence\"]])\n",
    "    return answers\n",
    "\n",
    "\n",
    "train_answers = pd.DataFrame(get_answers(train_data, train_questions, train_docid))\n",
    "test_answers = pd.DataFrame(get_answers(test_data, test_questions, test_docid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e5527b",
   "metadata": {},
   "source": [
    "The above get_answers returns train_answers and test_answers which, gives us in the following columns\n",
    "\n",
    "-   Question\n",
    "-   Related Document ID\n",
    "-   Answer (if no answer to that question, return no answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4567fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(data, questions, documentids):  # (done by Finn, tweaked by Dan)\n",
    "    documents = []\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q]  # add the document id\n",
    "        df = data[data[\"Question\"] == question]\n",
    "        sentences = df[\"Sentence\"].tolist()\n",
    "        for i in range(0, len(sentences) - 1):\n",
    "            sentences[i] = sentences[i] + \" \"\n",
    "        documents.append([doc_id, \"\".join(sentences)])\n",
    "    return documents\n",
    "\n",
    "\n",
    "train_documents = pd.DataFrame(\n",
    "    get_documents(train_data, train_questions, train_docid)\n",
    ")  # return the individual document in list\n",
    "test_documents = pd.DataFrame(\n",
    "    get_documents(test_data, test_questions, test_docid)\n",
    ")  # return the individual document in list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa7704",
   "metadata": {},
   "source": [
    "The above train_documents and test_documents called from the get_documents gives us in the following columns\n",
    "\n",
    "-   Document ID\n",
    "-   Full Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0432bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming all the columns for more standardised access\n",
    "train_answers.columns = [\"Question\", \"DocumentID\", \"Answer\"]\n",
    "test_answers.columns = [\"Question\", \"DocumentID\", \"Answer\"]\n",
    "train_documents.columns = [\"DocumentID\", \"Document\"]\n",
    "test_documents.columns = [\"DocumentID\", \"Document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "763141d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2117, 2117, 630, 630)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result is 2117, 2117, 630, 630\n",
    "\n",
    "len(train_answers), len(train_documents), len(test_answers), len(test_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5edcdf6",
   "metadata": {},
   "source": [
    "**Prior to tagging, we should maybe clean the document and answers first:** (stopped here)\n",
    "\n",
    "Maybe?\n",
    "\n",
    "-   lowercase (might lose context, but we can use on questions)\n",
    "-   removing any punctuation or weird symbols (do)\n",
    "-   removal of stop words? (probably not)\n",
    "\n",
    "Make sure that the pre-processing is standardised to be the same throughout doc and ans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5fddce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lower(text):\n",
    "    # Lowercase the text for question, answer and documents\n",
    "    text = text.lower()\n",
    "    pattern = r\"[^a-zA-Z0-9\\s]\"\n",
    "    cleaned_text = re.sub(pattern, \" \", text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "train_answers[[\"Question\", \"Answer\"]] = train_answers[[\"Question\", \"Answer\"]].applymap(\n",
    "    preprocess_lower\n",
    ")\n",
    "train_documents[\"Document\"] = train_documents[\"Document\"].apply(preprocess_lower)\n",
    "test_answers[[\"Question\", \"Answer\"]] = test_answers[[\"Question\", \"Answer\"]].applymap(\n",
    "    preprocess_lower\n",
    ")\n",
    "test_documents[\"Document\"] = test_documents[\"Document\"].apply(preprocess_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9785d51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>a partly submerged glacier cave on perito more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>in physics   circular motion is a movement of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D5</td>\n",
       "      <td>apollo creed is a fictional character from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D6</td>\n",
       "      <td>in the united states  the title of federal jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D7</td>\n",
       "      <td>the beretta 21a bobcat is a small pocket sized...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>D2805</td>\n",
       "      <td>blue mountain state is an american comedy seri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>D2806</td>\n",
       "      <td>apple inc   formerly apple computer  inc   is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>D2807</td>\n",
       "      <td>section 8 housing in the south bronx section 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>D2808</td>\n",
       "      <td>restaurants categorized by type and informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>D2810</td>\n",
       "      <td>u s  federal reserve notes in the mid 1990s th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2117 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DocumentID                                           Document\n",
       "0            D1  a partly submerged glacier cave on perito more...\n",
       "1            D2  in physics   circular motion is a movement of ...\n",
       "2            D5  apollo creed is a fictional character from the...\n",
       "3            D6  in the united states  the title of federal jud...\n",
       "4            D7  the beretta 21a bobcat is a small pocket sized...\n",
       "...         ...                                                ...\n",
       "2112      D2805  blue mountain state is an american comedy seri...\n",
       "2113      D2806  apple inc   formerly apple computer  inc   is ...\n",
       "2114      D2807  section 8 housing in the south bronx section 8...\n",
       "2115      D2808  restaurants categorized by type and informatio...\n",
       "2116      D2810  u s  federal reserve notes in the mid 1990s th...\n",
       "\n",
       "[2117 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3328fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelling(documents, answers):\n",
    "    tagged_documents = []\n",
    "    for q in range(len(answers)):\n",
    "        tagged_document = []\n",
    "        qn = answers[\"Question\"].loc[q]\n",
    "        doc_id = answers[\"DocumentID\"].loc[q]\n",
    "        content = documents.loc[documents[\"DocumentID\"] == doc_id, \"Document\"].values[0]\n",
    "        answer = answers[\"Answer\"].loc[q]\n",
    "\n",
    "        if answer == \"no answer\":\n",
    "            tokens = word_tokenize(content)\n",
    "            for j in range(len(tokens)):\n",
    "                tagged_document.append(\"N\")  # none\n",
    "        else:\n",
    "            parts = content.partition(answer)\n",
    "            for j in range(len(parts)):\n",
    "                tokens = word_tokenize(parts[j])\n",
    "                if j == 1:\n",
    "                    tagged_document.append(\"S\")  # start of answer\n",
    "                    for k in range(len(tokens) - 2):\n",
    "                        tagged_document.append(\"I\")  # inside of answer\n",
    "                    tagged_document.append(\"E\")  # end of answer\n",
    "                else:\n",
    "                    for k in range(len(tokens)):\n",
    "                        tagged_document.append(\"N\")  # outside answer\n",
    "        tagged_documents.append(tagged_document)\n",
    "    return tagged_documents\n",
    "\n",
    "\n",
    "train_doc_ans_labels = labelling(train_documents, train_answers)\n",
    "test_doc_ans_labels = labelling(test_documents, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fc6f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if tags are good\n",
    "def testing_tokens(ind, labels, documents, answers):\n",
    "    for i, j in zip(labels[ind], word_tokenize(documents[\"Document\"][ind])):\n",
    "        print([i, j])\n",
    "    print(answers[\"Answer\"][ind])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a294cdb",
   "metadata": {},
   "source": [
    "Cleaned Documents: train and test\n",
    "\n",
    "train_answers - contains the ['Question','DocumentID','Answer']\n",
    "\n",
    "train_documents - contains the ['DocumentID','Document']\n",
    "\n",
    "train_doc_ans_labels - contains a list of list of answer tags for each document,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83ed3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare the document for word embeddings:\n",
    "train_doc_ques = pd.DataFrame(\n",
    "    {\"Document\": train_documents[\"Document\"], \"Question\": train_answers[\"Question\"]}\n",
    ")\n",
    "test_doc_ques = pd.DataFrame(\n",
    "    {\"Document\": test_documents[\"Document\"], \"Question\": test_answers[\"Question\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a01e5",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "To use the CBOW model, we need the data in sentences. Extract this from the original dataset, don't use sent_tokenise, will mess with some of the fullstops, we want to maintain structure from above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99ba3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokens(data):\n",
    "    sentence_list = []\n",
    "    for i in range(len(data)):\n",
    "        sentence_list.append(word_tokenize(data[i]))\n",
    "    return sentence_list\n",
    "\n",
    "\n",
    "train_doc_list = word_tokens(train_doc_ques[\"Document\"])\n",
    "train_ques_list = word_tokens(train_doc_ques[\"Question\"])\n",
    "test_doc_list = word_tokens(test_doc_ques[\"Document\"])\n",
    "test_ques_list = word_tokens(test_doc_ques[\"Question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c491320",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = train_doc_list + train_ques_list + test_doc_list + test_ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acb5de66",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bc/5zby_td12xvblch941x43zl40000gn/T/ipykernel_37020/856903076.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model trained, don't have to run this multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m wc_cbow_model = Word2Vec(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombined_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             self.train(\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[1;32m   1074\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0m\u001b[1;32m   1435\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_corpus_file_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model trained, don't have to run this multiple times\n",
    "wc_cbow_model = Word2Vec(\n",
    "    sentences=combined_text,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=2,\n",
    "    epochs=30,\n",
    ")\n",
    "wc_cbow_model.save(\"cbow.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8cdd66",
   "metadata": {},
   "source": [
    "To implement QA\n",
    "\n",
    "1. Word Embeddings, using CBOW\n",
    "2. Feature Extraction 1 - POS tags\n",
    "3. Feature Extraction 2 - TF-IDF\n",
    "4. Feature Extraction 3 - NER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae9d019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this if model in directory \n",
    "wc_cbow_model = Word2Vec.load(\"./cbow.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee6b4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(doc):\n",
    "    tokenized_doc = word_tokenize(doc)\n",
    "    embeddings = [wc_cbow_model.wv[word] for word in tokenized_doc]\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "train_doc_ques[\"Doc_Embeddings\"] = train_doc_ques[\"Document\"].apply(get_word_embeddings)\n",
    "train_doc_ques[\"Q_Embeddings\"] = train_doc_ques[\"Question\"].apply(get_word_embeddings)\n",
    "test_doc_ques[\"Doc_Embeddings\"] = test_doc_ques[\"Document\"].apply(get_word_embeddings)\n",
    "test_doc_ques[\"Q_Embeddings\"] = test_doc_ques[\"Question\"].apply(get_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7da9a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_ques[\"Doc_Tokens\"] = train_doc_ques[\"Document\"].apply(word_tokenize)\n",
    "train_doc_ques[\"Q_Tokens\"] = train_doc_ques[\"Question\"].apply(word_tokenize)\n",
    "test_doc_ques[\"Doc_Tokens\"] = test_doc_ques[\"Document\"].apply(word_tokenize)\n",
    "test_doc_ques[\"Q_Tokens\"] = test_doc_ques[\"Question\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdf6d45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_count(doc):\n",
    "    count = 0\n",
    "    for i in range(len(doc)):\n",
    "        if len(doc[\"Doc_Embeddings\"][i]) != len(doc[\"Doc_Tokens\"][i]):\n",
    "            count += 1\n",
    "        elif len(doc[\"Q_Embeddings\"][i]) != len(doc[\"Q_Tokens\"][i]):\n",
    "            count += 1\n",
    "        else:\n",
    "            continue\n",
    "    return count\n",
    "\n",
    "\n",
    "check_count(train_doc_ques)  # looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46810415",
   "metadata": {},
   "source": [
    "Note, need to convert the POS tags, NER tags into embeddings. After this, pad the questions and answers to the max question/document length in the combined training and test set.\n",
    "\n",
    "### PoS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea21daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Apply the pos tags to the tokens\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# download the dependency and resource as required\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "train_doc_ques[\"Doc_POS\"] = train_doc_ques[\"Doc_Tokens\"].apply(pos_tag)\n",
    "train_doc_ques[\"Q_POS\"] = train_doc_ques[\"Q_Tokens\"].apply(pos_tag)\n",
    "test_doc_ques[\"Doc_POS\"] = test_doc_ques[\"Doc_Tokens\"].apply(pos_tag)\n",
    "test_doc_ques[\"Q_POS\"] = test_doc_ques[\"Q_Tokens\"].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e62df735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how', 'WRB'),\n",
       " ('african', 'JJ'),\n",
       " ('americans', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('immigrated', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('us', 'PRP')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the POS tags: # looks ok\n",
    "test_doc_ques[\"Q_POS\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfdef90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$': 0,\n",
       " 'CC': 1,\n",
       " 'CD': 2,\n",
       " 'DT': 3,\n",
       " 'EX': 4,\n",
       " 'FW': 5,\n",
       " 'IN': 6,\n",
       " 'JJ': 7,\n",
       " 'JJR': 8,\n",
       " 'JJS': 9,\n",
       " 'MD': 10,\n",
       " 'NN': 11,\n",
       " 'NNP': 12,\n",
       " 'NNPS': 13,\n",
       " 'NNS': 14,\n",
       " 'PDT': 15,\n",
       " 'POS': 16,\n",
       " 'PRP': 17,\n",
       " 'PRP$': 18,\n",
       " 'RB': 19,\n",
       " 'RBR': 20,\n",
       " 'RBS': 21,\n",
       " 'RP': 22,\n",
       " 'SYM': 23,\n",
       " 'TO': 24,\n",
       " 'UH': 25,\n",
       " 'VB': 26,\n",
       " 'VBD': 27,\n",
       " 'VBG': 28,\n",
       " 'VBN': 29,\n",
       " 'VBP': 30,\n",
       " 'VBZ': 31,\n",
       " 'WDT': 32,\n",
       " 'WP': 33,\n",
       " 'WP$': 34,\n",
       " 'WRB': 35}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all unique POS Tags\n",
    "all_pos_tags = (\n",
    "    train_doc_ques[\"Doc_POS\"].tolist()\n",
    "    + test_doc_ques[\"Doc_POS\"].tolist()\n",
    "    + train_doc_ques[\"Q_POS\"].tolist()\n",
    "    + test_doc_ques[\"Q_POS\"].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "def get_unique_pos(data):\n",
    "    pos_tags = set()\n",
    "    for item in data:\n",
    "        for _, pos_tag in item:\n",
    "            pos_tags.add(pos_tag)\n",
    "\n",
    "    pos_tag_index = {tag: i for i, tag in enumerate(sorted(pos_tags))}\n",
    "    return pos_tag_index\n",
    "\n",
    "\n",
    "pos_iden = get_unique_pos(all_pos_tags)  # list of tags\n",
    "pos_iden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c06ede",
   "metadata": {},
   "source": [
    "### NER Tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b9a90",
   "metadata": {},
   "source": [
    "### Steps to run this:\n",
    "\n",
    "-   pip install spacy\n",
    "-   python -m spacy download en_core_web_sm\n",
    "\n",
    "If loaded for the first time, restart kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89121aaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (3.5.2)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.5.3-cp39-cp39-macosx_10_9_x86_64.whl (6.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.9 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: setuptools in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.22.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: spacy\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.5.2\n",
      "    Uninstalling spacy-3.5.2:\n",
      "      Successfully uninstalled spacy-3.5.2\n",
      "Successfully installed spacy-3.5.3\n",
      "2023-05-16 18:48:15.992279: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 229 kB/s eta 0:00:01    |████████████████▋               | 6.6 MB 3.5 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.26.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.0)\n",
      "Requirement already satisfied: jinja2 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.62.3)\n",
      "Requirement already satisfied: setuptools in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (58.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/daniel/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 18:48:53.847284: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# nltk using Spacy\n",
    "# pip install -U spacy\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "# loading pre-trained model of NER\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab395e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tagging(texts):\n",
    "    tagged_texts = []\n",
    "    for text in texts:\n",
    "        doc = spacy.tokens.Doc(nlp.vocab, words=text)\n",
    "        nlp.get_pipe(\"ner\")(doc)\n",
    "        tagged_texts.append([(token.text, token.ent_type_) for token in doc])\n",
    "    return tagged_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "946fb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will take a while...\n",
    "train_doc_ques[\"Doc_NER\"] = ner_tagging(train_doc_ques[\"Doc_Tokens\"])\n",
    "train_doc_ques[\"Q_NER\"] = ner_tagging(train_doc_ques[\"Q_Tokens\"])\n",
    "test_doc_ques[\"Doc_NER\"] = ner_tagging(test_doc_ques[\"Doc_Tokens\"])\n",
    "test_doc_ques[\"Q_NER\"] = ner_tagging(test_doc_ques[\"Q_Tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25cba08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'CARDINAL': 1,\n",
       " 'DATE': 2,\n",
       " 'EVENT': 3,\n",
       " 'FAC': 4,\n",
       " 'GPE': 5,\n",
       " 'LANGUAGE': 6,\n",
       " 'LAW': 7,\n",
       " 'LOC': 8,\n",
       " 'MONEY': 9,\n",
       " 'NORP': 10,\n",
       " 'ORDINAL': 11,\n",
       " 'ORG': 12,\n",
       " 'PERCENT': 13,\n",
       " 'PERSON': 14,\n",
       " 'PRODUCT': 15,\n",
       " 'QUANTITY': 16,\n",
       " 'TIME': 17,\n",
       " 'WORK_OF_ART': 18}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar approach to the POS\n",
    "\n",
    "# Extract all unique POS Tags\n",
    "all_ner_tags = (\n",
    "    train_doc_ques[\"Doc_NER\"].tolist()\n",
    "    + test_doc_ques[\"Doc_NER\"].tolist()\n",
    "    + train_doc_ques[\"Q_NER\"].tolist()\n",
    "    + test_doc_ques[\"Q_NER\"].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "def get_unique_ner(data):\n",
    "    ner_tags = set()\n",
    "    for item in data:\n",
    "        for _, ner_tag in item:\n",
    "            ner_tags.add(ner_tag)\n",
    "\n",
    "    ner_tag_index = {tag: i for i, tag in enumerate(sorted(ner_tags))}\n",
    "    return ner_tag_index\n",
    "\n",
    "\n",
    "ner_iden = get_unique_pos(all_ner_tags)  # list of tags\n",
    "ner_iden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d28ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ohv dims\n",
    "ner_idx = ner_iden.values()\n",
    "aa = np.eye(max(ner_idx) + 1)\n",
    "#aa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fafd8",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "First, calculate the document frequency of each token in the entire corpus (training documents + testing documents). The result is a dictionary where each token is a key and its value is the document frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85be9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_frequency(corpus):\n",
    "    \"\"\"\n",
    "    Computes the document frequency for every token in the corpus.\n",
    "    Returns a dictionary {token: doc_freq, ...}\n",
    "    \"\"\"\n",
    "    document_frequency = {}\n",
    "    for document in corpus:\n",
    "        for token in np.unique(document):\n",
    "            try:\n",
    "                document_frequency[token] += 1\n",
    "            except:\n",
    "                document_frequency[token] = 1\n",
    "    return document_frequency\n",
    "\n",
    "\n",
    "train_corpus = (\n",
    "    train_doc_ques[\"Doc_Tokens\"].tolist() + train_doc_ques[\"Q_Tokens\"].tolist()\n",
    ")\n",
    "test_corpus = test_doc_ques[\"Doc_Tokens\"].tolist() + test_doc_ques[\"Q_Tokens\"].tolist()\n",
    "train_doc_freq = document_frequency(train_corpus)\n",
    "test_doc_freq = document_frequency(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0adc55c",
   "metadata": {},
   "source": [
    "Now calculate TF-IDF using the document frequency from above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10f840c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "def compute_tf_idf(corpus, doc_frequency):\n",
    "    \"\"\"\n",
    "    Computes the term frequency inverse document frequency for every token in every document in the corpus.\n",
    "    Returns a list the same shape as the list of tokenized documents except every token is replaced with the tf-idf\n",
    "    for that token.\n",
    "    \"\"\"\n",
    "    tf_idf = {}\n",
    "    tf_idf_list = []\n",
    "    N = len(doc_frequency)\n",
    "    doc_id = 0\n",
    "    for document in corpus:\n",
    "        tf_idf_doc = []\n",
    "        counter = Counter(document)\n",
    "        total_num_words = len(document)\n",
    "        for token in np.unique(document):\n",
    "            tf = counter[token] / total_num_words\n",
    "            df = doc_frequency[token]\n",
    "            idf = math.log(N / (df + 1)) + 1\n",
    "            tf_idf[doc_id, token] = tf * idf\n",
    "        for token in document:\n",
    "            tf_idf_doc.append(tf_idf[doc_id, token])\n",
    "        tf_idf_list.append(tf_idf_doc)\n",
    "        doc_id += 1\n",
    "    return tf_idf_list\n",
    "\n",
    "\n",
    "train_doc_ques[\"Doc_TFIDF\"] = compute_tf_idf(\n",
    "    train_doc_ques[\"Doc_Tokens\"].tolist(), train_doc_freq\n",
    ")\n",
    "train_doc_ques[\"Q_TFIDF\"] = compute_tf_idf(\n",
    "    train_doc_ques[\"Q_Tokens\"].tolist(), train_doc_freq\n",
    ")\n",
    "test_doc_ques[\"Doc_TFIDF\"] = compute_tf_idf(\n",
    "    test_doc_ques[\"Doc_Tokens\"].tolist(), test_doc_freq\n",
    ")\n",
    "test_doc_ques[\"Q_TFIDF\"] = compute_tf_idf(\n",
    "    test_doc_ques[\"Q_Tokens\"].tolist(), test_doc_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8565260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Question</th>\n",
       "      <th>Doc_Embeddings</th>\n",
       "      <th>Q_Embeddings</th>\n",
       "      <th>Doc_Tokens</th>\n",
       "      <th>Q_Tokens</th>\n",
       "      <th>Doc_POS</th>\n",
       "      <th>Q_POS</th>\n",
       "      <th>Doc_NER</th>\n",
       "      <th>Q_NER</th>\n",
       "      <th>Doc_TFIDF</th>\n",
       "      <th>Q_TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>african immigration to the united states refer...</td>\n",
       "      <td>how african americans were immigrated to the us</td>\n",
       "      <td>[[0.88913774, -0.053637512, -0.34699965, 0.234...</td>\n",
       "      <td>[[0.25111866, -0.7270332, -0.56952626, 1.05881...</td>\n",
       "      <td>[african, immigration, to, the, united, states...</td>\n",
       "      <td>[how, african, americans, were, immigrated, to...</td>\n",
       "      <td>[(african, JJ), (immigration, NN), (to, TO), (...</td>\n",
       "      <td>[(how, WRB), (african, JJ), (americans, NNS), ...</td>\n",
       "      <td>[(african, ORG), (immigration, ORG), (to, ), (...</td>\n",
       "      <td>[(how, ), (african, NORP), (americans, NORP), ...</td>\n",
       "      <td>[0.2444438957631187, 0.16900818614783913, 0.28...</td>\n",
       "      <td>[0.7085526392283996, 0.934997901293929, 0.9907...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a prison  from old french prisoun   also known...</td>\n",
       "      <td>how large were early jails</td>\n",
       "      <td>[[0.18600275, -0.2672038, -2.469172, 1.7943542...</td>\n",
       "      <td>[[0.25111866, -0.7270332, -0.56952626, 1.05881...</td>\n",
       "      <td>[a, prison, from, old, french, prisoun, also, ...</td>\n",
       "      <td>[how, large, were, early, jails]</td>\n",
       "      <td>[(a, DT), (prison, NN), (from, IN), (old, JJ),...</td>\n",
       "      <td>[(how, WRB), (large, JJ), (were, VBD), (early,...</td>\n",
       "      <td>[(a, ), (prison, ), (from, ), (old, ), (french...</td>\n",
       "      <td>[(how, ), (large, ), (were, ), (early, ), (jai...</td>\n",
       "      <td>[0.23736000397378015, 0.15055751080418858, 0.0...</td>\n",
       "      <td>[1.1336842227654393, 1.2988332831657334, 1.145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a small  electrically powered pump a large  el...</td>\n",
       "      <td>how a water pump works</td>\n",
       "      <td>[[0.18600275, -0.2672038, -2.469172, 1.7943542...</td>\n",
       "      <td>[[0.25111866, -0.7270332, -0.56952626, 1.05881...</td>\n",
       "      <td>[a, small, electrically, powered, pump, a, lar...</td>\n",
       "      <td>[how, a, water, pump, works]</td>\n",
       "      <td>[(a, DT), (small, JJ), (electrically, RB), (po...</td>\n",
       "      <td>[(how, WRB), (a, DT), (water, NN), (pump, NN),...</td>\n",
       "      <td>[(a, ), (small, ), (electrically, ), (powered,...</td>\n",
       "      <td>[(how, ), (a, ), (water, ), (pump, ), (works, )]</td>\n",
       "      <td>[0.18179077227423296, 0.07355471089668812, 0.1...</td>\n",
       "      <td>[1.1336842227654393, 0.8362375524614717, 1.412...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lolita is a 1962 comedy drama film by stanley ...</td>\n",
       "      <td>how old was sue lyon when she made lolita</td>\n",
       "      <td>[[-0.24463412, 0.06878885, 0.363463, 0.1364551...</td>\n",
       "      <td>[[0.25111866, -0.7270332, -0.56952626, 1.05881...</td>\n",
       "      <td>[lolita, is, a, 1962, comedy, drama, film, by,...</td>\n",
       "      <td>[how, old, was, sue, lyon, when, she, made, lo...</td>\n",
       "      <td>[(lolita, NN), (is, VBZ), (a, DT), (1962, CD),...</td>\n",
       "      <td>[(how, WRB), (old, JJ), (was, VBD), (sue, NN),...</td>\n",
       "      <td>[(lolita, ), (is, ), (a, ), (1962, DATE), (com...</td>\n",
       "      <td>[(how, ), (old, ), (was, ), (sue, PERSON), (ly...</td>\n",
       "      <td>[0.22677748220200447, 0.031618095192546124, 0....</td>\n",
       "      <td>[0.6298245682030218, 0.7657996161610346, 0.528...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>each antibody binds to a specific antigen   an...</td>\n",
       "      <td>how are antibodies used in</td>\n",
       "      <td>[[1.2020115, 2.6943376, -5.0276637, 3.097347, ...</td>\n",
       "      <td>[[0.25111866, -0.7270332, -0.56952626, 1.05881...</td>\n",
       "      <td>[each, antibody, binds, to, a, specific, antig...</td>\n",
       "      <td>[how, are, antibodies, used, in]</td>\n",
       "      <td>[(each, DT), (antibody, NN), (binds, VBZ), (to...</td>\n",
       "      <td>[(how, WRB), (are, VBP), (antibodies, NNS), (u...</td>\n",
       "      <td>[(each, ), (antibody, ), (binds, ), (to, ), (a...</td>\n",
       "      <td>[(how, ), (are, ), (antibodies, ), (used, ), (...</td>\n",
       "      <td>[0.05405618374532114, 0.2406872267443911, 0.01...</td>\n",
       "      <td>[1.1336842227654393, 0.9670455719243073, 1.862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>american cuts of beef including the brisket br...</td>\n",
       "      <td>where is the brisket from</td>\n",
       "      <td>[[-1.6133411, 0.7031535, 0.6441129, -0.7412261...</td>\n",
       "      <td>[[0.20251973, -2.1292784, -0.22558218, 2.18635...</td>\n",
       "      <td>[american, cuts, of, beef, including, the, bri...</td>\n",
       "      <td>[where, is, the, brisket, from]</td>\n",
       "      <td>[(american, JJ), (cuts, NNS), (of, IN), (beef,...</td>\n",
       "      <td>[(where, WRB), (is, VBZ), (the, DT), (brisket,...</td>\n",
       "      <td>[(american, NORP), (cuts, ), (of, ), (beef, ),...</td>\n",
       "      <td>[(where, ), (is, ), (the, ), (brisket, ), (fro...</td>\n",
       "      <td>[0.044673596326364536, 0.15610157311465894, 0....</td>\n",
       "      <td>[1.1856348821507936, 0.8030996178906715, 0.792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>the arm architecture describes a family of ris...</td>\n",
       "      <td>what is arm chipset</td>\n",
       "      <td>[[-0.024290355, -0.34677127, -0.9102898, 0.579...</td>\n",
       "      <td>[[1.6417806, -0.42128003, -2.0681963, -3.08729...</td>\n",
       "      <td>[the, arm, architecture, describes, a, family,...</td>\n",
       "      <td>[what, is, arm, chipset]</td>\n",
       "      <td>[(the, DT), (arm, NN), (architecture, NN), (de...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (arm, JJ), (chipset, NN)]</td>\n",
       "      <td>[(the, ), (arm, ), (architecture, ), (describe...</td>\n",
       "      <td>[(what, ), (is, ), (arm, ), (chipset, )]</td>\n",
       "      <td>[0.13240256299185374, 0.2925150013371109, 0.06...</td>\n",
       "      <td>[1.200426291895723, 1.0038745223633394, 2.1882...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>june bug or junebug may refer to  beetles  phy...</td>\n",
       "      <td>what is the life span of june bugs</td>\n",
       "      <td>[[-3.2918558, 0.07746171, 2.024428, 1.5457352,...</td>\n",
       "      <td>[[1.6417806, -0.42128003, -2.0681963, -3.08729...</td>\n",
       "      <td>[june, bug, or, junebug, may, refer, to, beetl...</td>\n",
       "      <td>[what, is, the, life, span, of, june, bugs]</td>\n",
       "      <td>[(june, NN), (bug, NN), (or, CC), (junebug, NN...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (the, DT), (life, NN),...</td>\n",
       "      <td>[(june, DATE), (bug, ), (or, ), (junebug, ), (...</td>\n",
       "      <td>[(what, ), (is, ), (the, ), (life, ), (span, )...</td>\n",
       "      <td>[0.3625851507468464, 0.345024546712403, 0.0560...</td>\n",
       "      <td>[0.6002131459478615, 0.5019372611816697, 0.495...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>this is a list of known biological mothers und...</td>\n",
       "      <td>who is the youngest female to give birth world...</td>\n",
       "      <td>[[-1.4733256, 1.7044597, -5.0576773, 2.08202, ...</td>\n",
       "      <td>[[-0.20219268, 0.45227763, -1.5316664, -0.1189...</td>\n",
       "      <td>[this, is, a, list, of, known, biological, mot...</td>\n",
       "      <td>[who, is, the, youngest, female, to, give, bir...</td>\n",
       "      <td>[(this, DT), (is, VBZ), (a, DT), (list, NN), (...</td>\n",
       "      <td>[(who, WP), (is, VBZ), (the, DT), (youngest, J...</td>\n",
       "      <td>[(this, ), (is, ), (a, ), (list, ), (of, ), (k...</td>\n",
       "      <td>[(who, ), (is, ), (the, ), (youngest, ), (fema...</td>\n",
       "      <td>[0.4279277383207209, 0.3088844684194891, 0.321...</td>\n",
       "      <td>[0.532822100709197, 0.40154980894533576, 0.396...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>a broodmare and foal a mare is an adult female...</td>\n",
       "      <td>what is an open mare</td>\n",
       "      <td>[[0.18600275, -0.2672038, -2.469172, 1.7943542...</td>\n",
       "      <td>[[1.6417806, -0.42128003, -2.0681963, -3.08729...</td>\n",
       "      <td>[a, broodmare, and, foal, a, mare, is, an, adu...</td>\n",
       "      <td>[what, is, an, open, mare]</td>\n",
       "      <td>[(a, DT), (broodmare, NN), (and, CC), (foal, V...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (an, DT), (open, JJ), ...</td>\n",
       "      <td>[(a, ), (broodmare, ), (and, ), (foal, ), (a, ...</td>\n",
       "      <td>[(what, ), (is, ), (an, ), (open, ), (mare, )]</td>\n",
       "      <td>[0.555313999681446, 0.1563392477290576, 0.1335...</td>\n",
       "      <td>[0.9603410335165784, 0.8030996178906715, 0.964...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Document  \\\n",
       "0    african immigration to the united states refer...   \n",
       "1    a prison  from old french prisoun   also known...   \n",
       "2    a small  electrically powered pump a large  el...   \n",
       "3    lolita is a 1962 comedy drama film by stanley ...   \n",
       "4    each antibody binds to a specific antigen   an...   \n",
       "..                                                 ...   \n",
       "625  american cuts of beef including the brisket br...   \n",
       "626  the arm architecture describes a family of ris...   \n",
       "627  june bug or junebug may refer to  beetles  phy...   \n",
       "628  this is a list of known biological mothers und...   \n",
       "629  a broodmare and foal a mare is an adult female...   \n",
       "\n",
       "                                              Question  \\\n",
       "0      how african americans were immigrated to the us   \n",
       "1                           how large were early jails   \n",
       "2                               how a water pump works   \n",
       "3            how old was sue lyon when she made lolita   \n",
       "4                           how are antibodies used in   \n",
       "..                                                 ...   \n",
       "625                          where is the brisket from   \n",
       "626                                what is arm chipset   \n",
       "627                 what is the life span of june bugs   \n",
       "628  who is the youngest female to give birth world...   \n",
       "629                              what is an open mare    \n",
       "\n",
       "                                        Doc_Embeddings  \\\n",
       "0    [[0.88913774, -0.053637512, -0.34699965, 0.234...   \n",
       "1    [[0.18600275, -0.2672038, -2.469172, 1.7943542...   \n",
       "2    [[0.18600275, -0.2672038, -2.469172, 1.7943542...   \n",
       "3    [[-0.24463412, 0.06878885, 0.363463, 0.1364551...   \n",
       "4    [[1.2020115, 2.6943376, -5.0276637, 3.097347, ...   \n",
       "..                                                 ...   \n",
       "625  [[-1.6133411, 0.7031535, 0.6441129, -0.7412261...   \n",
       "626  [[-0.024290355, -0.34677127, -0.9102898, 0.579...   \n",
       "627  [[-3.2918558, 0.07746171, 2.024428, 1.5457352,...   \n",
       "628  [[-1.4733256, 1.7044597, -5.0576773, 2.08202, ...   \n",
       "629  [[0.18600275, -0.2672038, -2.469172, 1.7943542...   \n",
       "\n",
       "                                          Q_Embeddings  \\\n",
       "0    [[0.25111866, -0.7270332, -0.56952626, 1.05881...   \n",
       "1    [[0.25111866, -0.7270332, -0.56952626, 1.05881...   \n",
       "2    [[0.25111866, -0.7270332, -0.56952626, 1.05881...   \n",
       "3    [[0.25111866, -0.7270332, -0.56952626, 1.05881...   \n",
       "4    [[0.25111866, -0.7270332, -0.56952626, 1.05881...   \n",
       "..                                                 ...   \n",
       "625  [[0.20251973, -2.1292784, -0.22558218, 2.18635...   \n",
       "626  [[1.6417806, -0.42128003, -2.0681963, -3.08729...   \n",
       "627  [[1.6417806, -0.42128003, -2.0681963, -3.08729...   \n",
       "628  [[-0.20219268, 0.45227763, -1.5316664, -0.1189...   \n",
       "629  [[1.6417806, -0.42128003, -2.0681963, -3.08729...   \n",
       "\n",
       "                                            Doc_Tokens  \\\n",
       "0    [african, immigration, to, the, united, states...   \n",
       "1    [a, prison, from, old, french, prisoun, also, ...   \n",
       "2    [a, small, electrically, powered, pump, a, lar...   \n",
       "3    [lolita, is, a, 1962, comedy, drama, film, by,...   \n",
       "4    [each, antibody, binds, to, a, specific, antig...   \n",
       "..                                                 ...   \n",
       "625  [american, cuts, of, beef, including, the, bri...   \n",
       "626  [the, arm, architecture, describes, a, family,...   \n",
       "627  [june, bug, or, junebug, may, refer, to, beetl...   \n",
       "628  [this, is, a, list, of, known, biological, mot...   \n",
       "629  [a, broodmare, and, foal, a, mare, is, an, adu...   \n",
       "\n",
       "                                              Q_Tokens  \\\n",
       "0    [how, african, americans, were, immigrated, to...   \n",
       "1                     [how, large, were, early, jails]   \n",
       "2                         [how, a, water, pump, works]   \n",
       "3    [how, old, was, sue, lyon, when, she, made, lo...   \n",
       "4                     [how, are, antibodies, used, in]   \n",
       "..                                                 ...   \n",
       "625                    [where, is, the, brisket, from]   \n",
       "626                           [what, is, arm, chipset]   \n",
       "627        [what, is, the, life, span, of, june, bugs]   \n",
       "628  [who, is, the, youngest, female, to, give, bir...   \n",
       "629                         [what, is, an, open, mare]   \n",
       "\n",
       "                                               Doc_POS  \\\n",
       "0    [(african, JJ), (immigration, NN), (to, TO), (...   \n",
       "1    [(a, DT), (prison, NN), (from, IN), (old, JJ),...   \n",
       "2    [(a, DT), (small, JJ), (electrically, RB), (po...   \n",
       "3    [(lolita, NN), (is, VBZ), (a, DT), (1962, CD),...   \n",
       "4    [(each, DT), (antibody, NN), (binds, VBZ), (to...   \n",
       "..                                                 ...   \n",
       "625  [(american, JJ), (cuts, NNS), (of, IN), (beef,...   \n",
       "626  [(the, DT), (arm, NN), (architecture, NN), (de...   \n",
       "627  [(june, NN), (bug, NN), (or, CC), (junebug, NN...   \n",
       "628  [(this, DT), (is, VBZ), (a, DT), (list, NN), (...   \n",
       "629  [(a, DT), (broodmare, NN), (and, CC), (foal, V...   \n",
       "\n",
       "                                                 Q_POS  \\\n",
       "0    [(how, WRB), (african, JJ), (americans, NNS), ...   \n",
       "1    [(how, WRB), (large, JJ), (were, VBD), (early,...   \n",
       "2    [(how, WRB), (a, DT), (water, NN), (pump, NN),...   \n",
       "3    [(how, WRB), (old, JJ), (was, VBD), (sue, NN),...   \n",
       "4    [(how, WRB), (are, VBP), (antibodies, NNS), (u...   \n",
       "..                                                 ...   \n",
       "625  [(where, WRB), (is, VBZ), (the, DT), (brisket,...   \n",
       "626  [(what, WP), (is, VBZ), (arm, JJ), (chipset, NN)]   \n",
       "627  [(what, WP), (is, VBZ), (the, DT), (life, NN),...   \n",
       "628  [(who, WP), (is, VBZ), (the, DT), (youngest, J...   \n",
       "629  [(what, WP), (is, VBZ), (an, DT), (open, JJ), ...   \n",
       "\n",
       "                                               Doc_NER  \\\n",
       "0    [(african, ORG), (immigration, ORG), (to, ), (...   \n",
       "1    [(a, ), (prison, ), (from, ), (old, ), (french...   \n",
       "2    [(a, ), (small, ), (electrically, ), (powered,...   \n",
       "3    [(lolita, ), (is, ), (a, ), (1962, DATE), (com...   \n",
       "4    [(each, ), (antibody, ), (binds, ), (to, ), (a...   \n",
       "..                                                 ...   \n",
       "625  [(american, NORP), (cuts, ), (of, ), (beef, ),...   \n",
       "626  [(the, ), (arm, ), (architecture, ), (describe...   \n",
       "627  [(june, DATE), (bug, ), (or, ), (junebug, ), (...   \n",
       "628  [(this, ), (is, ), (a, ), (list, ), (of, ), (k...   \n",
       "629  [(a, ), (broodmare, ), (and, ), (foal, ), (a, ...   \n",
       "\n",
       "                                                 Q_NER  \\\n",
       "0    [(how, ), (african, NORP), (americans, NORP), ...   \n",
       "1    [(how, ), (large, ), (were, ), (early, ), (jai...   \n",
       "2     [(how, ), (a, ), (water, ), (pump, ), (works, )]   \n",
       "3    [(how, ), (old, ), (was, ), (sue, PERSON), (ly...   \n",
       "4    [(how, ), (are, ), (antibodies, ), (used, ), (...   \n",
       "..                                                 ...   \n",
       "625  [(where, ), (is, ), (the, ), (brisket, ), (fro...   \n",
       "626           [(what, ), (is, ), (arm, ), (chipset, )]   \n",
       "627  [(what, ), (is, ), (the, ), (life, ), (span, )...   \n",
       "628  [(who, ), (is, ), (the, ), (youngest, ), (fema...   \n",
       "629     [(what, ), (is, ), (an, ), (open, ), (mare, )]   \n",
       "\n",
       "                                             Doc_TFIDF  \\\n",
       "0    [0.2444438957631187, 0.16900818614783913, 0.28...   \n",
       "1    [0.23736000397378015, 0.15055751080418858, 0.0...   \n",
       "2    [0.18179077227423296, 0.07355471089668812, 0.1...   \n",
       "3    [0.22677748220200447, 0.031618095192546124, 0....   \n",
       "4    [0.05405618374532114, 0.2406872267443911, 0.01...   \n",
       "..                                                 ...   \n",
       "625  [0.044673596326364536, 0.15610157311465894, 0....   \n",
       "626  [0.13240256299185374, 0.2925150013371109, 0.06...   \n",
       "627  [0.3625851507468464, 0.345024546712403, 0.0560...   \n",
       "628  [0.4279277383207209, 0.3088844684194891, 0.321...   \n",
       "629  [0.555313999681446, 0.1563392477290576, 0.1335...   \n",
       "\n",
       "                                               Q_TFIDF  \n",
       "0    [0.7085526392283996, 0.934997901293929, 0.9907...  \n",
       "1    [1.1336842227654393, 1.2988332831657334, 1.145...  \n",
       "2    [1.1336842227654393, 0.8362375524614717, 1.412...  \n",
       "3    [0.6298245682030218, 0.7657996161610346, 0.528...  \n",
       "4    [1.1336842227654393, 0.9670455719243073, 1.862...  \n",
       "..                                                 ...  \n",
       "625  [1.1856348821507936, 0.8030996178906715, 0.792...  \n",
       "626  [1.200426291895723, 1.0038745223633394, 2.1882...  \n",
       "627  [0.6002131459478615, 0.5019372611816697, 0.495...  \n",
       "628  [0.532822100709197, 0.40154980894533576, 0.396...  \n",
       "629  [0.9603410335165784, 0.8030996178906715, 0.964...  \n",
       "\n",
       "[630 rows x 12 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc_ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d807d2d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Question</th>\n",
       "      <th>Doc_Embeddings</th>\n",
       "      <th>Q_Embeddings</th>\n",
       "      <th>Doc_Tokens</th>\n",
       "      <th>Q_Tokens</th>\n",
       "      <th>Doc_POS</th>\n",
       "      <th>Q_POS</th>\n",
       "      <th>Doc_NER</th>\n",
       "      <th>Q_NER</th>\n",
       "      <th>Doc_TFIDF</th>\n",
       "      <th>Q_TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a partly submerged glacier cave on perito more...</td>\n",
       "      <td>how are glacier caves formed</td>\n",
       "      <td>[[0.18600275, -0.2672038, -2.469172, 1.7943542...</td>\n",
       "      <td>[[0.25111866, -0.7270332, -0.56952626, 1.05881...</td>\n",
       "      <td>[a, partly, submerged, glacier, cave, on, peri...</td>\n",
       "      <td>[how, are, glacier, caves, formed]</td>\n",
       "      <td>[(a, DT), (partly, RB), (submerged, VBN), (gla...</td>\n",
       "      <td>[(how, WRB), (are, VBP), (glacier, JJ), (caves...</td>\n",
       "      <td>[(a, ), (partly, ), (submerged, ), (glacier, )...</td>\n",
       "      <td>[(how, ), (are, ), (glacier, ), (caves, ), (fo...</td>\n",
       "      <td>[0.24679288919367473, 0.145118630440616, 0.161...</td>\n",
       "      <td>[1.0342546228402965, 0.8500722992276416, 1.989...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in physics   circular motion is a movement of ...</td>\n",
       "      <td>how are the directions of the velocity and for...</td>\n",
       "      <td>[[-1.5330905, 0.4225784, -0.2228741, 0.4760204...</td>\n",
       "      <td>[[0.25111866, -0.7270332, -0.56952626, 1.05881...</td>\n",
       "      <td>[in, physics, circular, motion, is, a, movemen...</td>\n",
       "      <td>[how, are, the, directions, of, the, velocity,...</td>\n",
       "      <td>[(in, IN), (physics, NNS), (circular, JJ), (mo...</td>\n",
       "      <td>[(how, WRB), (are, VBP), (the, DT), (direction...</td>\n",
       "      <td>[(in, ), (physics, ), (circular, ), (motion, )...</td>\n",
       "      <td>[(how, ), (are, ), (the, ), (directions, ), (o...</td>\n",
       "      <td>[0.10158600494541081, 0.047978500911022494, 0....</td>\n",
       "      <td>[0.34475154094676547, 0.2833574330758805, 0.45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apollo creed is a fictional character from the...</td>\n",
       "      <td>how did apollo creed die</td>\n",
       "      <td>[[-0.27247757, 0.75547856, 0.87444097, -0.0673...</td>\n",
       "      <td>[[0.25111866, -0.7270332, -0.56952626, 1.05881...</td>\n",
       "      <td>[apollo, creed, is, a, fictional, character, f...</td>\n",
       "      <td>[how, did, apollo, creed, die]</td>\n",
       "      <td>[(apollo, NNS), (creed, VBP), (is, VBZ), (a, D...</td>\n",
       "      <td>[(how, WRB), (did, VBD), (apollo, VB), (creed,...</td>\n",
       "      <td>[(apollo, ORG), (creed, ), (is, ), (a, ), (fic...</td>\n",
       "      <td>[(how, ), (did, ), (apollo, ORG), (creed, ), (...</td>\n",
       "      <td>[0.1506184153725464, 0.2875274053249442, 0.057...</td>\n",
       "      <td>[1.0342546228402965, 1.1571023666681808, 1.787...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in the united states  the title of federal jud...</td>\n",
       "      <td>how long is the term for federal judges</td>\n",
       "      <td>[[-1.5330905, 0.4225784, -0.2228741, 0.4760204...</td>\n",
       "      <td>[[0.25111866, -0.7270332, -0.56952626, 1.05881...</td>\n",
       "      <td>[in, the, united, states, the, title, of, fede...</td>\n",
       "      <td>[how, long, is, the, term, for, federal, judges]</td>\n",
       "      <td>[(in, IN), (the, DT), (united, JJ), (states, V...</td>\n",
       "      <td>[(how, WRB), (long, JJ), (is, VBZ), (the, DT),...</td>\n",
       "      <td>[(in, ), (the, GPE), (united, GPE), (states, G...</td>\n",
       "      <td>[(how, ), (long, ), (is, ), (the, ), (term, ),...</td>\n",
       "      <td>[0.08829203392786322, 0.3204713483897852, 0.16...</td>\n",
       "      <td>[0.6464091392751853, 0.7778250162349475, 0.429...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the beretta 21a bobcat is a small pocket sized...</td>\n",
       "      <td>how a beretta model 21 pistols magazines works</td>\n",
       "      <td>[[-0.024290355, -0.34677127, -0.9102898, 0.579...</td>\n",
       "      <td>[[0.25111866, -0.7270332, -0.56952626, 1.05881...</td>\n",
       "      <td>[the, beretta, 21a, bobcat, is, a, small, pock...</td>\n",
       "      <td>[how, a, beretta, model, 21, pistols, magazine...</td>\n",
       "      <td>[(the, DT), (beretta, NN), (21a, CD), (bobcat,...</td>\n",
       "      <td>[(how, WRB), (a, DT), (beretta, NN), (model, N...</td>\n",
       "      <td>[(the, ), (beretta, ), (21a, ), (bobcat, ), (i...</td>\n",
       "      <td>[(how, ), (a, ), (beretta, PRODUCT), (model, )...</td>\n",
       "      <td>[0.21161559146390707, 0.8290244630930195, 0.22...</td>\n",
       "      <td>[0.6464091392751853, 0.4550243894508378, 1.243...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>blue mountain state is an american comedy seri...</td>\n",
       "      <td>where was blue mountain state filmed at</td>\n",
       "      <td>[[-0.14039661, -0.07358733, 0.9209801, -0.3817...</td>\n",
       "      <td>[[0.20251973, -2.1292784, -0.22558218, 2.18635...</td>\n",
       "      <td>[blue, mountain, state, is, an, american, come...</td>\n",
       "      <td>[where, was, blue, mountain, state, filmed, at]</td>\n",
       "      <td>[(blue, JJ), (mountain, NN), (state, NN), (is,...</td>\n",
       "      <td>[(where, WRB), (was, VBD), (blue, JJ), (mounta...</td>\n",
       "      <td>[(blue, LOC), (mountain, LOC), (state, ), (is,...</td>\n",
       "      <td>[(where, ), (was, ), (blue, ), (mountain, ), (...</td>\n",
       "      <td>[0.29476307406437613, 0.3823599012669257, 0.22...</td>\n",
       "      <td>[0.7503384296942396, 0.5907013675064214, 1.063...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>apple inc   formerly apple computer  inc   is ...</td>\n",
       "      <td>when was apple computer founded</td>\n",
       "      <td>[[0.071747534, 1.0473902, 1.9560602, 0.4480185...</td>\n",
       "      <td>[[-4.2278485, -2.5841281, -0.2910819, 0.805927...</td>\n",
       "      <td>[apple, inc, formerly, apple, computer, inc, i...</td>\n",
       "      <td>[when, was, apple, computer, founded]</td>\n",
       "      <td>[(apple, NN), (inc, VBP), (formerly, RB), (app...</td>\n",
       "      <td>[(when, WRB), (was, VBD), (apple, NN), (comput...</td>\n",
       "      <td>[(apple, ORG), (inc, ORG), (formerly, ORG), (a...</td>\n",
       "      <td>[(when, ), (was, ), (apple, ), (computer, ), (...</td>\n",
       "      <td>[0.26158075612861253, 0.089435411305154, 0.029...</td>\n",
       "      <td>[0.9915721940674992, 0.8269819145089902, 1.700...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>section 8 housing in the south bronx section 8...</td>\n",
       "      <td>what is section eight housing</td>\n",
       "      <td>[[1.0145441, 0.23937027, -1.1242148, 0.8756493...</td>\n",
       "      <td>[[1.6417806, -0.42128003, -2.0681963, -3.08729...</td>\n",
       "      <td>[section, 8, housing, in, the, south, bronx, s...</td>\n",
       "      <td>[what, is, section, eight, housing]</td>\n",
       "      <td>[(section, NN), (8, CD), (housing, NN), (in, I...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (section, NN), (eight,...</td>\n",
       "      <td>[(section, LAW), (8, LAW), (housing, ), (in, )...</td>\n",
       "      <td>[(what, ), (is, ), (section, ), (eight, CARDIN...</td>\n",
       "      <td>[0.19207867615144178, 0.16090249029058593, 0.2...</td>\n",
       "      <td>[0.8426670929983164, 0.6879048840289103, 1.514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>restaurants categorized by type and informatio...</td>\n",
       "      <td>what is the main type of restaurant</td>\n",
       "      <td>[[0.005273128, 1.09162, 0.88325566, 1.41998, 0...</td>\n",
       "      <td>[[1.6417806, -0.42128003, -2.0681963, -3.08729...</td>\n",
       "      <td>[restaurants, categorized, by, type, and, info...</td>\n",
       "      <td>[what, is, the, main, type, of, restaurant]</td>\n",
       "      <td>[(restaurants, NNS), (categorized, VBN), (by, ...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (the, DT), (main, JJ),...</td>\n",
       "      <td>[(restaurants, ), (categorized, ), (by, ), (ty...</td>\n",
       "      <td>[(what, ), (is, ), (the, ), (main, ), (type, )...</td>\n",
       "      <td>[0.8769638560774586, 0.9255146376556289, 0.404...</td>\n",
       "      <td>[0.6019050664273689, 0.4913606314492216, 0.483...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>u s  federal reserve notes in the mid 1990s th...</td>\n",
       "      <td>what is us dollar worth based on</td>\n",
       "      <td>[[1.3357239, 1.242577, 1.4468316, 0.8571099, -...</td>\n",
       "      <td>[[1.6417806, -0.42128003, -2.0681963, -3.08729...</td>\n",
       "      <td>[u, s, federal, reserve, notes, in, the, mid, ...</td>\n",
       "      <td>[what, is, us, dollar, worth, based, on]</td>\n",
       "      <td>[(u, JJ), (s, JJ), (federal, JJ), (reserve, NN...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (us, PRP), (dollar, NN...</td>\n",
       "      <td>[(u, ), (s, ORG), (federal, ORG), (reserve, OR...</td>\n",
       "      <td>[(what, ), (is, ), (us, ), (dollar, ), (worth,...</td>\n",
       "      <td>[0.26861217997302017, 0.19882361968755038, 0.3...</td>\n",
       "      <td>[0.6019050664273689, 0.4913606314492216, 0.892...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2117 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Document  \\\n",
       "0     a partly submerged glacier cave on perito more...   \n",
       "1     in physics   circular motion is a movement of ...   \n",
       "2     apollo creed is a fictional character from the...   \n",
       "3     in the united states  the title of federal jud...   \n",
       "4     the beretta 21a bobcat is a small pocket sized...   \n",
       "...                                                 ...   \n",
       "2112  blue mountain state is an american comedy seri...   \n",
       "2113  apple inc   formerly apple computer  inc   is ...   \n",
       "2114  section 8 housing in the south bronx section 8...   \n",
       "2115  restaurants categorized by type and informatio...   \n",
       "2116  u s  federal reserve notes in the mid 1990s th...   \n",
       "\n",
       "                                               Question  \\\n",
       "0                         how are glacier caves formed    \n",
       "1     how are the directions of the velocity and for...   \n",
       "2                              how did apollo creed die   \n",
       "3               how long is the term for federal judges   \n",
       "4        how a beretta model 21 pistols magazines works   \n",
       "...                                                 ...   \n",
       "2112            where was blue mountain state filmed at   \n",
       "2113                    when was apple computer founded   \n",
       "2114                      what is section eight housing   \n",
       "2115                what is the main type of restaurant   \n",
       "2116                   what is us dollar worth based on   \n",
       "\n",
       "                                         Doc_Embeddings  \\\n",
       "0     [[0.18600275, -0.2672038, -2.469172, 1.7943542...   \n",
       "1     [[-1.5330905, 0.4225784, -0.2228741, 0.4760204...   \n",
       "2     [[-0.27247757, 0.75547856, 0.87444097, -0.0673...   \n",
       "3     [[-1.5330905, 0.4225784, -0.2228741, 0.4760204...   \n",
       "4     [[-0.024290355, -0.34677127, -0.9102898, 0.579...   \n",
       "...                                                 ...   \n",
       "2112  [[-0.14039661, -0.07358733, 0.9209801, -0.3817...   \n",
       "2113  [[0.071747534, 1.0473902, 1.9560602, 0.4480185...   \n",
       "2114  [[1.0145441, 0.23937027, -1.1242148, 0.8756493...   \n",
       "2115  [[0.005273128, 1.09162, 0.88325566, 1.41998, 0...   \n",
       "2116  [[1.3357239, 1.242577, 1.4468316, 0.8571099, -...   \n",
       "\n",
       "                                           Q_Embeddings  \\\n",
       "0     [[0.25111866, -0.7270332, -0.56952626, 1.05881...   \n",
       "1     [[0.25111866, -0.7270332, -0.56952626, 1.05881...   \n",
       "2     [[0.25111866, -0.7270332, -0.56952626, 1.05881...   \n",
       "3     [[0.25111866, -0.7270332, -0.56952626, 1.05881...   \n",
       "4     [[0.25111866, -0.7270332, -0.56952626, 1.05881...   \n",
       "...                                                 ...   \n",
       "2112  [[0.20251973, -2.1292784, -0.22558218, 2.18635...   \n",
       "2113  [[-4.2278485, -2.5841281, -0.2910819, 0.805927...   \n",
       "2114  [[1.6417806, -0.42128003, -2.0681963, -3.08729...   \n",
       "2115  [[1.6417806, -0.42128003, -2.0681963, -3.08729...   \n",
       "2116  [[1.6417806, -0.42128003, -2.0681963, -3.08729...   \n",
       "\n",
       "                                             Doc_Tokens  \\\n",
       "0     [a, partly, submerged, glacier, cave, on, peri...   \n",
       "1     [in, physics, circular, motion, is, a, movemen...   \n",
       "2     [apollo, creed, is, a, fictional, character, f...   \n",
       "3     [in, the, united, states, the, title, of, fede...   \n",
       "4     [the, beretta, 21a, bobcat, is, a, small, pock...   \n",
       "...                                                 ...   \n",
       "2112  [blue, mountain, state, is, an, american, come...   \n",
       "2113  [apple, inc, formerly, apple, computer, inc, i...   \n",
       "2114  [section, 8, housing, in, the, south, bronx, s...   \n",
       "2115  [restaurants, categorized, by, type, and, info...   \n",
       "2116  [u, s, federal, reserve, notes, in, the, mid, ...   \n",
       "\n",
       "                                               Q_Tokens  \\\n",
       "0                    [how, are, glacier, caves, formed]   \n",
       "1     [how, are, the, directions, of, the, velocity,...   \n",
       "2                        [how, did, apollo, creed, die]   \n",
       "3      [how, long, is, the, term, for, federal, judges]   \n",
       "4     [how, a, beretta, model, 21, pistols, magazine...   \n",
       "...                                                 ...   \n",
       "2112    [where, was, blue, mountain, state, filmed, at]   \n",
       "2113              [when, was, apple, computer, founded]   \n",
       "2114                [what, is, section, eight, housing]   \n",
       "2115        [what, is, the, main, type, of, restaurant]   \n",
       "2116           [what, is, us, dollar, worth, based, on]   \n",
       "\n",
       "                                                Doc_POS  \\\n",
       "0     [(a, DT), (partly, RB), (submerged, VBN), (gla...   \n",
       "1     [(in, IN), (physics, NNS), (circular, JJ), (mo...   \n",
       "2     [(apollo, NNS), (creed, VBP), (is, VBZ), (a, D...   \n",
       "3     [(in, IN), (the, DT), (united, JJ), (states, V...   \n",
       "4     [(the, DT), (beretta, NN), (21a, CD), (bobcat,...   \n",
       "...                                                 ...   \n",
       "2112  [(blue, JJ), (mountain, NN), (state, NN), (is,...   \n",
       "2113  [(apple, NN), (inc, VBP), (formerly, RB), (app...   \n",
       "2114  [(section, NN), (8, CD), (housing, NN), (in, I...   \n",
       "2115  [(restaurants, NNS), (categorized, VBN), (by, ...   \n",
       "2116  [(u, JJ), (s, JJ), (federal, JJ), (reserve, NN...   \n",
       "\n",
       "                                                  Q_POS  \\\n",
       "0     [(how, WRB), (are, VBP), (glacier, JJ), (caves...   \n",
       "1     [(how, WRB), (are, VBP), (the, DT), (direction...   \n",
       "2     [(how, WRB), (did, VBD), (apollo, VB), (creed,...   \n",
       "3     [(how, WRB), (long, JJ), (is, VBZ), (the, DT),...   \n",
       "4     [(how, WRB), (a, DT), (beretta, NN), (model, N...   \n",
       "...                                                 ...   \n",
       "2112  [(where, WRB), (was, VBD), (blue, JJ), (mounta...   \n",
       "2113  [(when, WRB), (was, VBD), (apple, NN), (comput...   \n",
       "2114  [(what, WP), (is, VBZ), (section, NN), (eight,...   \n",
       "2115  [(what, WP), (is, VBZ), (the, DT), (main, JJ),...   \n",
       "2116  [(what, WP), (is, VBZ), (us, PRP), (dollar, NN...   \n",
       "\n",
       "                                                Doc_NER  \\\n",
       "0     [(a, ), (partly, ), (submerged, ), (glacier, )...   \n",
       "1     [(in, ), (physics, ), (circular, ), (motion, )...   \n",
       "2     [(apollo, ORG), (creed, ), (is, ), (a, ), (fic...   \n",
       "3     [(in, ), (the, GPE), (united, GPE), (states, G...   \n",
       "4     [(the, ), (beretta, ), (21a, ), (bobcat, ), (i...   \n",
       "...                                                 ...   \n",
       "2112  [(blue, LOC), (mountain, LOC), (state, ), (is,...   \n",
       "2113  [(apple, ORG), (inc, ORG), (formerly, ORG), (a...   \n",
       "2114  [(section, LAW), (8, LAW), (housing, ), (in, )...   \n",
       "2115  [(restaurants, ), (categorized, ), (by, ), (ty...   \n",
       "2116  [(u, ), (s, ORG), (federal, ORG), (reserve, OR...   \n",
       "\n",
       "                                                  Q_NER  \\\n",
       "0     [(how, ), (are, ), (glacier, ), (caves, ), (fo...   \n",
       "1     [(how, ), (are, ), (the, ), (directions, ), (o...   \n",
       "2     [(how, ), (did, ), (apollo, ORG), (creed, ), (...   \n",
       "3     [(how, ), (long, ), (is, ), (the, ), (term, ),...   \n",
       "4     [(how, ), (a, ), (beretta, PRODUCT), (model, )...   \n",
       "...                                                 ...   \n",
       "2112  [(where, ), (was, ), (blue, ), (mountain, ), (...   \n",
       "2113  [(when, ), (was, ), (apple, ), (computer, ), (...   \n",
       "2114  [(what, ), (is, ), (section, ), (eight, CARDIN...   \n",
       "2115  [(what, ), (is, ), (the, ), (main, ), (type, )...   \n",
       "2116  [(what, ), (is, ), (us, ), (dollar, ), (worth,...   \n",
       "\n",
       "                                              Doc_TFIDF  \\\n",
       "0     [0.24679288919367473, 0.145118630440616, 0.161...   \n",
       "1     [0.10158600494541081, 0.047978500911022494, 0....   \n",
       "2     [0.1506184153725464, 0.2875274053249442, 0.057...   \n",
       "3     [0.08829203392786322, 0.3204713483897852, 0.16...   \n",
       "4     [0.21161559146390707, 0.8290244630930195, 0.22...   \n",
       "...                                                 ...   \n",
       "2112  [0.29476307406437613, 0.3823599012669257, 0.22...   \n",
       "2113  [0.26158075612861253, 0.089435411305154, 0.029...   \n",
       "2114  [0.19207867615144178, 0.16090249029058593, 0.2...   \n",
       "2115  [0.8769638560774586, 0.9255146376556289, 0.404...   \n",
       "2116  [0.26861217997302017, 0.19882361968755038, 0.3...   \n",
       "\n",
       "                                                Q_TFIDF  \n",
       "0     [1.0342546228402965, 0.8500722992276416, 1.989...  \n",
       "1     [0.34475154094676547, 0.2833574330758805, 0.45...  \n",
       "2     [1.0342546228402965, 1.1571023666681808, 1.787...  \n",
       "3     [0.6464091392751853, 0.7778250162349475, 0.429...  \n",
       "4     [0.6464091392751853, 0.4550243894508378, 1.243...  \n",
       "...                                                 ...  \n",
       "2112  [0.7503384296942396, 0.5907013675064214, 1.063...  \n",
       "2113  [0.9915721940674992, 0.8269819145089902, 1.700...  \n",
       "2114  [0.8426670929983164, 0.6879048840289103, 1.514...  \n",
       "2115  [0.6019050664273689, 0.4913606314492216, 0.483...  \n",
       "2116  [0.6019050664273689, 0.4913606314492216, 0.892...  \n",
       "\n",
       "[2117 rows x 12 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc_ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60b20141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vectorize(\n",
    "    pos_tagger, ner_tagger, data\n",
    "):  # pass in the unique dict for ner or pos\n",
    "    pos_idx = pos_tagger.values()\n",
    "    pos_ohv = np.eye(max(pos_idx) + 1)  # create the ohv\n",
    "    ner_idx = ner_tagger.values()\n",
    "    ner_ohv = np.eye(max(ner_idx) + 1)\n",
    "\n",
    "    dpos_full_ohv, dner_full_ohv = [], []  # lists to append to\n",
    "    qpos_full_ohv, qner_full_ohv = [], []  # lists to append to\n",
    "\n",
    "    for item in data[\"Doc_POS\"]:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            pos_index_iden = pos_tagger[tag]\n",
    "            sent_ohv.append(pos_ohv[pos_index_iden])\n",
    "        dpos_full_ohv.append(sent_ohv)\n",
    "\n",
    "    for item in data[\"Q_POS\"]:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            pos_index_iden = pos_tagger[tag]\n",
    "            sent_ohv.append(pos_ohv[pos_index_iden])\n",
    "        qpos_full_ohv.append(sent_ohv)\n",
    "\n",
    "    for item in data[\"Doc_NER\"]:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            ner_index_iden = ner_tagger[tag]\n",
    "            sent_ohv.append(ner_ohv[ner_index_iden])\n",
    "        dner_full_ohv.append(sent_ohv)\n",
    "\n",
    "    for item in data[\"Q_NER\"]:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            ner_index_iden = ner_tagger[tag]\n",
    "            sent_ohv.append(ner_ohv[ner_index_iden])\n",
    "        qner_full_ohv.append(sent_ohv)\n",
    "\n",
    "    return (dpos_full_ohv, qpos_full_ohv, dner_full_ohv, qner_full_ohv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c766e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ohv for doc\n",
    "(\n",
    "    train_doc_pos_ohv,\n",
    "    train_q_pos_ohv,\n",
    "    train_doc_ner_ohv,\n",
    "    train_q_ner_ohv,\n",
    ") = one_hot_vectorize(pos_iden, ner_iden, train_doc_ques)\n",
    "test_doc_pos_ohv, test_q_pos_ohv, test_doc_ner_ohv, test_q_ner_ohv = one_hot_vectorize(\n",
    "    pos_iden, ner_iden, test_doc_ques\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68e412d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dataframe to just tokens and embeddings:\n",
    "doc_emb_train = train_doc_ques[[\"Doc_Tokens\", \"Doc_Embeddings\", \"Doc_TFIDF\"]]\n",
    "doc_pos_ner = pd.DataFrame({\"Doc_POS\": train_doc_pos_ohv, \"Doc_NER\": train_doc_ner_ohv})\n",
    "doc_emb_train = pd.concat([doc_emb_train, doc_pos_ner], axis=1)\n",
    "\n",
    "q_emb_train = train_doc_ques[[\"Q_Tokens\", \"Q_Embeddings\", \"Q_TFIDF\"]]\n",
    "q_pos_ner = pd.DataFrame({\"Q_POS\": train_q_pos_ohv, \"Q_NER\": train_q_ner_ohv})\n",
    "q_emb_train = pd.concat([q_emb_train, q_pos_ner], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d10ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb_test = test_doc_ques[[\"Doc_Tokens\", \"Doc_Embeddings\", \"Doc_TFIDF\"]]\n",
    "doc_pos_ner = pd.DataFrame({\"Doc_POS\": test_doc_pos_ohv, \"Doc_NER\": test_doc_ner_ohv})\n",
    "doc_emb_test = pd.concat([doc_emb_test, doc_pos_ner], axis=1)\n",
    "\n",
    "q_emb_test = test_doc_ques[[\"Q_Tokens\", \"Q_Embeddings\", \"Q_TFIDF\"]]\n",
    "q_pos_ner = pd.DataFrame({\"Q_POS\": test_q_pos_ohv, \"Q_NER\": test_q_ner_ohv})\n",
    "q_emb_test = pd.concat([q_emb_test, q_pos_ner], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad89b3",
   "metadata": {},
   "source": [
    "### Word Embeddings (Doc and Qn)\n",
    "\n",
    "The embeddings of the questions and answers of the train and test set can be found here:\n",
    "\n",
    "-   Train Document - doc_emb_train\n",
    "-   Train Q - q_emb_train\n",
    "-   Test Document - doc_emb_test\n",
    "-   Test Q - q_emb_test\n",
    "\n",
    "The max_document size is 1675 and max_question size is 23.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c75c753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc_Tokens        [a, partly, submerged, glacier, cave, on, peri...\n",
       "Doc_Embeddings    [[0.18600275, -0.2672038, -2.469172, 1.7943542...\n",
       "Doc_TFIDF         [0.24679288919367473, 0.145118630440616, 0.161...\n",
       "Doc_POS           [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "Doc_NER           [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb_train.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49488c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2117"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_emb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c7aa57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_array(data, data_type=\"Document\"):\n",
    "    num_vec_length = 156\n",
    "    max_doc = 1675\n",
    "    max_qn = 23\n",
    "    zero_vec = np.zeros(156)\n",
    "\n",
    "    if data_type == \"Document\":\n",
    "        full_vec = []  # create a list for list of list for document\n",
    "        for dat in range(len(data)):  # go through each line\n",
    "            doc_ques = data.loc[dat]  # document data\n",
    "            v = []  # create list to each word\n",
    "            for j in range(len(doc_ques.iloc[0])):\n",
    "                vn = []  # list of concat word embeddings\n",
    "                vn.append(doc_ques.iloc[1][j].tolist()) #Word2Vec\n",
    "                vn.append(doc_ques.iloc[2][j]) # TF-IDF\n",
    "                vn.append(doc_ques.iloc[3][j].tolist()) # POS\n",
    "                vn.append(doc_ques.iloc[4][j].tolist()) # NER\n",
    "                flatten = [\n",
    "                    item\n",
    "                    for sublist in vn\n",
    "                    for item in (sublist if isinstance(sublist, list) else [sublist])\n",
    "                ]\n",
    "                v.append(flatten)\n",
    "            while len(v) < max_doc:\n",
    "                v.append(zero_vec)\n",
    "            full_vec.append(v)\n",
    "\n",
    "    if data_type == \"Question\":\n",
    "        full_vec = []  # create a list for list of list for document\n",
    "        for dat in range(len(data)):  # go through each line\n",
    "            doc_ques = data.loc[dat]  # document data\n",
    "            v = []  # create list to each word\n",
    "            for j in range(len(doc_ques.iloc[0])):\n",
    "                vn = []  # list of concat word embeddings\n",
    "                vn.append(doc_ques.iloc[1][j].tolist()) #Word2Vec\n",
    "                vn.append(doc_ques.iloc[2][j]) # TF-IDF\n",
    "                vn.append(doc_ques.iloc[3][j].tolist()) #POS\n",
    "                vn.append(doc_ques.iloc[4][j].tolist()) #NER\n",
    "                flatten = [\n",
    "                    item\n",
    "                    for sublist in vn\n",
    "                    for item in (sublist if isinstance(sublist, list) else [sublist])\n",
    "                ]\n",
    "                v.append(flatten)\n",
    "            while len(v) < max_qn:\n",
    "                v.append(zero_vec)\n",
    "            full_vec.append(v)\n",
    "    return full_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0088de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Test Documents to pass in, takes about a min\n",
    "final_doc_train = full_array(doc_emb_train, data_type=\"Document\")\n",
    "final_doc_test = full_array(doc_emb_test, data_type=\"Document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbb830b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Test Questions to pass in, takes about a few seconds\n",
    "final_qn_train = full_array(q_emb_train, data_type=\"Question\")\n",
    "final_qn_test = full_array(q_emb_test, data_type=\"Question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e9e3d",
   "metadata": {},
   "source": [
    "### Converting into Tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db6375d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df61114f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_doc_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bc/5zby_td12xvblch941x43zl40000gn/T/ipykernel_40802/3033670222.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# takes a min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf_final_doc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_doc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtf_final_doc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_doc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf_final_qn_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_qn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf_final_qn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_qn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_doc_train' is not defined"
     ]
    }
   ],
   "source": [
    "# takes a min\n",
    "tf_final_doc_train = torch.tensor(final_doc_train, device=device)\n",
    "tf_final_doc_test = torch.tensor(final_doc_test, device=device)\n",
    "tf_final_qn_train = torch.tensor(final_qn_train, device=device)\n",
    "tf_final_qn_test = torch.tensor(final_qn_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dba3fbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_final_doc_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bc/5zby_td12xvblch941x43zl40000gn/T/ipykernel_40802/3184650958.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# check dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_final_doc_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_final_doc_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_final_qn_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_final_qn_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_final_doc_train' is not defined"
     ]
    }
   ],
   "source": [
    "# check dimensions\n",
    "print(tf_final_doc_train.shape)\n",
    "print(tf_final_doc_test.shape)\n",
    "print(tf_final_qn_train.shape)\n",
    "print(tf_final_qn_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123bb0ea",
   "metadata": {},
   "source": [
    "**Input Embedding Ablation Study**\n",
    "\n",
    "In the model input embedding Ablation study, we are given 3 variations of input embeddings to test. We will test 3 options:\n",
    "\n",
    "1. Word2Vec only # 100 dims\n",
    "2. Word2Vec + Tf-IDF # 101 dims\n",
    "3. Word2Vec + all features (TF-IDF, POS, NER) # 156 dims\n",
    "\n",
    "Since we are using tensors, we can use tensor slicing to take out the relevant features.\n",
    "Our tensor of embeddings are built as follows (w2v, TF-IDF, POS, NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e23b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensors(tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test, option=3):\n",
    "    if option == 3:\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test\n",
    "    elif option == 1:\n",
    "        tf_doc_train = tf_doc_train[:, :, :100]\n",
    "        tf_doc_test = tf_doc_test[:, :, :100]\n",
    "        tf_qn_train = tf_qn_train[:, :, :100]\n",
    "        tf_qn_test = tf_qn_test[:, :, :100]\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test\n",
    "    elif option == 2:\n",
    "        tf_doc_train = tf_doc_train[:, :, :101]\n",
    "        tf_doc_test = tf_doc_test[:, :, :101]\n",
    "        tf_qn_train = tf_qn_train[:, :, :101]\n",
    "        tf_qn_test = tf_qn_test[:, :, :101]\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a70458a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2117, 1675, 100])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change option to see size\n",
    "(convert_tensors(tf_final_doc_train, tf_final_doc_test, tf_final_qn_train, tf_final_qn_test, 1)[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f6e40",
   "metadata": {},
   "source": [
    "Our answer should perhaps also be in the form of (1x1675) list containing ['N','S','I','E']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8538510",
   "metadata": {},
   "source": [
    "Additionally, the labels should be one hot vectorised, as they are cateogorical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8adde8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(labels):\n",
    "    check = []\n",
    "    for i in labels:\n",
    "        if len(i) < 1675:\n",
    "            while len(i) < 1675:\n",
    "                i.append('N')\n",
    "            check.append(i)\n",
    "        else:\n",
    "            check.append(i)\n",
    "    return check\n",
    "tr_labels = convert_labels(train_doc_ans_labels)\n",
    "ts_labels = convert_labels(test_doc_ans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9ee50025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_labels(labels):\n",
    "    # Create a dictionary that maps each label to a unique integer\n",
    "    label_to_int = {'N': 0, 'S': 1, 'I': 2, 'E': 3}\n",
    "\n",
    "    # Map the labels to integers\n",
    "    int_labels = [[label_to_int[label] for label in sequence] for sequence in labels]\n",
    "\n",
    "    # Create an identity matrix of size 4 (since there are 4 labels)\n",
    "    identity = np.eye(4)\n",
    "\n",
    "    # Use the integer labels as indices to select rows from the identity matrix\n",
    "    one_hot_labels = [identity[sequence] for sequence in int_labels]\n",
    "    return one_hot_labels\n",
    "\n",
    "tr_encoded = one_hot_encode_labels(tr_labels)\n",
    "tst_encoded = one_hot_encode_labels(ts_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0b7a7",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Bi-LSTM for Document Portion\n",
    "\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "\n",
    "        # Define the Bi-LSTM layers for the document and the question\n",
    "        self.document_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.question_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, document, question):\n",
    "        # Pass the document and the question through their respective Bi-LSTM layers\n",
    "        document_output, _ = self.document_lstm(document)\n",
    "        question_output, _ = self.question_lstm(question)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from typing import Literal, Dict, Type, Union\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class NNType(Enum):\n",
    "    RNN = \"rnn\"\n",
    "    LSTM = \"lstm\"\n",
    "    GRU = \"gru\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "NN_MAP: Dict[NNType, Type[Union[nn.RNN, nn.LSTM, nn.GRU]]] = {\n",
    "    NNType.RNN: nn.RNN,\n",
    "    NNType.LSTM: nn.LSTM,\n",
    "    NNType.GRU: nn.GRU,\n",
    "}\n",
    "\n",
    "\n",
    "class EncoderBiRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        embedding: nn.Embedding,\n",
    "        nn_type: Literal[\"rnn\", \"lstm\", \"gru\"] = \"rnn\",\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super(EncoderBiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.nn_type = NNType(nn_type)\n",
    "        self.nn = NN_MAP[self.nn_type](\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: nn.Embedding, hidden: Tensor):\n",
    "        embedded: Tensor = self.embedding(input).view(1, 1, -1)\n",
    "        output: Tensor\n",
    "        output, hidden = self.nn(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (\n",
    "            torch.zeros(2, 1, self.hidden_size, device=device)\n",
    "            if self.nn_type != NNType.LSTM\n",
    "            else (\n",
    "                torch.zeros(2, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(2, 1, self.hidden_size, device=device),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AttentionMethod(Enum):\n",
    "    DOT_PRODUCT = \"dot_product\"\n",
    "    SCALE_DOT_PRODUCT = \"scale_dot_product\"\n",
    "    COSINE_SIMILARITY = \"cosine_similarity\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class DecoderBiRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        embedding: nn.Embedding,\n",
    "        max_length: int,\n",
    "        nn_type: Literal[\"rnn\", \"lstm\", \"gru\"] = \"rnn\",\n",
    "        num_layers=1,\n",
    "        dropout_p=0.1,\n",
    "        attention_method: Literal[\n",
    "            \"dot_product\",\n",
    "            \"scale_dot_product\",\n",
    "            \"cosine_similarity\",\n",
    "        ] = \"dot_product\",\n",
    "    ):\n",
    "        super(DecoderBiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.embedding = embedding\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.nn_type = NNType(nn_type)\n",
    "        self.attention_method = AttentionMethod(attention_method)\n",
    "        self.nn = NN_MAP[self.nn_type](\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.out = nn.Linear(self.hidden_size * 4, self.output_size)\n",
    "\n",
    "    def cal_attention(self, hidden: Tensor, encoder_hiddens: Tensor):\n",
    "        if self.attention_method == AttentionMethod.DOT_PRODUCT:\n",
    "            if self.nn_type == NNType.LSTM:  # For BiLSTM\n",
    "                energy = torch.bmm(hidden[0], encoder_hiddens.T.repeat(2, 1, 1))\n",
    "                attn_weights = F.softmax(energy, dim=-1)\n",
    "                attn_output = torch.bmm(attn_weights, encoder_hiddens.repeat(2, 1, 1))\n",
    "                concat_output = torch.cat(\n",
    "                    (attn_output[0], hidden[0][0], attn_output[1], hidden[0][1]), 1\n",
    "                )\n",
    "            else:  # For BiRNN & BiGRU\n",
    "                energy = torch.bmm(hidden, encoder_hiddens.T.repeat(2, 1, 1))\n",
    "                attn_weights = F.softmax(energy, dim=-1)\n",
    "                attn_output = torch.bmm(attn_weights, encoder_hiddens.repeat(2, 1, 1))\n",
    "                concat_output = torch.cat(\n",
    "                    (attn_output[0], hidden[0], attn_output[1], hidden[1]), 1\n",
    "                )\n",
    "\n",
    "        elif self.attention_method == AttentionMethod.COSINE_SIMILARITY:\n",
    "            if self.nn_type == NNType.LSTM:  # For LSTM\n",
    "                cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
    "                h_n, c_n = hidden\n",
    "                # h_n_reshaped = h_n.mean(dim=0, keepdim=True)\n",
    "                attn_weights_f = F.softmax(\n",
    "                    cosine_similarity(h_n[0].unsqueeze(0), encoder_hiddens), dim=-1\n",
    "                )\n",
    "                attn_output_f = torch.bmm(\n",
    "                    attn_weights_f.unsqueeze(0), encoder_hiddens.unsqueeze(0)\n",
    "                )\n",
    "                attn_weights_b = F.softmax(\n",
    "                    cosine_similarity(h_n[1].unsqueeze(0), encoder_hiddens), dim=-1\n",
    "                )\n",
    "                attn_output_b = torch.bmm(\n",
    "                    attn_weights_b.unsqueeze(0), encoder_hiddens.unsqueeze(0)\n",
    "                )\n",
    "                concat_output = torch.cat(\n",
    "                    (\n",
    "                        attn_output_f[0],\n",
    "                        h_n[0],\n",
    "                        attn_output_b[0],\n",
    "                        h_n[1],\n",
    "                    ),\n",
    "                    1,\n",
    "                )\n",
    "\n",
    "            else:  # For RNN & GRU\n",
    "                cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
    "                # hidden_reshaped = hidden.mean(dim=0, keepdim=True)\n",
    "                # print(hidden_reshaped.shape)\n",
    "                attn_weights_f = F.softmax(\n",
    "                    cosine_similarity(hidden[0].unsqueeze(0), encoder_hiddens), dim=-1\n",
    "                )\n",
    "                attn_output_f = torch.bmm(\n",
    "                    attn_weights_f.unsqueeze(0), encoder_hiddens.unsqueeze(0)\n",
    "                )\n",
    "                attn_weights_b = F.softmax(\n",
    "                    cosine_similarity(hidden[1].unsqueeze(0), encoder_hiddens), dim=-1\n",
    "                )\n",
    "                attn_output_b = torch.bmm(\n",
    "                    attn_weights_b.unsqueeze(0), encoder_hiddens.unsqueeze(0)\n",
    "                )\n",
    "                concat_output = torch.cat(\n",
    "                    (\n",
    "                        attn_output_f[0],\n",
    "                        hidden[0],\n",
    "                        attn_output_b[0],\n",
    "                        hidden[1],\n",
    "                    ),\n",
    "                    1,\n",
    "                )\n",
    "        else:\n",
    "            if self.nn_type == NNType.LSTM:  # For LSTM\n",
    "                energy = torch.bmm(\n",
    "                    hidden[0], encoder_hiddens.T.repeat(2, 1, 1)\n",
    "                ) / np.sqrt(self.hidden_size)\n",
    "                attn_weights = F.softmax(energy, dim=-1)\n",
    "                attn_output = torch.bmm(attn_weights, encoder_hiddens.repeat(2, 1, 1))\n",
    "                concat_output = torch.cat(\n",
    "                    (attn_output[0], hidden[0][0], attn_output[1], hidden[0][1]), 1\n",
    "                )\n",
    "            else:  # For RNN & GRU\n",
    "                energy = torch.bmm(hidden, encoder_hiddens.T.repeat(2, 1, 1)) / np.sqrt(\n",
    "                    self.hidden_size\n",
    "                )\n",
    "                attn_weights = F.softmax(energy, dim=-1)\n",
    "                attn_output = torch.bmm(attn_weights, encoder_hiddens.repeat(2, 1, 1))\n",
    "                concat_output = torch.cat(\n",
    "                    (attn_output[0], hidden[0], attn_output[1], hidden[1]), 1\n",
    "                )\n",
    "        return concat_output\n",
    "\n",
    "    def forward(self, input: nn.Embedding, hidden: Tensor, encoder_hiddens: Tensor):\n",
    "        embedded: Tensor = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        _, hidden = self.nn(embedded, hidden)\n",
    "        concat_output = self.cal_attention(hidden, encoder_hiddens)\n",
    "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (\n",
    "            torch.zeros(2, 1, self.hidden_size, device=device)\n",
    "            if self.nn_type != NNType.LSTM\n",
    "            else (\n",
    "                torch.zeros(2, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(2, 1, self.hidden_size, device=device),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53da45d",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e9274dd",
   "metadata": {
    "tags": [
     "parameters",
     "Max doc length"
    ]
   },
   "outputs": [],
   "source": [
    "MAX_DOC_LENGTH = 1675  # Max doc length\n",
    "MAX_QN_LENGTH = 23  # Max question length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e43c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(\n",
    "    input_tensor,\n",
    "    target_tensor,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    criterion,\n",
    "):\n",
    "    # Initialize the hidden state of the encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # Set the gradients of the optimizers to zero\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Get the length of the input and target tensors\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # Initialize the loss to zero\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over the length of the input tensor\n",
    "    for ei in range(input_length):\n",
    "        # Pass each element of the input tensor through the encoder\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    # Set the initial input to the decoder as the first element of the target tensor\n",
    "    decoder_input = target_tensor[0]\n",
    "    # Set the initial hidden state of the decoder as the final hidden state of the encoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Iterate over the length of the target tensor\n",
    "    for di in range(target_length):\n",
    "        # Pass each element of the target tensor through the decoder\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        # Calculate and accumulate the loss by comparing the output of the decoder to the target tensor\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        # Set the next input to the decoder as the current element of the target tensor\n",
    "        decoder_input = target_tensor[di]\n",
    "\n",
    "    # Compute gradients using backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # Return average loss per element in target sequence\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4152800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    input_tensor,\n",
    "    target_tensor,\n",
    "    documentRNN,\n",
    "    questionRNN,\n",
    "    documentRNN_optimizer,\n",
    "    questionRNN_optimizer,\n",
    "    criterion,\n",
    "    max_doc_length=MAX_DOC_LENGTH,\n",
    "    max_qn_length=MAX_QN_LENGTH,\n",
    "    nn_type=\"rnn\",\n",
    "):\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # it is for storing the hidden states of input sequence later, which will be used for calculating the attention during the decoding process\n",
    "    documentRNN_hiddens = torch.zeros(\n",
    "        max_doc_length, documentRNN.hidden_size * 2, device=device\n",
    "    )\n",
    "\n",
    "    # zero-initialize an initial hidden state\n",
    "    documentRNN_hidden = documentRNN.initHidden()\n",
    "    questionRNN_hidden = questionRNN.initHidden()\n",
    "    loss = 0\n",
    "    documentRNN_optimizer.zero_grad()\n",
    "    questionRNN_optimizer.zero_grad()\n",
    "\n",
    "    # Feed the input_tensor into the encoder we defined\n",
    "    for i in range(input_length):\n",
    "        documentRNN_output, documentRNN_hidden = documentRNN(\n",
    "            input_tensor[i], documentRNN_hidden\n",
    "        )\n",
    "        documentRNN_hiddens[i] = (\n",
    "            documentRNN_hidden[0][0, 0]\n",
    "            if nn_type == \"lstm\"\n",
    "            else documentRNN_hidden[0, 0]\n",
    "        )\n",
    "\n",
    "    # Set the initial input to the decoder as the first element of the target tensor\n",
    "    questionRNN_input = target_tensor[0]\n",
    "\n",
    "    # Teacher forcing: Feed the target as the next input\n",
    "    for i in range(target_length):\n",
    "        questionRNN_output, questionRNN_hidden = questionRNN(\n",
    "            questionRNN_input, questionRNN_hidden, documentRNN_hiddens\n",
    "        )\n",
    "        loss += criterion(questionRNN_output, target_tensor[i])\n",
    "        questionRNN_input = target_tensor[i]\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    documentRNN_optimizer.step()\n",
    "    questionRNN_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82256bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "# Helper functions for training\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (- %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "def trainIters(\n",
    "    documentRNN,\n",
    "    questionRNN,\n",
    "    n_iters,\n",
    "    print_every=1000,\n",
    "    plot_every=100,\n",
    "    learning_rate=0.01,\n",
    "    nn_type=\"rnn\",\n",
    "):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    documentRNN_optimizer = optim.AdamW(documentRNN.parameters(), lr=learning_rate)\n",
    "    questionRNN_optimizer = optim.AdamW(questionRNN.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        random_choice_ix = random.choice(\n",
    "            range(n_data)\n",
    "        )  # Get a random index within the scope of input data\n",
    "        input_index_r = [[ind] for ind in input_index[random_choice_ix]]\n",
    "        target_index_r = [[ind] for ind in target_index[random_choice_ix]]\n",
    "\n",
    "        input_tensor = torch.LongTensor(input_index_r).to(device)\n",
    "        target_tensor = torch.LongTensor(target_index_r).to(device)\n",
    "\n",
    "        loss = train(\n",
    "            input_tensor,\n",
    "            target_tensor,\n",
    "            documentRNN,\n",
    "            questionRNN,\n",
    "            documentRNN_optimizer,\n",
    "            questionRNN_optimizer,\n",
    "            criterion,\n",
    "            nn_type=nn_type,\n",
    "        )\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(\n",
    "                \"%s (%d %d%%) %.4f\"\n",
    "                % (\n",
    "                    timeSince(start, iter / n_iters),\n",
    "                    iter,\n",
    "                    iter / n_iters * 100,\n",
    "                    print_loss_avg,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0452a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_sent = pre_process([sentence])[0]\n",
    "        intput_index = [word_to_ix[word] for word in input_sent]\n",
    "        input_tensor = torch.LongTensor([[ind] for ind in intput_index]).to(device)\n",
    "\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_hiddens = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            # encoder_hiddens[ei] += encoder_hidden[0, 0]\n",
    "            encoder_hiddens[ei] = encoder_hidden[0][0, 0]  # LSTM\n",
    "\n",
    "        decoder_input = torch.tensor([[word_to_ix[\"<BOS>\"]]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_hiddens\n",
    "            )\n",
    "            topv, topi = decoder_output.data.topk(\n",
    "                1\n",
    "            )  # simply adopt the predicted tag with the highest probabiity\n",
    "            if (\n",
    "                topi.item() == word_to_ix[\"<EOS>\"]\n",
    "            ):  # if <EOS> is generated, stop the generation\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(\n",
    "                    word_list[topi.item()]\n",
    "                )  # get the predicted word based on the index\n",
    "            # use the predicted output as the input for the next time step generation\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
