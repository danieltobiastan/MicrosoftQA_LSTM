{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "528971ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/finnmurphy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c3cb696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data\n",
    "train_data = pd.read_csv('WikiQA-train.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('WikiQA-test.tsv', sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fad5982",
   "metadata": {},
   "source": [
    "Extract the unique questions from the train and test data frames, including the documentID and the DocumentTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "034c4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_documenttag(data):\n",
    "    qd = data[['Question', 'QuestionID', 'DocumentID','DocumentTitle']].drop_duplicates()\n",
    "    return qd\n",
    "train_question_doctag = get_questions_documenttag(train_data)\n",
    "test_question_doctag = get_questions_documenttag(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "368895a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique questions\n",
    "train_questions = train_question_doctag['Question']\n",
    "test_questions = test_question_doctag['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3f8f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique document ids\n",
    "train_docid = train_question_doctag['DocumentID']\n",
    "test_docid = test_question_doctag['DocumentID']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fbf4306",
   "metadata": {},
   "source": [
    "Extract the answers to those questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2d6a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(data, questions, documentids): \n",
    "    answers = [] # list of answers\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q] # add the document id\n",
    "        df = data[data['Question'] == question]\n",
    "        index = df.loc[df['Label'] == 1]['Sentence'].index.values\n",
    "        if len(index) == 0: # if no answer found\n",
    "            answers.append([question, doc_id, 'No answer'])\n",
    "        else: # if 1 answer found\n",
    "            answers.append([question, doc_id, df.loc[index[0], \"Sentence\"]])\n",
    "    return answers\n",
    "\n",
    "train_answers = pd.DataFrame(get_answers(train_data, train_questions, train_docid))\n",
    "test_answers = pd.DataFrame(get_answers(test_data, test_questions, test_docid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12e5527b",
   "metadata": {},
   "source": [
    "The above get_answers returns train_answers and test_answers which, gives us in the following columns\n",
    "- Question\n",
    "- Related Document ID\n",
    "- Answer (if no answer to that question, return no answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac4567fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(data, questions, documentids): # (done by Finn, tweaked by Dan)\n",
    "    documents = []\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q] # add the document id\n",
    "        df = data[data['Question'] == question]\n",
    "        sentences = df['Sentence'].tolist()\n",
    "        for i in range(0, len(sentences) - 1):\n",
    "            sentences[i] = sentences[i] + ' '\n",
    "        documents.append([doc_id,''.join(sentences)])\n",
    "    return documents\n",
    "\n",
    "train_documents = pd.DataFrame(get_documents(train_data, train_questions, train_docid)) # return the individual document in list\n",
    "test_documents = pd.DataFrame(get_documents(test_data, test_questions, test_docid)) # return the individual document in list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0aa7704",
   "metadata": {},
   "source": [
    "The above train_documents and test_documents called from the get_documents gives us in the following columns\n",
    "- Document ID\n",
    "- Full Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0432bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming all the columns for more standardised access\n",
    "train_answers.columns = ['Question','DocumentID','Answer']\n",
    "test_answers.columns = ['Question','DocumentID','Answer']\n",
    "train_documents.columns = ['DocumentID','Document']\n",
    "test_documents.columns = ['DocumentID','Document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "763141d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2117, 2117, 630, 630)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result is 2117, 2117, 630, 630\n",
    "\n",
    "len(train_answers),len(train_documents), len(test_answers),len(test_documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5edcdf6",
   "metadata": {},
   "source": [
    "**Prior to tagging, we should maybe clean the document and answers first:** (stopped here)\n",
    "\n",
    "Maybe? \n",
    "- lowercase (might lose context, but we can use on questions)\n",
    "- removing any punctuation or weird symbols (do)\n",
    "- removal of stop words? (probably not)\n",
    "\n",
    "Make sure that the pre-processing is standardised to be the same throughout doc and ans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5fddce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lower(text):\n",
    "    # Lowercase the text for question, answer and documents\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "train_answers[['Question', 'Answer']] = train_answers[['Question', 'Answer']].applymap(preprocess_lower)\n",
    "train_documents['Document'] = train_documents['Document'].apply(preprocess_lower)\n",
    "test_answers[['Question', 'Answer']] = test_answers[['Question', 'Answer']].applymap(preprocess_lower)\n",
    "test_documents['Document'] = test_documents['Document'].apply(preprocess_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9785d51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>a partly submerged glacier cave on perito more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>in physics , circular motion is a movement of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D5</td>\n",
       "      <td>apollo creed is a fictional character from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D6</td>\n",
       "      <td>in the united states, the title of federal jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D7</td>\n",
       "      <td>the beretta 21a bobcat is a small pocket-sized...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>D2805</td>\n",
       "      <td>blue mountain state is an american comedy seri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>D2806</td>\n",
       "      <td>apple inc., formerly apple computer, inc., is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>D2807</td>\n",
       "      <td>section 8 housing in the south bronx section 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>D2808</td>\n",
       "      <td>restaurants categorized by type and informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>D2810</td>\n",
       "      <td>u.s. federal reserve notes in the mid-1990s th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2117 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DocumentID                                           Document\n",
       "0            D1  a partly submerged glacier cave on perito more...\n",
       "1            D2  in physics , circular motion is a movement of ...\n",
       "2            D5  apollo creed is a fictional character from the...\n",
       "3            D6  in the united states, the title of federal jud...\n",
       "4            D7  the beretta 21a bobcat is a small pocket-sized...\n",
       "...         ...                                                ...\n",
       "2112      D2805  blue mountain state is an american comedy seri...\n",
       "2113      D2806  apple inc., formerly apple computer, inc., is ...\n",
       "2114      D2807  section 8 housing in the south bronx section 8...\n",
       "2115      D2808  restaurants categorized by type and informatio...\n",
       "2116      D2810  u.s. federal reserve notes in the mid-1990s th...\n",
       "\n",
       "[2117 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3328fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelling(documents, answers):\n",
    "    tagged_documents = []\n",
    "    for q in range(len(answers)):\n",
    "        tagged_document = []\n",
    "        qn = answers['Question'].loc[q]\n",
    "        doc_id = answers['DocumentID'].loc[q]\n",
    "        content = documents.loc[documents['DocumentID'] == doc_id,'Document'].values[0]\n",
    "        answer = answers['Answer'].loc[q]\n",
    "\n",
    "        if answer == 'no answer':\n",
    "            tokens = word_tokenize(content)\n",
    "            for j in range(len(tokens)):\n",
    "                tagged_document.append('N') # none \n",
    "        else:\n",
    "            parts = content.partition(answer)\n",
    "            for j in range(len(parts)):\n",
    "                tokens = word_tokenize(parts[j])\n",
    "                if j == 1:\n",
    "                    tagged_document.append('S') # start of answer\n",
    "                    for k in range(len(tokens) - 2):\n",
    "                        tagged_document.append('I') # inside of answer\n",
    "                    tagged_document.append('E') # end of answer\n",
    "                else:\n",
    "                    for k in range(len(tokens)):\n",
    "                        tagged_document.append('N') # outside answer\n",
    "        tagged_documents.append(tagged_document)\n",
    "    return(tagged_documents)\n",
    "\n",
    "train_doc_ans_labels = labelling(train_documents, train_answers)\n",
    "test_doc_ans_labels = labelling(test_documents, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3fc6f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'the']\n",
      "['N', 'big']\n",
      "['N', 'ten']\n",
      "['N', 'conference']\n",
      "['N', ',']\n",
      "['N', 'formerly']\n",
      "['N', 'western']\n",
      "['N', 'conference']\n",
      "['N', 'and']\n",
      "['N', 'big']\n",
      "['N', 'nine']\n",
      "['N', 'conference']\n",
      "['N', ',']\n",
      "['N', 'is']\n",
      "['N', 'the']\n",
      "['N', 'oldest']\n",
      "['N', 'division']\n",
      "['N', 'i']\n",
      "['N', 'college']\n",
      "['N', 'athletic']\n",
      "['N', 'conference']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'united']\n",
      "['N', 'states']\n",
      "['N', '.']\n",
      "['S', 'its']\n",
      "['I', 'twelve']\n",
      "['I', 'member']\n",
      "['I', 'institutions']\n",
      "['I', '(']\n",
      "['I', 'which']\n",
      "['I', 'are']\n",
      "['I', 'primarily']\n",
      "['I', 'flagship']\n",
      "['I', 'research']\n",
      "['I', 'universities']\n",
      "['I', 'in']\n",
      "['I', 'their']\n",
      "['I', 'respective']\n",
      "['I', 'states']\n",
      "['I', ',']\n",
      "['I', 'well-regarded']\n",
      "['I', 'academically']\n",
      "['I', ',']\n",
      "['I', 'and']\n",
      "['I', 'with']\n",
      "['I', 'relatively']\n",
      "['I', 'large']\n",
      "['I', 'student']\n",
      "['I', 'enrollment']\n",
      "['I', ')']\n",
      "['I', 'are']\n",
      "['I', 'located']\n",
      "['I', 'primarily']\n",
      "['I', 'in']\n",
      "['I', 'the']\n",
      "['I', 'midwest']\n",
      "['I', ',']\n",
      "['I', 'stretching']\n",
      "['I', 'from']\n",
      "['I', 'nebraska']\n",
      "['I', 'in']\n",
      "['I', 'the']\n",
      "['I', 'west']\n",
      "['I', 'to']\n",
      "['I', 'penn']\n",
      "['I', 'state']\n",
      "['I', 'in']\n",
      "['I', 'the']\n",
      "['I', 'east']\n",
      "['E', '.']\n",
      "['N', 'the']\n",
      "['N', 'conference']\n",
      "['N', 'competes']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'ncaa']\n",
      "['N', \"'s\"]\n",
      "['N', 'division']\n",
      "['N', 'i']\n",
      "['N', ';']\n",
      "['N', 'its']\n",
      "['N', 'football']\n",
      "['N', 'teams']\n",
      "['N', 'compete']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'football']\n",
      "['N', 'bowl']\n",
      "['N', 'subdivision']\n",
      "['N', '(']\n",
      "['N', 'fbs']\n",
      "['N', ')']\n",
      "['N', ',']\n",
      "['N', 'formerly']\n",
      "['N', 'known']\n",
      "['N', 'as']\n",
      "['N', 'division']\n",
      "['N', 'i-a']\n",
      "['N', ',']\n",
      "['N', 'the']\n",
      "['N', 'highest']\n",
      "['N', 'level']\n",
      "['N', 'of']\n",
      "['N', 'ncaa']\n",
      "['N', 'competition']\n",
      "['N', 'in']\n",
      "['N', 'that']\n",
      "['N', 'sport']\n",
      "['N', '.']\n",
      "['N', 'member']\n",
      "['N', 'schools']\n",
      "['N', 'of']\n",
      "['N', 'the']\n",
      "['N', 'big']\n",
      "['N', 'ten']\n",
      "['N', '(']\n",
      "['N', 'or']\n",
      "['N', ',']\n",
      "['N', 'in']\n",
      "['N', 'two']\n",
      "['N', 'cases']\n",
      "['N', ',']\n",
      "['N', 'their']\n",
      "['N', 'parent']\n",
      "['N', 'university']\n",
      "['N', 'systems']\n",
      "['N', ')']\n",
      "['N', 'also']\n",
      "['N', 'are']\n",
      "['N', 'members']\n",
      "['N', 'of']\n",
      "['N', 'the']\n",
      "['N', 'committee']\n",
      "['N', 'on']\n",
      "['N', 'institutional']\n",
      "['N', 'cooperation']\n",
      "['N', ',']\n",
      "['N', 'a']\n",
      "['N', 'leading']\n",
      "['N', 'educational']\n",
      "['N', 'and']\n",
      "['N', 'research']\n",
      "['N', 'consortium']\n",
      "['N', '.']\n",
      "['N', 'despite']\n",
      "['N', 'the']\n",
      "['N', 'conference']\n",
      "['N', \"'s\"]\n",
      "['N', 'name']\n",
      "['N', ',']\n",
      "['N', 'the']\n",
      "['N', 'big']\n",
      "['N', 'ten']\n",
      "['N', 'actually']\n",
      "['N', 'consists']\n",
      "['N', 'of']\n",
      "['N', '12']\n",
      "['N', 'schools']\n",
      "['N', ',']\n",
      "['N', 'following']\n",
      "['N', 'the']\n",
      "['N', 'addition']\n",
      "['N', 'of']\n",
      "['N', 'the']\n",
      "['N', 'pennsylvania']\n",
      "['N', 'state']\n",
      "['N', 'university']\n",
      "['N', 'in']\n",
      "['N', '1993']\n",
      "['N', 'and']\n",
      "['N', 'the']\n",
      "['N', 'university']\n",
      "['N', 'of']\n",
      "['N', 'nebraska–lincoln']\n",
      "['N', 'in']\n",
      "['N', '2011.']\n",
      "['N', 'in']\n",
      "['N', '2014']\n",
      "['N', ',']\n",
      "['N', 'the']\n",
      "['N', 'conference']\n",
      "['N', 'will']\n",
      "['N', 'expand']\n",
      "['N', 'to']\n",
      "['N', '14']\n",
      "['N', 'members']\n",
      "['N', 'with']\n",
      "['N', 'the']\n",
      "['N', 'additions']\n",
      "['N', 'of']\n",
      "['N', 'the']\n",
      "['N', 'university']\n",
      "['N', 'of']\n",
      "['N', 'maryland']\n",
      "['N', ',']\n",
      "['N', 'college']\n",
      "['N', 'park']\n",
      "['N', 'and']\n",
      "['N', 'rutgers']\n",
      "['N', ',']\n",
      "['N', 'the']\n",
      "['N', 'state']\n",
      "['N', 'university']\n",
      "['N', 'of']\n",
      "['N', 'new']\n",
      "['N', 'jersey']\n",
      "['N', '.']\n",
      "['N', 'it']\n",
      "['N', 'is']\n",
      "['N', 'not']\n",
      "['N', 'to']\n",
      "['N', 'be']\n",
      "['N', 'confused']\n",
      "['N', 'with']\n",
      "['N', 'the']\n",
      "['N', 'big']\n",
      "['N', '12']\n",
      "['N', 'conference']\n",
      "['N', ',']\n",
      "['N', 'which']\n",
      "['N', 'has']\n",
      "['N', 'only']\n",
      "['N', 'ten']\n",
      "['N', 'schools']\n",
      "['N', 'and']\n",
      "['N', 'represents']\n",
      "['N', 'a']\n",
      "['N', 'different']\n",
      "['N', 'region']\n",
      "['N', 'of']\n",
      "['N', 'the']\n",
      "['N', 'country']\n",
      "['N', '.']\n",
      "its twelve member institutions (which are primarily flagship research universities in their respective states, well-regarded academically, and with relatively large student enrollment) are located primarily in the midwest , stretching from nebraska in the west to penn state in the east.\n"
     ]
    }
   ],
   "source": [
    "# check if tags are good\n",
    "def testing_tokens(ind, labels, documents, answers):\n",
    "    for i,j in zip(labels[ind],word_tokenize(documents['Document'][ind])):\n",
    "        print([i,j])\n",
    "    print(answers['Answer'][ind])\n",
    "testing_tokens(100 , train_doc_ans_labels, train_documents, train_answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a294cdb",
   "metadata": {},
   "source": [
    "Cleaned Documents: train and test\n",
    "\n",
    "train_answers - contains the ['Question','DocumentID','Answer'] \n",
    "\n",
    "train_documents - contains the ['DocumentID','Document']\n",
    "\n",
    "train_doc_ans_labels - contains a list of list of answer tags for each document, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83ed3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare the document for word embeddings:\n",
    "train_doc_ques = pd.DataFrame({'Document': train_documents['Document'],\n",
    "                               'Question': train_answers['Question']})\n",
    "test_doc_ques = pd.DataFrame({'Document': test_documents['Document'],\n",
    "                               'Question': test_answers['Question']})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "453a01e5",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "To use the CBOW model, we need the data in sentences. Extract this from the original dataset, don't use sent_tokenise, will mess with some of the fullstops, we want to maintain structure from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "99ba3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokens(data):\n",
    "    sentence_list = []\n",
    "    for i in range(len(data)):\n",
    "        sentence_list.append(word_tokenize(data[i]))\n",
    "    return(sentence_list)\n",
    "train_doc_list = word_tokens(train_doc_ques['Document'])\n",
    "train_ques_list = word_tokens(train_doc_ques['Question'])\n",
    "test_doc_list = word_tokens(test_doc_ques['Document'])\n",
    "test_ques_list = word_tokens(test_doc_ques['Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c491320",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = train_doc_list + train_ques_list + test_doc_list + test_ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "acb5de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_cbow_model = Word2Vec(sentences=combined_text, vector_size=100, window=5, min_count=1, workers=2, epochs=30)\n",
    "wc_cbow_model.save(\"cbow.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f8cdd66",
   "metadata": {},
   "source": [
    "To implement QA\n",
    "\n",
    "1. Word Embeddings, using CBOW\n",
    "2. Feature Extraction 1 - POS tags\n",
    "3. Feature Extraction 2 - TF-IDF \n",
    "4. Feature Extraction 3 - NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee6b4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(doc):\n",
    "    tokenized_doc = word_tokenize(doc)\n",
    "    embeddings = [wc_cbow_model.wv[word] for word in tokenized_doc]\n",
    "    return embeddings\n",
    "\n",
    "train_doc_ques['Doc_Embeddings'] = train_doc_ques['Document'].apply(get_word_embeddings)\n",
    "train_doc_ques['Q_Embeddings'] = train_doc_ques['Question'].apply(get_word_embeddings)\n",
    "test_doc_ques['Doc_Embeddings'] = test_doc_ques['Document'].apply(get_word_embeddings)\n",
    "test_doc_ques['Q_Embeddings'] = test_doc_ques['Question'].apply(get_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7da9a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_ques['Doc_Tokens'] = train_doc_ques['Document'].apply(word_tokenize)\n",
    "train_doc_ques['Q_Tokens'] =  train_doc_ques['Question'].apply(word_tokenize)\n",
    "test_doc_ques['Doc_Tokens'] = test_doc_ques['Document'].apply(word_tokenize)\n",
    "test_doc_ques['Q_Tokens'] = test_doc_ques['Question'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cdf6d45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_count(doc):\n",
    "    count = 0\n",
    "    for i in range(len(doc)):\n",
    "        if len(doc['Doc_Embeddings'][i]) != len(doc['Doc_Tokens'][i]):\n",
    "            count += 1\n",
    "        elif len(doc['Q_Embeddings'][i]) != len(doc['Q_Tokens'][i]):\n",
    "            count += 1\n",
    "        else:\n",
    "            continue\n",
    "    return(count)\n",
    "        \n",
    "check_count(train_doc_ques) # looks good"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46810415",
   "metadata": {},
   "source": [
    "Note, need to convert the POS tags, NER tags into embeddings. After this, pad the questions and answers to the max question/document length in the combined training and test set.\n",
    "\n",
    "### PoS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea21daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/finnmurphy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Apply the pos tags to the tokens \n",
    "from nltk.tag import pos_tag\n",
    "# download the dependency and resource as required\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "train_doc_ques['Doc_POS'] = train_doc_ques['Doc_Tokens'].apply(pos_tag)\n",
    "train_doc_ques['Q_POS'] =  train_doc_ques['Q_Tokens'].apply(pos_tag)\n",
    "test_doc_ques['Doc_POS'] = test_doc_ques['Doc_Tokens'].apply(pos_tag)\n",
    "test_doc_ques['Q_POS'] = test_doc_ques['Q_Tokens'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e62df735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how', 'WRB'),\n",
       " ('african', 'JJ'),\n",
       " ('americans', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('immigrated', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('us', 'PRP')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the POS tags: # looks ok\n",
    "test_doc_ques['Q_POS'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cfdef90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#': 0,\n",
       " '$': 1,\n",
       " \"''\": 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " ',': 5,\n",
       " '.': 6,\n",
       " ':': 7,\n",
       " 'CC': 8,\n",
       " 'CD': 9,\n",
       " 'DT': 10,\n",
       " 'EX': 11,\n",
       " 'FW': 12,\n",
       " 'IN': 13,\n",
       " 'JJ': 14,\n",
       " 'JJR': 15,\n",
       " 'JJS': 16,\n",
       " 'MD': 17,\n",
       " 'NN': 18,\n",
       " 'NNP': 19,\n",
       " 'NNPS': 20,\n",
       " 'NNS': 21,\n",
       " 'PDT': 22,\n",
       " 'POS': 23,\n",
       " 'PRP': 24,\n",
       " 'PRP$': 25,\n",
       " 'RB': 26,\n",
       " 'RBR': 27,\n",
       " 'RBS': 28,\n",
       " 'RP': 29,\n",
       " 'SYM': 30,\n",
       " 'TO': 31,\n",
       " 'UH': 32,\n",
       " 'VB': 33,\n",
       " 'VBD': 34,\n",
       " 'VBG': 35,\n",
       " 'VBN': 36,\n",
       " 'VBP': 37,\n",
       " 'VBZ': 38,\n",
       " 'WDT': 39,\n",
       " 'WP': 40,\n",
       " 'WP$': 41,\n",
       " 'WRB': 42,\n",
       " '``': 43}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all unique POS Tags\n",
    "all_pos_tags = train_doc_ques['Doc_POS'].tolist() + test_doc_ques['Doc_POS'].tolist() + train_doc_ques['Q_POS'].tolist() + test_doc_ques['Q_POS'].tolist()\n",
    "\n",
    "def get_unique_pos(data):\n",
    "    pos_tags = set()\n",
    "    for item in data:\n",
    "        for _,pos_tag in item:\n",
    "            pos_tags.add(pos_tag)\n",
    "\n",
    "    pos_tag_index = {tag: i for i, tag in enumerate(sorted(pos_tags))}\n",
    "    return pos_tag_index\n",
    "\n",
    "pos_iden = get_unique_pos(all_pos_tags) # list of tags\n",
    "pos_iden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00c06ede",
   "metadata": {},
   "source": [
    "### NER Tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d9b9a90",
   "metadata": {},
   "source": [
    "### Steps to run this:\n",
    "\n",
    "- pip install spacy \n",
    "- python -m spacy download en_core_web_sm\n",
    "\n",
    "If loaded for the first time, restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89121aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk using Spacy\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "# loading pre-trained model of NER\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab395e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tagging(texts):\n",
    "    tagged_texts = []\n",
    "    for text in texts:\n",
    "        doc = spacy.tokens.Doc(nlp.vocab, words=text)\n",
    "        nlp.get_pipe(\"ner\")(doc)\n",
    "        tagged_texts.append([(token.text, token.ent_type_) for token in doc])\n",
    "    return tagged_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "946fb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will take a while...\n",
    "train_doc_ques['Doc_NER'] = ner_tagging(train_doc_ques['Doc_Tokens'])\n",
    "train_doc_ques['Q_NER'] = ner_tagging(train_doc_ques['Q_Tokens'])\n",
    "test_doc_ques['Doc_NER'] = ner_tagging(test_doc_ques['Doc_Tokens'])\n",
    "test_doc_ques['Q_NER'] = ner_tagging(test_doc_ques['Q_Tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "25cba08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'CARDINAL': 1,\n",
       " 'DATE': 2,\n",
       " 'EVENT': 3,\n",
       " 'FAC': 4,\n",
       " 'GPE': 5,\n",
       " 'LANGUAGE': 6,\n",
       " 'LAW': 7,\n",
       " 'LOC': 8,\n",
       " 'MONEY': 9,\n",
       " 'NORP': 10,\n",
       " 'ORDINAL': 11,\n",
       " 'ORG': 12,\n",
       " 'PERCENT': 13,\n",
       " 'PERSON': 14,\n",
       " 'PRODUCT': 15,\n",
       " 'QUANTITY': 16,\n",
       " 'TIME': 17,\n",
       " 'WORK_OF_ART': 18}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar approach to the POS\n",
    "\n",
    "# Extract all unique POS Tags\n",
    "all_ner_tags = train_doc_ques['Doc_NER'].tolist() + test_doc_ques['Doc_NER'].tolist() + train_doc_ques['Q_NER'].tolist() + test_doc_ques['Q_NER'].tolist()\n",
    "\n",
    "def get_unique_ner(data):\n",
    "    ner_tags = set()\n",
    "    for item in data:\n",
    "        for _,ner_tag in item:\n",
    "            ner_tags.add(ner_tag)\n",
    "\n",
    "    ner_tag_index = {tag: i for i, tag in enumerate(sorted(ner_tags))}\n",
    "    return ner_tag_index\n",
    "\n",
    "ner_iden = get_unique_pos(all_ner_tags) # list of tags\n",
    "ner_iden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d28ac77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_idx = ner_iden.values()\n",
    "aa = np.eye(max(ner_idx) + 1)\n",
    "len(aa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd6fafd8",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "First, calculate the document frequency of each token in the entire corpus (training documents + testing documents). The result is a dictionary where each token is a key and its value is the document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "85be9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_frequency(corpus):\n",
    "    \"\"\"\n",
    "    Computes the document frequency for every token in the corpus. \n",
    "    Returns a dictionary {token: doc_freq, ...}\n",
    "    \"\"\"\n",
    "    document_frequency = {}\n",
    "    for document in corpus:\n",
    "        for token in np.unique(document):\n",
    "            try:\n",
    "                document_frequency[token] += 1\n",
    "            except:\n",
    "                document_frequency[token] = 1\n",
    "    return document_frequency\n",
    "\n",
    "train_corpus = train_doc_ques['Doc_Tokens'].tolist() + train_doc_ques['Q_Tokens'].tolist()\n",
    "test_corpus = test_doc_ques['Doc_Tokens'].tolist() + test_doc_ques['Q_Tokens'].tolist()\n",
    "train_doc_freq = document_frequency(train_corpus)\n",
    "test_doc_freq = document_frequency(test_corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0adc55c",
   "metadata": {},
   "source": [
    "Now calculate TF-IDF using the document frequency from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "10f840c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 62)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def compute_tf_idf(corpus, doc_frequency):\n",
    "    \"\"\"\n",
    "    Computes the term frequency inverse document frequency for every token in every document in the corpus.\n",
    "    \"\"\"\n",
    "    tf_idf = {}\n",
    "    tf_idf_list = []\n",
    "    N = len(doc_frequency)\n",
    "    doc_id = 0\n",
    "    for document in corpus:\n",
    "        tf_idf_doc = []\n",
    "        counter = Counter(document)\n",
    "        total_num_words = len(document)\n",
    "        for token in np.unique(document):\n",
    "            tf = counter[token] / total_num_words\n",
    "            df = doc_frequency[token]\n",
    "            idf = math.log(N / (df + 1)) + 1\n",
    "            tf_idf[doc_id, token] = tf * idf\n",
    "        for token in document:\n",
    "            tf_idf_doc.append(tf_idf[doc_id, token])\n",
    "        tf_idf_list.append(tf_idf_doc)\n",
    "        doc_id += 1\n",
    "    return tf_idf_list\n",
    "\n",
    "train_doc_tf_idf = compute_tf_idf(train_doc_ques['Doc_Tokens'].tolist(), train_doc_freq)\n",
    "train_q_tf_idf = compute_tf_idf(train_doc_ques['Q_Tokens'].tolist(), train_doc_freq)\n",
    "test_doc_tf_idf = compute_tf_idf(test_doc_ques['Doc_Tokens'].tolist(), test_doc_freq)\n",
    "test_q_tf_idf = compute_tf_idf(test_doc_ques['Q_Tokens'].tolist(), test_doc_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d807d2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Question</th>\n",
       "      <th>Doc_Embeddings</th>\n",
       "      <th>Q_Embeddings</th>\n",
       "      <th>Doc_Tokens</th>\n",
       "      <th>Q_Tokens</th>\n",
       "      <th>Doc_POS</th>\n",
       "      <th>Q_POS</th>\n",
       "      <th>Doc_NER</th>\n",
       "      <th>Q_NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a partly submerged glacier cave on perito more...</td>\n",
       "      <td>how are glacier caves formed?</td>\n",
       "      <td>[[2.930701, -0.3543262, 1.688855, -1.1701261, ...</td>\n",
       "      <td>[[-2.889528, -1.7223865, 0.31415024, -1.810862...</td>\n",
       "      <td>[a, partly, submerged, glacier, cave, on, peri...</td>\n",
       "      <td>[how, are, glacier, caves, formed, ?]</td>\n",
       "      <td>[(a, DT), (partly, RB), (submerged, VBN), (gla...</td>\n",
       "      <td>[(how, WRB), (are, VBP), (glacier, JJ), (caves...</td>\n",
       "      <td>[(a, ), (partly, ), (submerged, ), (glacier, )...</td>\n",
       "      <td>[(how, ), (are, ), (glacier, ), (caves, ), (fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in physics , circular motion is a movement of ...</td>\n",
       "      <td>how are the directions of the velocity and for...</td>\n",
       "      <td>[[-0.1628859, 0.003616946, 0.6113787, 0.151834...</td>\n",
       "      <td>[[-2.889528, -1.7223865, 0.31415024, -1.810862...</td>\n",
       "      <td>[in, physics, ,, circular, motion, is, a, move...</td>\n",
       "      <td>[how, are, the, directions, of, the, velocity,...</td>\n",
       "      <td>[(in, IN), (physics, NNS), (,, ,), (circular, ...</td>\n",
       "      <td>[(how, WRB), (are, VBP), (the, DT), (direction...</td>\n",
       "      <td>[(in, ), (physics, ), (,, ), (circular, ), (mo...</td>\n",
       "      <td>[(how, ), (are, ), (the, ), (directions, ), (o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apollo creed is a fictional character from the...</td>\n",
       "      <td>how did apollo creed die</td>\n",
       "      <td>[[0.31325287, 0.7915107, -0.22150037, -0.34180...</td>\n",
       "      <td>[[-2.889528, -1.7223865, 0.31415024, -1.810862...</td>\n",
       "      <td>[apollo, creed, is, a, fictional, character, f...</td>\n",
       "      <td>[how, did, apollo, creed, die]</td>\n",
       "      <td>[(apollo, NNS), (creed, VBP), (is, VBZ), (a, D...</td>\n",
       "      <td>[(how, WRB), (did, VBD), (apollo, VB), (creed,...</td>\n",
       "      <td>[(apollo, ORG), (creed, ), (is, ), (a, ), (fic...</td>\n",
       "      <td>[(how, ), (did, ), (apollo, ORG), (creed, ), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in the united states, the title of federal jud...</td>\n",
       "      <td>how long is the term for federal judges</td>\n",
       "      <td>[[-0.1628859, 0.003616946, 0.6113787, 0.151834...</td>\n",
       "      <td>[[-2.889528, -1.7223865, 0.31415024, -1.810862...</td>\n",
       "      <td>[in, the, united, states, ,, the, title, of, f...</td>\n",
       "      <td>[how, long, is, the, term, for, federal, judges]</td>\n",
       "      <td>[(in, IN), (the, DT), (united, JJ), (states, N...</td>\n",
       "      <td>[(how, WRB), (long, JJ), (is, VBZ), (the, DT),...</td>\n",
       "      <td>[(in, ), (the, GPE), (united, GPE), (states, G...</td>\n",
       "      <td>[(how, ), (long, ), (is, ), (the, ), (term, ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the beretta 21a bobcat is a small pocket-sized...</td>\n",
       "      <td>how a beretta model 21 pistols magazines works</td>\n",
       "      <td>[[0.5426823, -0.91372156, -0.005410878, 0.4399...</td>\n",
       "      <td>[[-2.889528, -1.7223865, 0.31415024, -1.810862...</td>\n",
       "      <td>[the, beretta, 21a, bobcat, is, a, small, pock...</td>\n",
       "      <td>[how, a, beretta, model, 21, pistols, magazine...</td>\n",
       "      <td>[(the, DT), (beretta, NN), (21a, CD), (bobcat,...</td>\n",
       "      <td>[(how, WRB), (a, DT), (beretta, NN), (model, N...</td>\n",
       "      <td>[(the, ), (beretta, ), (21a, ), (bobcat, ), (i...</td>\n",
       "      <td>[(how, ), (a, ), (beretta, PRODUCT), (model, )...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document   \n",
       "0  a partly submerged glacier cave on perito more...  \\\n",
       "1  in physics , circular motion is a movement of ...   \n",
       "2  apollo creed is a fictional character from the...   \n",
       "3  in the united states, the title of federal jud...   \n",
       "4  the beretta 21a bobcat is a small pocket-sized...   \n",
       "\n",
       "                                            Question   \n",
       "0                      how are glacier caves formed?  \\\n",
       "1  how are the directions of the velocity and for...   \n",
       "2                           how did apollo creed die   \n",
       "3            how long is the term for federal judges   \n",
       "4     how a beretta model 21 pistols magazines works   \n",
       "\n",
       "                                      Doc_Embeddings   \n",
       "0  [[2.930701, -0.3543262, 1.688855, -1.1701261, ...  \\\n",
       "1  [[-0.1628859, 0.003616946, 0.6113787, 0.151834...   \n",
       "2  [[0.31325287, 0.7915107, -0.22150037, -0.34180...   \n",
       "3  [[-0.1628859, 0.003616946, 0.6113787, 0.151834...   \n",
       "4  [[0.5426823, -0.91372156, -0.005410878, 0.4399...   \n",
       "\n",
       "                                        Q_Embeddings   \n",
       "0  [[-2.889528, -1.7223865, 0.31415024, -1.810862...  \\\n",
       "1  [[-2.889528, -1.7223865, 0.31415024, -1.810862...   \n",
       "2  [[-2.889528, -1.7223865, 0.31415024, -1.810862...   \n",
       "3  [[-2.889528, -1.7223865, 0.31415024, -1.810862...   \n",
       "4  [[-2.889528, -1.7223865, 0.31415024, -1.810862...   \n",
       "\n",
       "                                          Doc_Tokens   \n",
       "0  [a, partly, submerged, glacier, cave, on, peri...  \\\n",
       "1  [in, physics, ,, circular, motion, is, a, move...   \n",
       "2  [apollo, creed, is, a, fictional, character, f...   \n",
       "3  [in, the, united, states, ,, the, title, of, f...   \n",
       "4  [the, beretta, 21a, bobcat, is, a, small, pock...   \n",
       "\n",
       "                                            Q_Tokens   \n",
       "0              [how, are, glacier, caves, formed, ?]  \\\n",
       "1  [how, are, the, directions, of, the, velocity,...   \n",
       "2                     [how, did, apollo, creed, die]   \n",
       "3   [how, long, is, the, term, for, federal, judges]   \n",
       "4  [how, a, beretta, model, 21, pistols, magazine...   \n",
       "\n",
       "                                             Doc_POS   \n",
       "0  [(a, DT), (partly, RB), (submerged, VBN), (gla...  \\\n",
       "1  [(in, IN), (physics, NNS), (,, ,), (circular, ...   \n",
       "2  [(apollo, NNS), (creed, VBP), (is, VBZ), (a, D...   \n",
       "3  [(in, IN), (the, DT), (united, JJ), (states, N...   \n",
       "4  [(the, DT), (beretta, NN), (21a, CD), (bobcat,...   \n",
       "\n",
       "                                               Q_POS   \n",
       "0  [(how, WRB), (are, VBP), (glacier, JJ), (caves...  \\\n",
       "1  [(how, WRB), (are, VBP), (the, DT), (direction...   \n",
       "2  [(how, WRB), (did, VBD), (apollo, VB), (creed,...   \n",
       "3  [(how, WRB), (long, JJ), (is, VBZ), (the, DT),...   \n",
       "4  [(how, WRB), (a, DT), (beretta, NN), (model, N...   \n",
       "\n",
       "                                             Doc_NER   \n",
       "0  [(a, ), (partly, ), (submerged, ), (glacier, )...  \\\n",
       "1  [(in, ), (physics, ), (,, ), (circular, ), (mo...   \n",
       "2  [(apollo, ORG), (creed, ), (is, ), (a, ), (fic...   \n",
       "3  [(in, ), (the, GPE), (united, GPE), (states, G...   \n",
       "4  [(the, ), (beretta, ), (21a, ), (bobcat, ), (i...   \n",
       "\n",
       "                                               Q_NER  \n",
       "0  [(how, ), (are, ), (glacier, ), (caves, ), (fo...  \n",
       "1  [(how, ), (are, ), (the, ), (directions, ), (o...  \n",
       "2  [(how, ), (did, ), (apollo, ORG), (creed, ), (...  \n",
       "3  [(how, ), (long, ), (is, ), (the, ), (term, ),...  \n",
       "4  [(how, ), (a, ), (beretta, PRODUCT), (model, )...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc_ques.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60b20141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vectorize(pos_tagger, ner_tagger, data): # pass in the unique dict for ner or pos\n",
    "    pos_idx = pos_tagger.values()\n",
    "    pos_ohv = np.eye(max(pos_idx) + 1) # create the ohv\n",
    "    ner_idx = ner_tagger.values()\n",
    "    ner_ohv = np.eye(max(ner_idx) + 1)\n",
    "    \n",
    "    dpos_full_ohv, dner_full_ohv = [], [] # lists to append to \n",
    "    qpos_full_ohv, qner_full_ohv = [], [] # lists to append to\n",
    "\n",
    "    for item in data['Doc_POS']:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            pos_index_iden = pos_tagger[tag]\n",
    "            sent_ohv.append(pos_ohv[pos_index_iden])\n",
    "        dpos_full_ohv.append(sent_ohv)\n",
    "    \n",
    "    for item in data['Q_POS']:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            pos_index_iden = pos_tagger[tag]\n",
    "            sent_ohv.append(pos_ohv[pos_index_iden])\n",
    "        qpos_full_ohv.append(sent_ohv)\n",
    "    \n",
    "    for item in data['Doc_NER']:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            ner_index_iden = ner_tagger[tag]\n",
    "            sent_ohv.append(ner_ohv[ner_index_iden])\n",
    "        dner_full_ohv.append(sent_ohv)\n",
    "    \n",
    "    for item in data['Q_NER']:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            ner_index_iden = ner_tagger[tag]\n",
    "            sent_ohv.append(ner_ohv[ner_index_iden])\n",
    "        qner_full_ohv.append(sent_ohv)\n",
    "    \n",
    "    return(dpos_full_ohv, qpos_full_ohv, dner_full_ohv, qner_full_ohv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c766e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ohv for doc\n",
    "train_doc_pos_ohv, train_q_pos_ohv, train_doc_ner_ohv, train_q_ner_ohv = one_hot_vectorize(pos_iden, ner_iden, train_doc_ques)\n",
    "test_doc_pos_ohv, test_q_pos_ohv, test_doc_ner_ohv, test_q_ner_ohv = one_hot_vectorize(pos_iden, ner_iden, test_doc_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ffd8910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Question</th>\n",
       "      <th>Doc_Embeddings</th>\n",
       "      <th>Q_Embeddings</th>\n",
       "      <th>Doc_Tokens</th>\n",
       "      <th>Q_Tokens</th>\n",
       "      <th>Doc_POS</th>\n",
       "      <th>Q_POS</th>\n",
       "      <th>Doc_NER</th>\n",
       "      <th>Q_NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a partly submerged glacier cave on perito more...</td>\n",
       "      <td>how are glacier caves formed?</td>\n",
       "      <td>[[2.9428787, -0.752085, 0.5050353, -1.7412678,...</td>\n",
       "      <td>[[-3.5760026, -0.99570036, 0.27545172, -1.0006...</td>\n",
       "      <td>[a, partly, submerged, glacier, cave, on, peri...</td>\n",
       "      <td>[how, are, glacier, caves, formed, ?]</td>\n",
       "      <td>[(a, DT), (partly, RB), (submerged, VBN), (gla...</td>\n",
       "      <td>[(how, WRB), (are, VBP), (glacier, JJ), (caves...</td>\n",
       "      <td>[(a, ), (partly, ), (submerged, ), (glacier, )...</td>\n",
       "      <td>[(how, ), (are, ), (glacier, ), (caves, ), (fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in physics , circular motion is a movement of ...</td>\n",
       "      <td>how are the directions of the velocity and for...</td>\n",
       "      <td>[[-0.6306072, 0.5474009, 1.064857, -1.73173, -...</td>\n",
       "      <td>[[-3.5760026, -0.99570036, 0.27545172, -1.0006...</td>\n",
       "      <td>[in, physics, ,, circular, motion, is, a, move...</td>\n",
       "      <td>[how, are, the, directions, of, the, velocity,...</td>\n",
       "      <td>[(in, IN), (physics, NNS), (,, ,), (circular, ...</td>\n",
       "      <td>[(how, WRB), (are, VBP), (the, DT), (direction...</td>\n",
       "      <td>[(in, ), (physics, ), (,, ), (circular, ), (mo...</td>\n",
       "      <td>[(how, ), (are, ), (the, ), (directions, ), (o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apollo creed is a fictional character from the...</td>\n",
       "      <td>how did apollo creed die</td>\n",
       "      <td>[[0.08698449, 0.91935104, -0.29889715, -0.3628...</td>\n",
       "      <td>[[-3.5760026, -0.99570036, 0.27545172, -1.0006...</td>\n",
       "      <td>[apollo, creed, is, a, fictional, character, f...</td>\n",
       "      <td>[how, did, apollo, creed, die]</td>\n",
       "      <td>[(apollo, NNS), (creed, VBP), (is, VBZ), (a, D...</td>\n",
       "      <td>[(how, WRB), (did, VBD), (apollo, VB), (creed,...</td>\n",
       "      <td>[(apollo, ORG), (creed, ), (is, ), (a, ), (fic...</td>\n",
       "      <td>[(how, ), (did, ), (apollo, ORG), (creed, ), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in the united states, the title of federal jud...</td>\n",
       "      <td>how long is the term for federal judges</td>\n",
       "      <td>[[-0.6306072, 0.5474009, 1.064857, -1.73173, -...</td>\n",
       "      <td>[[-3.5760026, -0.99570036, 0.27545172, -1.0006...</td>\n",
       "      <td>[in, the, united, states, ,, the, title, of, f...</td>\n",
       "      <td>[how, long, is, the, term, for, federal, judges]</td>\n",
       "      <td>[(in, IN), (the, DT), (united, JJ), (states, N...</td>\n",
       "      <td>[(how, WRB), (long, JJ), (is, VBZ), (the, DT),...</td>\n",
       "      <td>[(in, ), (the, GPE), (united, GPE), (states, G...</td>\n",
       "      <td>[(how, ), (long, ), (is, ), (the, ), (term, ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the beretta 21a bobcat is a small pocket-sized...</td>\n",
       "      <td>how a beretta model 21 pistols magazines works</td>\n",
       "      <td>[[0.65172535, -1.1209415, 0.3399397, -0.155244...</td>\n",
       "      <td>[[-3.5760026, -0.99570036, 0.27545172, -1.0006...</td>\n",
       "      <td>[the, beretta, 21a, bobcat, is, a, small, pock...</td>\n",
       "      <td>[how, a, beretta, model, 21, pistols, magazine...</td>\n",
       "      <td>[(the, DT), (beretta, NN), (21a, CD), (bobcat,...</td>\n",
       "      <td>[(how, WRB), (a, DT), (beretta, NN), (model, N...</td>\n",
       "      <td>[(the, ), (beretta, ), (21a, ), (bobcat, ), (i...</td>\n",
       "      <td>[(how, ), (a, ), (beretta, PRODUCT), (model, )...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document   \n",
       "0  a partly submerged glacier cave on perito more...  \\\n",
       "1  in physics , circular motion is a movement of ...   \n",
       "2  apollo creed is a fictional character from the...   \n",
       "3  in the united states, the title of federal jud...   \n",
       "4  the beretta 21a bobcat is a small pocket-sized...   \n",
       "\n",
       "                                            Question   \n",
       "0                      how are glacier caves formed?  \\\n",
       "1  how are the directions of the velocity and for...   \n",
       "2                           how did apollo creed die   \n",
       "3            how long is the term for federal judges   \n",
       "4     how a beretta model 21 pistols magazines works   \n",
       "\n",
       "                                      Doc_Embeddings   \n",
       "0  [[2.9428787, -0.752085, 0.5050353, -1.7412678,...  \\\n",
       "1  [[-0.6306072, 0.5474009, 1.064857, -1.73173, -...   \n",
       "2  [[0.08698449, 0.91935104, -0.29889715, -0.3628...   \n",
       "3  [[-0.6306072, 0.5474009, 1.064857, -1.73173, -...   \n",
       "4  [[0.65172535, -1.1209415, 0.3399397, -0.155244...   \n",
       "\n",
       "                                        Q_Embeddings   \n",
       "0  [[-3.5760026, -0.99570036, 0.27545172, -1.0006...  \\\n",
       "1  [[-3.5760026, -0.99570036, 0.27545172, -1.0006...   \n",
       "2  [[-3.5760026, -0.99570036, 0.27545172, -1.0006...   \n",
       "3  [[-3.5760026, -0.99570036, 0.27545172, -1.0006...   \n",
       "4  [[-3.5760026, -0.99570036, 0.27545172, -1.0006...   \n",
       "\n",
       "                                          Doc_Tokens   \n",
       "0  [a, partly, submerged, glacier, cave, on, peri...  \\\n",
       "1  [in, physics, ,, circular, motion, is, a, move...   \n",
       "2  [apollo, creed, is, a, fictional, character, f...   \n",
       "3  [in, the, united, states, ,, the, title, of, f...   \n",
       "4  [the, beretta, 21a, bobcat, is, a, small, pock...   \n",
       "\n",
       "                                            Q_Tokens   \n",
       "0              [how, are, glacier, caves, formed, ?]  \\\n",
       "1  [how, are, the, directions, of, the, velocity,...   \n",
       "2                     [how, did, apollo, creed, die]   \n",
       "3   [how, long, is, the, term, for, federal, judges]   \n",
       "4  [how, a, beretta, model, 21, pistols, magazine...   \n",
       "\n",
       "                                             Doc_POS   \n",
       "0  [(a, DT), (partly, RB), (submerged, VBN), (gla...  \\\n",
       "1  [(in, IN), (physics, NNS), (,, ,), (circular, ...   \n",
       "2  [(apollo, NNS), (creed, VBP), (is, VBZ), (a, D...   \n",
       "3  [(in, IN), (the, DT), (united, JJ), (states, N...   \n",
       "4  [(the, DT), (beretta, NN), (21a, CD), (bobcat,...   \n",
       "\n",
       "                                               Q_POS   \n",
       "0  [(how, WRB), (are, VBP), (glacier, JJ), (caves...  \\\n",
       "1  [(how, WRB), (are, VBP), (the, DT), (direction...   \n",
       "2  [(how, WRB), (did, VBD), (apollo, VB), (creed,...   \n",
       "3  [(how, WRB), (long, JJ), (is, VBZ), (the, DT),...   \n",
       "4  [(how, WRB), (a, DT), (beretta, NN), (model, N...   \n",
       "\n",
       "                                             Doc_NER   \n",
       "0  [(a, ), (partly, ), (submerged, ), (glacier, )...  \\\n",
       "1  [(in, ), (physics, ), (,, ), (circular, ), (mo...   \n",
       "2  [(apollo, ORG), (creed, ), (is, ), (a, ), (fic...   \n",
       "3  [(in, ), (the, GPE), (united, GPE), (states, G...   \n",
       "4  [(the, ), (beretta, ), (21a, ), (bobcat, ), (i...   \n",
       "\n",
       "                                               Q_NER  \n",
       "0  [(how, ), (are, ), (glacier, ), (caves, ), (fo...  \n",
       "1  [(how, ), (are, ), (the, ), (directions, ), (o...  \n",
       "2  [(how, ), (did, ), (apollo, ORG), (creed, ), (...  \n",
       "3  [(how, ), (long, ), (is, ), (the, ), (term, ),...  \n",
       "4  [(how, ), (a, ), (beretta, PRODUCT), (model, )...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc_ques[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68e412d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dataframe to just tokens and embeddings:\n",
    "doc_emb_train = train_doc_ques[['Doc_Tokens','Doc_Embeddings']]\n",
    "doc_pos_ner = pd.DataFrame({'Doc_POS':train_doc_pos_ohv,\n",
    "              'Doc_NER':train_doc_ner_ohv})\n",
    "doc_emb_train = pd.concat([doc_emb_train, doc_pos_ner], axis=1)\n",
    "\n",
    "q_emb_train = train_doc_ques[['Q_Tokens','Q_Embeddings']]\n",
    "q_pos_ner = pd.DataFrame({'Q_POS':train_q_pos_ohv,\n",
    "              'Q_NER':train_q_ner_ohv})\n",
    "q_emb_train = pd.concat([q_emb_train, q_pos_ner], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d10ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb_test = test_doc_ques[['Doc_Tokens','Doc_Embeddings']]\n",
    "doc_pos_ner = pd.DataFrame({'Doc_POS':test_doc_pos_ohv,\n",
    "              'Doc_NER':test_doc_ner_ohv})\n",
    "doc_emb_test = pd.concat([doc_emb_test, doc_pos_ner], axis=1)\n",
    "\n",
    "q_emb_test = test_doc_ques[['Q_Tokens','Q_Embeddings']]\n",
    "q_pos_ner = pd.DataFrame({'Q_POS':test_q_pos_ohv,\n",
    "              'Q_NER':test_q_ner_ohv})\n",
    "q_emb_test = pd.concat([q_emb_test, q_pos_ner], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10ad89b3",
   "metadata": {},
   "source": [
    "### Word Embeddings (Doc and Qn)\n",
    "- Still have to add TF-IDF.\n",
    "\n",
    "The embeddings of the questions and answers of the train and test set can be found here:\n",
    "\n",
    "- Train Document - doc_emb_train\n",
    "- Train Q - q_emb_train\n",
    "- Test Document - doc_emb_test\n",
    "- Test Q - q_emb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d1a059d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q_Tokens</th>\n",
       "      <th>Q_Embeddings</th>\n",
       "      <th>Q_POS</th>\n",
       "      <th>Q_NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[how, african, americans, were, immigrated, to...</td>\n",
       "      <td>[[-3.2678661, -1.5818017, 0.79780596, -2.83997...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[how, large, were, early, jails]</td>\n",
       "      <td>[[-3.2678661, -1.5818017, 0.79780596, -2.83997...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[how, a, water, pump, works]</td>\n",
       "      <td>[[-3.2678661, -1.5818017, 0.79780596, -2.83997...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[how, old, was, sue, lyon, when, she, made, lo...</td>\n",
       "      <td>[[-3.2678661, -1.5818017, 0.79780596, -2.83997...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[how, are, antibodies, used, in]</td>\n",
       "      <td>[[-3.2678661, -1.5818017, 0.79780596, -2.83997...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>[where, is, the, brisket, from]</td>\n",
       "      <td>[[-0.8713485, 1.0382011, -0.09509377, 0.432915...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>[what, is, arm, chipset]</td>\n",
       "      <td>[[-0.4813437, -3.4788215, -2.8871129, -1.69282...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>[what, is, the, life, span, of, june, bugs]</td>\n",
       "      <td>[[-0.4813437, -3.4788215, -2.8871129, -1.69282...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>[who, is, the, youngest, female, to, give, bir...</td>\n",
       "      <td>[[0.9287824, -1.5731605, 2.5839703, -3.4665534...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>[what, is, an, open, mare, ?]</td>\n",
       "      <td>[[-0.4813437, -3.4788215, -2.8871129, -1.69282...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Q_Tokens  \\\n",
       "0    [how, african, americans, were, immigrated, to...   \n",
       "1                     [how, large, were, early, jails]   \n",
       "2                         [how, a, water, pump, works]   \n",
       "3    [how, old, was, sue, lyon, when, she, made, lo...   \n",
       "4                     [how, are, antibodies, used, in]   \n",
       "..                                                 ...   \n",
       "625                    [where, is, the, brisket, from]   \n",
       "626                           [what, is, arm, chipset]   \n",
       "627        [what, is, the, life, span, of, june, bugs]   \n",
       "628  [who, is, the, youngest, female, to, give, bir...   \n",
       "629                      [what, is, an, open, mare, ?]   \n",
       "\n",
       "                                          Q_Embeddings  \\\n",
       "0    [[-3.2678661, -1.5818017, 0.79780596, -2.83997...   \n",
       "1    [[-3.2678661, -1.5818017, 0.79780596, -2.83997...   \n",
       "2    [[-3.2678661, -1.5818017, 0.79780596, -2.83997...   \n",
       "3    [[-3.2678661, -1.5818017, 0.79780596, -2.83997...   \n",
       "4    [[-3.2678661, -1.5818017, 0.79780596, -2.83997...   \n",
       "..                                                 ...   \n",
       "625  [[-0.8713485, 1.0382011, -0.09509377, 0.432915...   \n",
       "626  [[-0.4813437, -3.4788215, -2.8871129, -1.69282...   \n",
       "627  [[-0.4813437, -3.4788215, -2.8871129, -1.69282...   \n",
       "628  [[0.9287824, -1.5731605, 2.5839703, -3.4665534...   \n",
       "629  [[-0.4813437, -3.4788215, -2.8871129, -1.69282...   \n",
       "\n",
       "                                                 Q_POS  \\\n",
       "0    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "4    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "..                                                 ...   \n",
       "625  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "626  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "627  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "628  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "629  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                                 Q_NER  \n",
       "0    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "..                                                 ...  \n",
       "625  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "626  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "627  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "628  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "629  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "\n",
       "[630 rows x 4 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_emb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "de8ada31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1755"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max_length of the document\n",
    "max_len_doc = 0\n",
    "for i in doc_emb_train['Doc_Tokens']:\n",
    "    if len(i) > max_len_doc:\n",
    "        max_len_doc = len(i)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "for i in doc_emb_test['Doc_Tokens']:\n",
    "    if len(i) > max_len_doc:\n",
    "        max_len_doc = len(i)\n",
    "    else:\n",
    "        continue\n",
    "max_len_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "56ced293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max_length of question\n",
    "max_len_qn = 0\n",
    "for i in q_emb_train['Q_Tokens']:\n",
    "    if len(i) > max_len_qn:\n",
    "        max_len_qn = len(i)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "for i in q_emb_test['Q_Tokens']:\n",
    "    if len(i) > max_len_qn:\n",
    "        max_len_qn = len(i)\n",
    "    else:\n",
    "        continue\n",
    "max_len_qn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2dd8cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = test_doc_ques[-2:]['Question'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "5e7e5ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'an', 'open', 'mare', '?']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
