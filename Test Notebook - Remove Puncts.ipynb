{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528971ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3cb696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data\n",
    "train_data = pd.read_csv('WikiQA-train.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('WikiQA-test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad5982",
   "metadata": {},
   "source": [
    "Extract the unique questions from the train and test data frames, including the documentID and the DocumentTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034c4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_documenttag(data):\n",
    "    qd = data[['Question', 'QuestionID', 'DocumentID','DocumentTitle']].drop_duplicates()\n",
    "    return qd\n",
    "train_question_doctag = get_questions_documenttag(train_data)\n",
    "test_question_doctag = get_questions_documenttag(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368895a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique questions\n",
    "train_questions = train_question_doctag['Question']\n",
    "test_questions = test_question_doctag['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f8f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique document ids\n",
    "train_docid = train_question_doctag['DocumentID']\n",
    "test_docid = test_question_doctag['DocumentID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf4306",
   "metadata": {},
   "source": [
    "Extract the answers to those questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e2d6a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(data, questions, documentids): \n",
    "    answers = [] # list of answers\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q] # add the document id\n",
    "        df = data[data['Question'] == question]\n",
    "        index = df.loc[df['Label'] == 1]['Sentence'].index.values\n",
    "        if len(index) == 0: # if no answer found\n",
    "            answers.append([question, doc_id, 'No answer'])\n",
    "        else: # if 1 answer found\n",
    "            answers.append([question, doc_id, df.loc[index[0], \"Sentence\"]])\n",
    "    return answers\n",
    "\n",
    "train_answers = pd.DataFrame(get_answers(train_data, train_questions, train_docid))\n",
    "test_answers = pd.DataFrame(get_answers(test_data, test_questions, test_docid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e5527b",
   "metadata": {},
   "source": [
    "The above get_answers returns train_answers and test_answers which, gives us in the following columns\n",
    "- Question\n",
    "- Related Document ID\n",
    "- Answer (if no answer to that question, return no answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ac4567fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(data, questions, documentids): # (done by Finn, tweaked by Dan)\n",
    "    documents = []\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q] # add the document id\n",
    "        df = data[data['Question'] == question]\n",
    "        sentences = df['Sentence'].tolist()\n",
    "        for i in range(0, len(sentences) - 1):\n",
    "            sentences[i] = sentences[i] + ' '\n",
    "        documents.append([doc_id,''.join(sentences)])\n",
    "    return documents\n",
    "\n",
    "train_documents = pd.DataFrame(get_documents(train_data, train_questions, train_docid)) # return the individual document in list\n",
    "test_documents = pd.DataFrame(get_documents(test_data, test_questions, test_docid)) # return the individual document in list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa7704",
   "metadata": {},
   "source": [
    "The above train_documents and test_documents called from the get_documents gives us in the following columns\n",
    "- Document ID\n",
    "- Full Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0432bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming all the columns for more standardised access\n",
    "train_answers.columns = ['Question','DocumentID','Answer']\n",
    "test_answers.columns = ['Question','DocumentID','Answer']\n",
    "train_documents.columns = ['DocumentID','Document']\n",
    "test_documents.columns = ['DocumentID','Document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "763141d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2117, 2117, 630, 630)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result is 2117, 2117, 630, 630\n",
    "\n",
    "len(train_answers),len(train_documents), len(test_answers),len(test_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5edcdf6",
   "metadata": {},
   "source": [
    "**Prior to tagging, we should maybe clean the document and answers first:** (stopped here)\n",
    "\n",
    "Maybe? \n",
    "- lowercase (might lose context, but we can use on questions)\n",
    "- removing any punctuation or weird symbols (do)\n",
    "- removal of stop words? (probably not)\n",
    "\n",
    "Make sure that the pre-processing is standardised to be the same throughout doc and ans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c5fddce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lower(text):\n",
    "    # Lowercase the text for question, answer and documents\n",
    "    text = text.lower()\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "train_answers[['Question', 'Answer']] = train_answers[['Question', 'Answer']].applymap(preprocess_lower)\n",
    "train_documents['Document'] = train_documents['Document'].apply(preprocess_lower)\n",
    "test_answers[['Question', 'Answer']] = test_answers[['Question', 'Answer']].applymap(preprocess_lower)\n",
    "test_documents['Document'] = test_documents['Document'].apply(preprocess_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3328fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelling(documents, answers):\n",
    "    tagged_documents = []\n",
    "    for q in range(len(answers)):\n",
    "        tagged_document = []\n",
    "        qn = answers['Question'].loc[q]\n",
    "        doc_id = answers['DocumentID'].loc[q]\n",
    "        content = documents.loc[documents['DocumentID'] == doc_id,'Document'].values[0]\n",
    "        answer = answers['Answer'].loc[q]\n",
    "\n",
    "        if answer == 'no answer':\n",
    "            tokens = word_tokenize(content)\n",
    "            for j in range(len(tokens)):\n",
    "                tagged_document.append('N') # none \n",
    "        else:\n",
    "            parts = content.partition(answer)\n",
    "            for j in range(len(parts)):\n",
    "                tokens = word_tokenize(parts[j])\n",
    "                if j == 1:\n",
    "                    tagged_document.append('S') # start of answer\n",
    "                    for k in range(len(tokens) - 2):\n",
    "                        tagged_document.append('I') # inside of answer\n",
    "                    tagged_document.append('E') # end of answer\n",
    "                else:\n",
    "                    for k in range(len(tokens)):\n",
    "                        tagged_document.append('N') # outside answer\n",
    "        tagged_documents.append(tagged_document)\n",
    "    return(tagged_documents)\n",
    "\n",
    "train_doc_ans_labels = labelling(train_documents, train_answers)\n",
    "test_doc_ans_labels = labelling(test_documents, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3fc6f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'the']\n",
      "['N', 'pineapple']\n",
      "['N', 'ananas']\n",
      "['N', 'comosus']\n",
      "['N', 'is']\n",
      "['N', 'a']\n",
      "['N', 'tropical']\n",
      "['N', 'plant']\n",
      "['N', 'with']\n",
      "['N', 'edible']\n",
      "['N', 'multiple']\n",
      "['N', 'fruit']\n",
      "['N', 'consisting']\n",
      "['N', 'of']\n",
      "['N', 'coalesced']\n",
      "['N', 'berries']\n",
      "['N', 'and']\n",
      "['N', 'the']\n",
      "['N', 'most']\n",
      "['N', 'economically']\n",
      "['N', 'significant']\n",
      "['N', 'plant']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'bromeliaceae']\n",
      "['N', 'family']\n",
      "['S', 'pineapples']\n",
      "['I', 'may']\n",
      "['I', 'be']\n",
      "['I', 'cultivated']\n",
      "['I', 'from']\n",
      "['I', 'a']\n",
      "['I', 'crown']\n",
      "['I', 'cutting']\n",
      "['I', 'of']\n",
      "['I', 'the']\n",
      "['I', 'fruit']\n",
      "['I', 'possibly']\n",
      "['I', 'flowering']\n",
      "['I', 'in']\n",
      "['I', '20']\n",
      "['I', '24']\n",
      "['I', 'months']\n",
      "['I', 'and']\n",
      "['I', 'fruiting']\n",
      "['I', 'in']\n",
      "['I', 'the']\n",
      "['I', 'following']\n",
      "['I', 'six']\n",
      "['E', 'months']\n",
      "['N', 'pineapple']\n",
      "['N', 'does']\n",
      "['N', 'not']\n",
      "['N', 'ripen']\n",
      "['N', 'significantly']\n",
      "['N', 'post']\n",
      "['N', 'harvest']\n",
      "['N', 'pineapples']\n",
      "['N', 'are']\n",
      "['N', 'consumed']\n",
      "['N', 'fresh']\n",
      "['N', 'cooked']\n",
      "['N', 'juiced']\n",
      "['N', 'and']\n",
      "['N', 'preserved']\n",
      "['N', 'and']\n",
      "['N', 'are']\n",
      "['N', 'found']\n",
      "['N', 'in']\n",
      "['N', 'a']\n",
      "['N', 'wide']\n",
      "['N', 'array']\n",
      "['N', 'of']\n",
      "['N', 'cuisines']\n",
      "['N', 'in']\n",
      "['N', 'addition']\n",
      "['N', 'to']\n",
      "['N', 'consumption']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'philippines']\n",
      "['N', 'the']\n",
      "['N', 'pineapple']\n",
      "['N', 's']\n",
      "['N', 'leaves']\n",
      "['N', 'are']\n",
      "['N', 'used']\n",
      "['N', 'to']\n",
      "['N', 'produce']\n",
      "['N', 'the']\n",
      "['N', 'textile']\n",
      "['N', 'fiber']\n",
      "['N', 'pi']\n",
      "['N', 'a']\n",
      "['N', 'employed']\n",
      "['N', 'as']\n",
      "['N', 'a']\n",
      "['N', 'component']\n",
      "['N', 'of']\n",
      "['N', 'wall']\n",
      "['N', 'paper']\n",
      "['N', 'and']\n",
      "['N', 'furnishings']\n",
      "['N', 'amongst']\n",
      "['N', 'other']\n",
      "['N', 'uses']\n",
      "pineapples may be cultivated from a crown cutting of the fruit  possibly flowering in 20 24 months and fruiting in the following six months \n"
     ]
    }
   ],
   "source": [
    "# check if tags are good\n",
    "def testing_tokens(ind, labels, documents, answers):\n",
    "    for i,j in zip(labels[ind],word_tokenize(documents['Document'][ind])):\n",
    "        print([i,j])\n",
    "    print(answers['Answer'][ind])\n",
    "testing_tokens(144, train_doc_ans_labels, train_documents, train_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a294cdb",
   "metadata": {},
   "source": [
    "Cleaned Documents: train and test\n",
    "\n",
    "train_answers - contains the ['Question','DocumentID','Answer'] \n",
    "\n",
    "train_documents - contains the ['DocumentID','Document']\n",
    "\n",
    "train_doc_ans_labels - contains a list of list of answer tags for each document, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "83ed3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare the document for word embeddings:\n",
    "train_doc_ques = pd.DataFrame({'Document': train_documents['Document'],\n",
    "                               'Question': train_answers['Question']})\n",
    "test_doc_ques = pd.DataFrame({'Document': test_documents['Document'],\n",
    "                               'Question': test_answers['Question']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a01e5",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "To use the CBOW model, we need the data in sentences. Extract this from the original dataset, don't use sent_tokenise, will mess with some of the fullstops, we want to maintain structure from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "99ba3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokens(data):\n",
    "    sentence_list = []\n",
    "    for i in range(len(data)):\n",
    "        sentence_list.append(word_tokenize(data[i]))\n",
    "    return(sentence_list)\n",
    "train_doc_list = word_tokens(train_doc_ques['Document'])\n",
    "train_ques_list = word_tokens(train_doc_ques['Question'])\n",
    "test_doc_list = word_tokens(test_doc_ques['Document'])\n",
    "test_ques_list = word_tokens(test_doc_ques['Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1c491320",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = train_doc_list + train_ques_list + test_doc_list + test_ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "acb5de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_cbow_model = Word2Vec(sentences=combined_text, vector_size=100, window=5, min_count=1, workers=2, epochs=30)\n",
    "wc_cbow_model.save(\"cbow.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8cdd66",
   "metadata": {},
   "source": [
    "To implement QA\n",
    "\n",
    "1. Word Embeddings, using CBOW\n",
    "2. Feature Extraction 1 - POS tags\n",
    "3. Feature Extraction 2 - TF-IDF \n",
    "4. Feature Extraction 3 - NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ee6b4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(doc):\n",
    "    tokenized_doc = word_tokenize(doc)\n",
    "    embeddings = [wc_cbow_model.wv[word] for word in tokenized_doc]\n",
    "    return embeddings\n",
    "\n",
    "train_doc_ques['Doc_Embeddings'] = train_doc_ques['Document'].apply(get_word_embeddings)\n",
    "train_doc_ques['Q_Embeddings'] = train_doc_ques['Question'].apply(get_word_embeddings)\n",
    "test_doc_ques['Doc_Embeddings'] = test_doc_ques['Document'].apply(get_word_embeddings)\n",
    "test_doc_ques['Q_Embeddings'] = test_doc_ques['Question'].apply(get_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7da9a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_ques['Doc_Tokens'] = train_doc_ques['Document'].apply(word_tokenize)\n",
    "train_doc_ques['Q_Tokens'] =  train_doc_ques['Question'].apply(word_tokenize)\n",
    "test_doc_ques['Doc_Tokens'] = test_doc_ques['Document'].apply(word_tokenize)\n",
    "test_doc_ques['Q_Tokens'] = test_doc_ques['Question'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "cdf6d45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_count(doc):\n",
    "    count = 0\n",
    "    for i in range(len(doc)):\n",
    "        if len(doc['Doc_Embeddings'][i]) != len(doc['Doc_Tokens'][i]):\n",
    "            count += 1\n",
    "        elif len(doc['Q_Embeddings'][i]) != len(doc['Q_Tokens'][i]):\n",
    "            count += 1\n",
    "        else:\n",
    "            continue\n",
    "    return(count)\n",
    "        \n",
    "check_count(train_doc_ques) # looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46810415",
   "metadata": {},
   "source": [
    "Note, need to convert the POS tags, NER tags into embeddings. After this, pad the questions and answers to the max question/document length in the combined training and test set.\n",
    "\n",
    "### PoS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ea21daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Apply the pos tags to the tokens \n",
    "from nltk.tag import pos_tag\n",
    "# download the dependency and resource as required\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "train_doc_ques['Doc_POS'] = train_doc_ques['Doc_Tokens'].apply(pos_tag)\n",
    "train_doc_ques['Q_POS'] =  train_doc_ques['Q_Tokens'].apply(pos_tag)\n",
    "test_doc_ques['Doc_POS'] = test_doc_ques['Doc_Tokens'].apply(pos_tag)\n",
    "test_doc_ques['Q_POS'] = test_doc_ques['Q_Tokens'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e62df735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how', 'WRB'),\n",
       " ('african', 'JJ'),\n",
       " ('americans', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('immigrated', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('us', 'PRP')]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the POS tags: # looks ok\n",
    "test_doc_ques['Q_POS'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cfdef90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$': 0,\n",
       " 'CC': 1,\n",
       " 'CD': 2,\n",
       " 'DT': 3,\n",
       " 'EX': 4,\n",
       " 'FW': 5,\n",
       " 'IN': 6,\n",
       " 'JJ': 7,\n",
       " 'JJR': 8,\n",
       " 'JJS': 9,\n",
       " 'MD': 10,\n",
       " 'NN': 11,\n",
       " 'NNP': 12,\n",
       " 'NNPS': 13,\n",
       " 'NNS': 14,\n",
       " 'PDT': 15,\n",
       " 'POS': 16,\n",
       " 'PRP': 17,\n",
       " 'PRP$': 18,\n",
       " 'RB': 19,\n",
       " 'RBR': 20,\n",
       " 'RBS': 21,\n",
       " 'RP': 22,\n",
       " 'SYM': 23,\n",
       " 'TO': 24,\n",
       " 'UH': 25,\n",
       " 'VB': 26,\n",
       " 'VBD': 27,\n",
       " 'VBG': 28,\n",
       " 'VBN': 29,\n",
       " 'VBP': 30,\n",
       " 'VBZ': 31,\n",
       " 'WDT': 32,\n",
       " 'WP': 33,\n",
       " 'WP$': 34,\n",
       " 'WRB': 35}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all unique POS Tags\n",
    "all_pos_tags = train_doc_ques['Doc_POS'].tolist() + test_doc_ques['Doc_POS'].tolist() + train_doc_ques['Q_POS'].tolist() + test_doc_ques['Q_POS'].tolist()\n",
    "\n",
    "def get_unique_pos(data):\n",
    "    pos_tags = set()\n",
    "    for item in data:\n",
    "        for _,pos_tag in item:\n",
    "            pos_tags.add(pos_tag)\n",
    "\n",
    "    pos_tag_index = {tag: i for i, tag in enumerate(sorted(pos_tags))}\n",
    "    return pos_tag_index\n",
    "\n",
    "pos_iden = get_unique_pos(all_pos_tags) # list of tags\n",
    "pos_iden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c06ede",
   "metadata": {},
   "source": [
    "### NER Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b9a90",
   "metadata": {},
   "source": [
    "### Steps to run this:\n",
    "\n",
    "- pip install spacy \n",
    "- python -m spacy download en_core_web_sm\n",
    "\n",
    "If loaded for the first time, restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "89121aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk using Spacy\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "# loading pre-trained model of NER\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ab395e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tagging(texts):\n",
    "    tagged_texts = []\n",
    "    for text in texts:\n",
    "        doc = spacy.tokens.Doc(nlp.vocab, words=text)\n",
    "        nlp.get_pipe(\"ner\")(doc)\n",
    "        tagged_texts.append([(token.text, token.ent_type_) for token in doc])\n",
    "    return tagged_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "946fb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will take a while...\n",
    "train_doc_ques['Doc_NER'] = ner_tagging(train_doc_ques['Doc_Tokens'])\n",
    "train_doc_ques['Q_NER'] = ner_tagging(train_doc_ques['Q_Tokens'])\n",
    "test_doc_ques['Doc_NER'] = ner_tagging(test_doc_ques['Doc_Tokens'])\n",
    "test_doc_ques['Q_NER'] = ner_tagging(test_doc_ques['Q_Tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "25cba08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'CARDINAL': 1,\n",
       " 'DATE': 2,\n",
       " 'EVENT': 3,\n",
       " 'FAC': 4,\n",
       " 'GPE': 5,\n",
       " 'LANGUAGE': 6,\n",
       " 'LAW': 7,\n",
       " 'LOC': 8,\n",
       " 'MONEY': 9,\n",
       " 'NORP': 10,\n",
       " 'ORDINAL': 11,\n",
       " 'ORG': 12,\n",
       " 'PERCENT': 13,\n",
       " 'PERSON': 14,\n",
       " 'PRODUCT': 15,\n",
       " 'QUANTITY': 16,\n",
       " 'TIME': 17,\n",
       " 'WORK_OF_ART': 18}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar approach to the POS\n",
    "\n",
    "# Extract all unique POS Tags\n",
    "all_ner_tags = train_doc_ques['Doc_NER'].tolist() + test_doc_ques['Doc_NER'].tolist() + train_doc_ques['Q_NER'].tolist() + test_doc_ques['Q_NER'].tolist()\n",
    "\n",
    "def get_unique_ner(data):\n",
    "    ner_tags = set()\n",
    "    for item in data:\n",
    "        for _,ner_tag in item:\n",
    "            ner_tags.add(ner_tag)\n",
    "\n",
    "    ner_tag_index = {tag: i for i, tag in enumerate(sorted(ner_tags))}\n",
    "    return ner_tag_index\n",
    "\n",
    "ner_iden = get_unique_pos(all_ner_tags) # list of tags\n",
    "ner_iden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6d28ac77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_idx = ner_iden.values()\n",
    "aa = np.eye(max(ner_idx) + 1)\n",
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "60b20141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vectorize(pos_tagger, ner_tagger, data): # pass in the unique dict for ner or pos\n",
    "    pos_idx = pos_tagger.values()\n",
    "    pos_ohv = np.eye(max(pos_idx) + 1) # create the ohv\n",
    "    ner_idx = ner_tagger.values()\n",
    "    ner_ohv = np.eye(max(ner_idx) + 1)\n",
    "    \n",
    "    dpos_full_ohv, dner_full_ohv = [], [] # lists to append to \n",
    "    qpos_full_ohv, qner_full_ohv = [], [] # lists to append to\n",
    "\n",
    "    for item in data['Doc_POS']:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            pos_index_iden = pos_tagger[tag]\n",
    "            sent_ohv.append(pos_ohv[pos_index_iden])\n",
    "        dpos_full_ohv.append(sent_ohv)\n",
    "    \n",
    "    for item in data['Q_POS']:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            pos_index_iden = pos_tagger[tag]\n",
    "            sent_ohv.append(pos_ohv[pos_index_iden])\n",
    "        qpos_full_ohv.append(sent_ohv)\n",
    "    \n",
    "    for item in data['Doc_NER']:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            ner_index_iden = ner_tagger[tag]\n",
    "            sent_ohv.append(ner_ohv[ner_index_iden])\n",
    "        dner_full_ohv.append(sent_ohv)\n",
    "    \n",
    "    for item in data['Q_NER']:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            ner_index_iden = ner_tagger[tag]\n",
    "            sent_ohv.append(ner_ohv[ner_index_iden])\n",
    "        qner_full_ohv.append(sent_ohv)\n",
    "    \n",
    "    return(dpos_full_ohv, qpos_full_ohv, dner_full_ohv, qner_full_ohv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c766e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ohv for doc\n",
    "train_doc_pos_ohv, train_q_pos_ohv, train_doc_ner_ohv, train_q_ner_ohv = one_hot_vectorize(pos_iden, ner_iden, train_doc_ques)\n",
    "test_doc_pos_ohv, test_q_pos_ohv, test_doc_ner_ohv, test_q_ner_ohv = one_hot_vectorize(pos_iden, ner_iden, test_doc_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0ffd8910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sidney',\n",
       " 'patrick',\n",
       " 'crosby',\n",
       " 'ons',\n",
       " 'born',\n",
       " 'august',\n",
       " '7',\n",
       " '1987',\n",
       " 'is',\n",
       " 'a',\n",
       " 'canadian',\n",
       " 'professional',\n",
       " 'ice',\n",
       " 'hockey',\n",
       " 'player',\n",
       " 'who',\n",
       " 'is',\n",
       " 'captain',\n",
       " 'of',\n",
       " 'the',\n",
       " 'pittsburgh',\n",
       " 'penguins',\n",
       " 'of',\n",
       " 'the',\n",
       " 'national',\n",
       " 'hockey',\n",
       " 'league',\n",
       " 'nhl',\n",
       " 'crosby',\n",
       " 'was',\n",
       " 'drafted',\n",
       " 'first',\n",
       " 'overall',\n",
       " 'by',\n",
       " 'the',\n",
       " 'penguins',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'quebec',\n",
       " 'major',\n",
       " 'junior',\n",
       " 'hockey',\n",
       " 'league',\n",
       " 'qmjhl',\n",
       " 'during',\n",
       " 'his',\n",
       " 'two',\n",
       " 'year',\n",
       " 'major',\n",
       " 'junior',\n",
       " 'career',\n",
       " 'with',\n",
       " 'the',\n",
       " 'rimouski',\n",
       " 'oc',\n",
       " 'anic',\n",
       " 'he',\n",
       " 'earned',\n",
       " 'back',\n",
       " 'to',\n",
       " 'back',\n",
       " 'chl',\n",
       " 'player',\n",
       " 'of',\n",
       " 'the',\n",
       " 'year',\n",
       " 'awards',\n",
       " 'and',\n",
       " 'led',\n",
       " 'his',\n",
       " 'club',\n",
       " 'to',\n",
       " 'the',\n",
       " '2005',\n",
       " 'memorial',\n",
       " 'cup',\n",
       " 'final',\n",
       " 'nicknamed',\n",
       " 'the',\n",
       " 'next',\n",
       " 'one',\n",
       " 'he',\n",
       " 'was',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'highly',\n",
       " 'regarded',\n",
       " 'draft',\n",
       " 'picks',\n",
       " 'in',\n",
       " 'hockey',\n",
       " 'history',\n",
       " 'leading',\n",
       " 'many',\n",
       " 'to',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " '2005',\n",
       " 'draft',\n",
       " 'lottery',\n",
       " 'as',\n",
       " 'the',\n",
       " 'sidney',\n",
       " 'crosby',\n",
       " 'sweepstakes',\n",
       " 'in',\n",
       " 'his',\n",
       " 'first',\n",
       " 'nhl',\n",
       " 'season',\n",
       " 'crosby',\n",
       " 'finished',\n",
       " 'sixth',\n",
       " 'in',\n",
       " 'league',\n",
       " 'scoring',\n",
       " 'with',\n",
       " '102',\n",
       " 'points',\n",
       " '39',\n",
       " 'goals',\n",
       " '63',\n",
       " 'assists',\n",
       " 'and',\n",
       " 'was',\n",
       " 'a',\n",
       " 'runner',\n",
       " 'up',\n",
       " 'for',\n",
       " 'the',\n",
       " 'calder',\n",
       " 'memorial',\n",
       " 'trophy',\n",
       " 'won',\n",
       " 'by',\n",
       " 'alexander',\n",
       " 'ovechkin',\n",
       " 'by',\n",
       " 'his',\n",
       " 'second',\n",
       " 'season',\n",
       " 'he',\n",
       " 'led',\n",
       " 'the',\n",
       " 'nhl',\n",
       " 'with',\n",
       " '120',\n",
       " 'points',\n",
       " '36',\n",
       " 'goals',\n",
       " '84',\n",
       " 'assists',\n",
       " 'to',\n",
       " 'capture',\n",
       " 'the',\n",
       " 'art',\n",
       " 'ross',\n",
       " 'trophy',\n",
       " 'becoming',\n",
       " 'the',\n",
       " 'youngest',\n",
       " 'player',\n",
       " 'and',\n",
       " 'the',\n",
       " 'only',\n",
       " 'teenager',\n",
       " 'to',\n",
       " 'win',\n",
       " 'a',\n",
       " 'scoring',\n",
       " 'title',\n",
       " 'in',\n",
       " 'any',\n",
       " 'major',\n",
       " 'north',\n",
       " 'american',\n",
       " 'sports',\n",
       " 'league',\n",
       " 'that',\n",
       " 'same',\n",
       " 'season',\n",
       " 'crosby',\n",
       " 'also',\n",
       " 'won',\n",
       " 'the',\n",
       " 'hart',\n",
       " 'memorial',\n",
       " 'trophy',\n",
       " 'as',\n",
       " 'the',\n",
       " 'professional',\n",
       " 'hockey',\n",
       " 'writers',\n",
       " 'association',\n",
       " 's',\n",
       " 'choice',\n",
       " 'for',\n",
       " 'most',\n",
       " 'valuable',\n",
       " 'player',\n",
       " 'and',\n",
       " 'the',\n",
       " 'lester',\n",
       " 'b',\n",
       " 'pearson',\n",
       " 'award',\n",
       " 'as',\n",
       " 'the',\n",
       " 'nhl',\n",
       " 'players',\n",
       " 'association',\n",
       " 's',\n",
       " 'choice',\n",
       " 'for',\n",
       " 'most',\n",
       " 'outstanding',\n",
       " 'player',\n",
       " 'becoming',\n",
       " 'the',\n",
       " 'seventh',\n",
       " 'player',\n",
       " 'in',\n",
       " 'nhl',\n",
       " 'history',\n",
       " 'to',\n",
       " 'earn',\n",
       " 'all',\n",
       " 'three',\n",
       " 'awards',\n",
       " 'in',\n",
       " 'one',\n",
       " 'year',\n",
       " 'crosby',\n",
       " 'started',\n",
       " 'the',\n",
       " '2007',\n",
       " '08',\n",
       " 'season',\n",
       " 'with',\n",
       " 'the',\n",
       " 'team',\n",
       " 's',\n",
       " 'captaincy',\n",
       " 'and',\n",
       " 'subsequently',\n",
       " 'led',\n",
       " 'them',\n",
       " 'to',\n",
       " 'the',\n",
       " '2008',\n",
       " 'stanley',\n",
       " 'cup',\n",
       " 'finals',\n",
       " 'where',\n",
       " 'they',\n",
       " 'were',\n",
       " 'defeated',\n",
       " 'by',\n",
       " 'the',\n",
       " 'detroit',\n",
       " 'red',\n",
       " 'wings',\n",
       " 'in',\n",
       " 'six',\n",
       " 'games',\n",
       " 'the',\n",
       " 'penguins',\n",
       " 'returned',\n",
       " 'to',\n",
       " 'the',\n",
       " 'finals',\n",
       " 'against',\n",
       " 'detroit',\n",
       " 'the',\n",
       " 'following',\n",
       " 'year',\n",
       " 'and',\n",
       " 'won',\n",
       " 'in',\n",
       " 'seven',\n",
       " 'games',\n",
       " 'crosby',\n",
       " 'became',\n",
       " 'the',\n",
       " 'youngest',\n",
       " 'captain',\n",
       " 'in',\n",
       " 'nhl',\n",
       " 'history',\n",
       " 'to',\n",
       " 'win',\n",
       " 'the',\n",
       " 'stanley',\n",
       " 'cup',\n",
       " 'in',\n",
       " 'the',\n",
       " '2009',\n",
       " '10',\n",
       " 'season',\n",
       " 'crosby',\n",
       " 'scored',\n",
       " 'a',\n",
       " 'career',\n",
       " 'high',\n",
       " '51',\n",
       " 'goals',\n",
       " 'tying',\n",
       " 'him',\n",
       " 'with',\n",
       " 'steven',\n",
       " 'stamkos',\n",
       " 'for',\n",
       " 'the',\n",
       " 'rocket',\n",
       " 'richard',\n",
       " 'trophy',\n",
       " 'as',\n",
       " 'the',\n",
       " 'league',\n",
       " 'leader',\n",
       " 'with',\n",
       " '58',\n",
       " 'assists',\n",
       " 'he',\n",
       " 'totaled',\n",
       " '109',\n",
       " 'points',\n",
       " 'second',\n",
       " 'in',\n",
       " 'the',\n",
       " 'nhl',\n",
       " 'during',\n",
       " 'the',\n",
       " 'off',\n",
       " 'season',\n",
       " 'crosby',\n",
       " 'received',\n",
       " 'the',\n",
       " 'mark',\n",
       " 'messier',\n",
       " 'leadership',\n",
       " 'award',\n",
       " 'in',\n",
       " '2010',\n",
       " '11',\n",
       " 'crosby',\n",
       " 'sustained',\n",
       " 'a',\n",
       " 'concussion',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " 'of',\n",
       " 'hits',\n",
       " 'to',\n",
       " 'the',\n",
       " 'head',\n",
       " 'in',\n",
       " 'back',\n",
       " 'to',\n",
       " 'back',\n",
       " 'games',\n",
       " 'the',\n",
       " 'injury',\n",
       " 'left',\n",
       " 'him',\n",
       " 'sidelined',\n",
       " 'for',\n",
       " 'ten',\n",
       " 'and',\n",
       " 'a',\n",
       " 'half',\n",
       " 'months',\n",
       " 'however',\n",
       " 'after',\n",
       " 'playing',\n",
       " 'eight',\n",
       " 'games',\n",
       " 'in',\n",
       " 'the',\n",
       " '2011',\n",
       " '12',\n",
       " 'season',\n",
       " 'crosby',\n",
       " 's',\n",
       " 'concussion',\n",
       " 'like',\n",
       " 'symptoms',\n",
       " 'returned',\n",
       " 'in',\n",
       " 'december',\n",
       " '2011',\n",
       " 'and',\n",
       " 'he',\n",
       " 'did',\n",
       " 'not',\n",
       " 'return',\n",
       " 'until',\n",
       " 'mid',\n",
       " 'march',\n",
       " '2012',\n",
       " 'internationally',\n",
       " 'crosby',\n",
       " 'has',\n",
       " 'represented',\n",
       " 'canada',\n",
       " 'in',\n",
       " 'numerous',\n",
       " 'tournaments',\n",
       " 'for',\n",
       " 'the',\n",
       " 'country',\n",
       " 's',\n",
       " 'junior',\n",
       " 'and',\n",
       " 'men',\n",
       " 's',\n",
       " 'teams',\n",
       " 'after',\n",
       " 'competing',\n",
       " 'in',\n",
       " 'the',\n",
       " '2003',\n",
       " 'u',\n",
       " '18',\n",
       " 'junior',\n",
       " 'world',\n",
       " 'cup',\n",
       " 'he',\n",
       " 'represented',\n",
       " 'canada',\n",
       " 'in',\n",
       " 'back',\n",
       " 'to',\n",
       " 'back',\n",
       " 'iihf',\n",
       " 'world',\n",
       " 'u20',\n",
       " 'championships',\n",
       " 'winning',\n",
       " 'silver',\n",
       " 'in',\n",
       " '2004',\n",
       " 'and',\n",
       " 'gold',\n",
       " 'in',\n",
       " '2005',\n",
       " 'at',\n",
       " 'the',\n",
       " '2006',\n",
       " 'iihf',\n",
       " 'world',\n",
       " 'championship',\n",
       " 'he',\n",
       " 'led',\n",
       " 'the',\n",
       " 'tournament',\n",
       " 'in',\n",
       " 'scoring',\n",
       " 'while',\n",
       " 'also',\n",
       " 'earning',\n",
       " 'top',\n",
       " 'forward',\n",
       " 'and',\n",
       " 'all',\n",
       " 'star',\n",
       " 'team',\n",
       " 'honours',\n",
       " 'four',\n",
       " 'years',\n",
       " 'later',\n",
       " 'crosby',\n",
       " 'was',\n",
       " 'named',\n",
       " 'to',\n",
       " 'team',\n",
       " 'canada',\n",
       " 'for',\n",
       " 'the',\n",
       " '2010',\n",
       " 'winter',\n",
       " 'olympics',\n",
       " 'in',\n",
       " 'vancouver',\n",
       " 'playing',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " 'in',\n",
       " 'the',\n",
       " 'gold',\n",
       " 'medal',\n",
       " 'game',\n",
       " 'he',\n",
       " 'scored',\n",
       " 'the',\n",
       " 'game',\n",
       " 'winning',\n",
       " 'goal',\n",
       " 'in',\n",
       " 'overtime']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc_ques[:300]['Doc_Tokens'][290]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "68e412d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dataframe to just tokens and embeddings:\n",
    "doc_emb_train = train_doc_ques[['Doc_Tokens','Doc_Embeddings']]\n",
    "doc_pos_ner = pd.DataFrame({'Doc_POS':train_doc_pos_ohv,\n",
    "              'Doc_NER':train_doc_ner_ohv})\n",
    "doc_emb_train = pd.concat([doc_emb_train, doc_pos_ner], axis=1)\n",
    "\n",
    "q_emb_train = train_doc_ques[['Q_Tokens','Q_Embeddings']]\n",
    "q_pos_ner = pd.DataFrame({'Q_POS':train_q_pos_ohv,\n",
    "              'Q_NER':train_q_ner_ohv})\n",
    "q_emb_train = pd.concat([q_emb_train, q_pos_ner], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6d10ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb_test = test_doc_ques[['Doc_Tokens','Doc_Embeddings']]\n",
    "doc_pos_ner = pd.DataFrame({'Doc_POS':test_doc_pos_ohv,\n",
    "              'Doc_NER':test_doc_ner_ohv})\n",
    "doc_emb_test = pd.concat([doc_emb_test, doc_pos_ner], axis=1)\n",
    "\n",
    "q_emb_test = test_doc_ques[['Q_Tokens','Q_Embeddings']]\n",
    "q_pos_ner = pd.DataFrame({'Q_POS':test_q_pos_ohv,\n",
    "              'Q_NER':test_q_ner_ohv})\n",
    "q_emb_test = pd.concat([q_emb_test, q_pos_ner], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad89b3",
   "metadata": {},
   "source": [
    "### Word Embeddings (Doc and Qn)\n",
    "- Still have to add TF-IDF.\n",
    "\n",
    "The embeddings of the questions and answers of the train and test set can be found here:\n",
    "\n",
    "- Train Document - doc_emb_train\n",
    "- Train Q - q_emb_train\n",
    "- Test Document - doc_emb_test\n",
    "- Test Q - q_emb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d1a059d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q_Tokens</th>\n",
       "      <th>Q_Embeddings</th>\n",
       "      <th>Q_POS</th>\n",
       "      <th>Q_NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[how, african, americans, were, immigrated, to...</td>\n",
       "      <td>[[-1.2972423, 0.61705947, -2.289424, 1.9959564...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[how, large, were, early, jails]</td>\n",
       "      <td>[[-1.2972423, 0.61705947, -2.289424, 1.9959564...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[how, a, water, pump, works]</td>\n",
       "      <td>[[-1.2972423, 0.61705947, -2.289424, 1.9959564...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[how, old, was, sue, lyon, when, she, made, lo...</td>\n",
       "      <td>[[-1.2972423, 0.61705947, -2.289424, 1.9959564...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[how, are, antibodies, used, in]</td>\n",
       "      <td>[[-1.2972423, 0.61705947, -2.289424, 1.9959564...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>[where, is, the, brisket, from]</td>\n",
       "      <td>[[-0.59588027, -1.4281421, 0.5773438, 1.183183...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>[what, is, arm, chipset]</td>\n",
       "      <td>[[-1.1253945, 0.032915913, -1.9591076, -2.8268...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>[what, is, the, life, span, of, june, bugs]</td>\n",
       "      <td>[[-1.1253945, 0.032915913, -1.9591076, -2.8268...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>[who, is, the, youngest, female, to, give, bir...</td>\n",
       "      <td>[[-1.9893861, 2.442811, 0.9638027, 0.842218, -...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>[what, is, an, open, mare]</td>\n",
       "      <td>[[-1.1253945, 0.032915913, -1.9591076, -2.8268...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Q_Tokens  \\\n",
       "0    [how, african, americans, were, immigrated, to...   \n",
       "1                     [how, large, were, early, jails]   \n",
       "2                         [how, a, water, pump, works]   \n",
       "3    [how, old, was, sue, lyon, when, she, made, lo...   \n",
       "4                     [how, are, antibodies, used, in]   \n",
       "..                                                 ...   \n",
       "625                    [where, is, the, brisket, from]   \n",
       "626                           [what, is, arm, chipset]   \n",
       "627        [what, is, the, life, span, of, june, bugs]   \n",
       "628  [who, is, the, youngest, female, to, give, bir...   \n",
       "629                         [what, is, an, open, mare]   \n",
       "\n",
       "                                          Q_Embeddings  \\\n",
       "0    [[-1.2972423, 0.61705947, -2.289424, 1.9959564...   \n",
       "1    [[-1.2972423, 0.61705947, -2.289424, 1.9959564...   \n",
       "2    [[-1.2972423, 0.61705947, -2.289424, 1.9959564...   \n",
       "3    [[-1.2972423, 0.61705947, -2.289424, 1.9959564...   \n",
       "4    [[-1.2972423, 0.61705947, -2.289424, 1.9959564...   \n",
       "..                                                 ...   \n",
       "625  [[-0.59588027, -1.4281421, 0.5773438, 1.183183...   \n",
       "626  [[-1.1253945, 0.032915913, -1.9591076, -2.8268...   \n",
       "627  [[-1.1253945, 0.032915913, -1.9591076, -2.8268...   \n",
       "628  [[-1.9893861, 2.442811, 0.9638027, 0.842218, -...   \n",
       "629  [[-1.1253945, 0.032915913, -1.9591076, -2.8268...   \n",
       "\n",
       "                                                 Q_POS  \\\n",
       "0    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "4    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "..                                                 ...   \n",
       "625  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "626  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "627  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "628  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "629  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                                 Q_NER  \n",
       "0    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "..                                                 ...  \n",
       "625  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "626  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "627  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "628  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "629  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "\n",
       "[630 rows x 4 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_emb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "de8ada31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1675"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max_length of the document\n",
    "max_len_doc = 0\n",
    "for i in doc_emb_train['Doc_Tokens']:\n",
    "    if len(i) > max_len_doc:\n",
    "        max_len_doc = len(i)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "for i in doc_emb_test['Doc_Tokens']:\n",
    "    if len(i) > max_len_doc:\n",
    "        max_len_doc = len(i)\n",
    "    else:\n",
    "        continue\n",
    "max_len_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "56ced293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max_length of question\n",
    "max_len_qn = 0\n",
    "for i in q_emb_train['Q_Tokens']:\n",
    "    if len(i) > max_len_qn:\n",
    "        max_len_qn = len(i)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "for i in q_emb_test['Q_Tokens']:\n",
    "    if len(i) > max_len_qn:\n",
    "        max_len_qn = len(i)\n",
    "    else:\n",
    "        continue\n",
    "max_len_qn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b6495079",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = train_doc_ques[['Doc_Tokens', 'Q_Tokens']]\n",
    "test_corpus = test_doc_ques[['Doc_Tokens', 'Q_Tokens']]\n",
    "\n",
    "# Flatten the lists in each row and concatenate them\n",
    "def get_squeeze(corpus):\n",
    "    combined_list = []\n",
    "    for _, row in corpus.iterrows():\n",
    "        combined_list.extend(row['Doc_Tokens'])\n",
    "        combined_list.extend(row['Q_Tokens'])\n",
    "    return(combined_list)\n",
    "df_training = get_squeeze(training_corpus)\n",
    "df_test = get_squeeze(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "5c4150b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize a default dictionary with int\n",
    "def diction(df):\n",
    "    keys = dict.fromkeys(set(df),0)\n",
    "    for token in df:\n",
    "        keys[token] += 1\n",
    "    return keys\n",
    "d_train, d_test = diction(df_training), diction(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "9d7cc245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'librarian': 1,\n",
       " 'industrialized': 1,\n",
       " 'wrong': 3,\n",
       " 'transverse': 2,\n",
       " 'thromboembolism': 2,\n",
       " 'unburied': 1,\n",
       " 'tyrese': 1,\n",
       " 'elect': 2,\n",
       " 'resulted': 14,\n",
       " 'ctenocephalides': 2,\n",
       " 'split': 11,\n",
       " 'less': 33,\n",
       " 'glutamine': 3,\n",
       " 'angrily': 1,\n",
       " '72': 3,\n",
       " 'simplified': 2,\n",
       " 'graph': 7,\n",
       " 'beverage': 3,\n",
       " 'wet': 1,\n",
       " 'litter': 1,\n",
       " 'robert': 18,\n",
       " 'chapters': 3,\n",
       " 'screen': 9,\n",
       " 'gangs': 1,\n",
       " 'co': 22,\n",
       " 'defendant': 2,\n",
       " 'price': 19,\n",
       " 'bipedal': 1,\n",
       " 'rugby': 3,\n",
       " '94': 1,\n",
       " 'chords': 5,\n",
       " 'populated': 5,\n",
       " 'solution': 7,\n",
       " 'superman': 10,\n",
       " 'despotism': 1,\n",
       " 'greater': 22,\n",
       " 'technical': 9,\n",
       " 'iec': 4,\n",
       " 'worldwide': 42,\n",
       " 'egretta': 1,\n",
       " 'kitchener': 1,\n",
       " 'garners': 1,\n",
       " 'sir': 8,\n",
       " 'biomedical': 4,\n",
       " 'apa': 2,\n",
       " 'outlaw': 3,\n",
       " 'addamses': 1,\n",
       " 'asserting': 1,\n",
       " 'bush': 9,\n",
       " 'conductor': 2,\n",
       " 'prehistoric': 4,\n",
       " 'criticizing': 1,\n",
       " 'regretted': 1,\n",
       " 'landmass': 2,\n",
       " 'portable': 2,\n",
       " 'parks': 5,\n",
       " 'unauthorized': 1,\n",
       " 'answering': 9,\n",
       " 'hyenas': 1,\n",
       " 'rockets': 3,\n",
       " '6': 45,\n",
       " 'arid': 1,\n",
       " '1847': 3,\n",
       " 'familiar': 3,\n",
       " 'respective': 4,\n",
       " 'altered': 1,\n",
       " 'mentally': 1,\n",
       " 'tuppence': 1,\n",
       " '1346': 1,\n",
       " 'periodontitis': 1,\n",
       " 'goddess': 4,\n",
       " 'bd': 3,\n",
       " 'eligible': 9,\n",
       " 'crusaders': 3,\n",
       " 'lucia': 1,\n",
       " 'latke': 1,\n",
       " 'communications': 9,\n",
       " 'physiological': 3,\n",
       " 'inception': 1,\n",
       " 'healing': 2,\n",
       " 'disputed': 2,\n",
       " 'profess': 1,\n",
       " 'mascot': 5,\n",
       " 'successes': 3,\n",
       " '1959': 10,\n",
       " 'armies': 6,\n",
       " 'madox': 1,\n",
       " 'phase': 7,\n",
       " 'vin': 1,\n",
       " 'merged': 3,\n",
       " 'valencia': 1,\n",
       " 'effects': 30,\n",
       " 'via': 11,\n",
       " 'jews': 1,\n",
       " 'helper': 1,\n",
       " 'anticipated': 2,\n",
       " 'seeds': 5,\n",
       " 'serious': 3,\n",
       " 'superintendent': 1,\n",
       " 'vitamin': 1,\n",
       " 'cheap': 2,\n",
       " 'explores': 1,\n",
       " 'lodged': 1,\n",
       " 'troposphere': 4,\n",
       " 'failing': 1,\n",
       " 'phenomenon': 3,\n",
       " 'phenotypes': 2,\n",
       " 'ceilings': 3,\n",
       " 'charles': 26,\n",
       " 'drymon': 2,\n",
       " 'prom': 1,\n",
       " 'cmm': 2,\n",
       " 'oversees': 3,\n",
       " 'wm': 1,\n",
       " 'principles': 9,\n",
       " 'seniors': 1,\n",
       " 'truly': 2,\n",
       " 'treated': 6,\n",
       " 'supplement': 1,\n",
       " 'colin': 1,\n",
       " 'spall': 1,\n",
       " 'shifted': 1,\n",
       " 'hastened': 2,\n",
       " 'mia': 3,\n",
       " 'emotionally': 1,\n",
       " 'depressions': 1,\n",
       " 'dartmoor': 1,\n",
       " 'respect': 4,\n",
       " 'hemolytic': 1,\n",
       " 'millennium': 3,\n",
       " 'outsiders': 1,\n",
       " '850': 1,\n",
       " 'steel': 3,\n",
       " '1885': 2,\n",
       " 'climbing': 2,\n",
       " 'german': 26,\n",
       " 'continuation': 1,\n",
       " 'groove': 2,\n",
       " 'missiles': 2,\n",
       " 'hanoi': 1,\n",
       " '1859': 4,\n",
       " 'infantry': 3,\n",
       " 'predicting': 1,\n",
       " 'otherworldly': 1,\n",
       " 'armstrong': 1,\n",
       " 'renewed': 10,\n",
       " 'got': 1,\n",
       " 'tat': 1,\n",
       " 'crete': 2,\n",
       " 'piven': 1,\n",
       " 'globe': 18,\n",
       " 'hydroelectricity': 5,\n",
       " 'breaks': 3,\n",
       " 'bait': 1,\n",
       " 'anatomist': 1,\n",
       " 'surges': 1,\n",
       " 'marrying': 1,\n",
       " 'graduates': 2,\n",
       " 'viewed': 4,\n",
       " 'liver': 9,\n",
       " 'kills': 1,\n",
       " 'derive': 1,\n",
       " 'recipients': 5,\n",
       " 'oval': 2,\n",
       " '337': 1,\n",
       " 'ejaculation': 1,\n",
       " 'organizing': 2,\n",
       " 'shorter': 3,\n",
       " 'license': 4,\n",
       " 'piece': 6,\n",
       " 'formulated': 2,\n",
       " 'keys': 14,\n",
       " 'gertie': 2,\n",
       " 'wise': 1,\n",
       " 'synthesizers': 1,\n",
       " 'mighty': 2,\n",
       " '1970s': 16,\n",
       " 'around': 67,\n",
       " 'bassist': 2,\n",
       " '805': 1,\n",
       " 'x': 19,\n",
       " 'lampoon': 1,\n",
       " 'broodmare': 2,\n",
       " 'window': 2,\n",
       " 'scream': 1,\n",
       " 'subfamily': 4,\n",
       " 'bulkier': 1,\n",
       " 'outline': 1,\n",
       " 'sio4': 1,\n",
       " 'vega': 1,\n",
       " 'conducts': 1,\n",
       " 'diz': 1,\n",
       " 'staters': 1,\n",
       " 'dov': 1,\n",
       " 'aeroelastic': 1,\n",
       " 'peasantry': 1,\n",
       " 'verses': 3,\n",
       " 'opening': 9,\n",
       " 'difficulties': 1,\n",
       " 'bafta': 2,\n",
       " 'max': 2,\n",
       " 'reign': 8,\n",
       " 'proposals': 2,\n",
       " 'individually': 1,\n",
       " 'nha': 1,\n",
       " 'predatory': 1,\n",
       " 'nabisco': 2,\n",
       " 'louisa': 1,\n",
       " 'debut': 18,\n",
       " 'truman': 16,\n",
       " 'methodology': 3,\n",
       " 'ii': 52,\n",
       " 'cory': 3,\n",
       " 'scholars': 5,\n",
       " 'manufacturer': 5,\n",
       " 'rerun': 1,\n",
       " 'annual': 12,\n",
       " 'services': 27,\n",
       " 'complement': 1,\n",
       " '700': 3,\n",
       " 'siblings': 1,\n",
       " 'obligee': 3,\n",
       " 'megabyte': 1,\n",
       " 'dorgon': 1,\n",
       " 'sdram': 1,\n",
       " 'emission': 2,\n",
       " 'bolingbrook': 2,\n",
       " 'sternum': 1,\n",
       " 'vocabulary': 2,\n",
       " 'mountbatten': 1,\n",
       " 'hotels': 4,\n",
       " 'cylindrical': 5,\n",
       " 'hydrops': 1,\n",
       " 'brigadier': 2,\n",
       " 'bolivia': 1,\n",
       " 'flavoring': 2,\n",
       " 'planetary': 7,\n",
       " 'event': 29,\n",
       " 'lubricant': 1,\n",
       " 'hidden': 2,\n",
       " 'raeburn': 1,\n",
       " 'romances': 3,\n",
       " 'christening': 2,\n",
       " 'interestingly': 1,\n",
       " 'endocrine': 1,\n",
       " 'cattedrale': 1,\n",
       " 'seen': 11,\n",
       " 'teacher': 1,\n",
       " 'injury': 4,\n",
       " 'shakespeare': 11,\n",
       " 'hispanophone': 3,\n",
       " 'rule': 31,\n",
       " 'disposal': 2,\n",
       " 'ready': 2,\n",
       " 'nirvana': 1,\n",
       " 'activities': 14,\n",
       " 'wood': 9,\n",
       " 'search': 8,\n",
       " 'suicidium': 1,\n",
       " 'celtic': 1,\n",
       " 'squared': 1,\n",
       " 'appropriated': 1,\n",
       " 'bringing': 3,\n",
       " '371': 1,\n",
       " 'our': 18,\n",
       " 'determining': 3,\n",
       " 'incorporates': 5,\n",
       " 'resumed': 1,\n",
       " 'stoll': 1,\n",
       " 'tetrahydrate': 1,\n",
       " 'destinations': 1,\n",
       " 'sexes': 2,\n",
       " 'resurgence': 1,\n",
       " 'priesthood': 1,\n",
       " 'liberation': 1,\n",
       " 'zincum': 1,\n",
       " 'ankle': 1,\n",
       " 'crusades': 9,\n",
       " 'reaching': 5,\n",
       " 'armoury': 1,\n",
       " 'pick': 2,\n",
       " 'know': 3,\n",
       " 'chip': 3,\n",
       " 'negotiable': 2,\n",
       " 'zar': 1,\n",
       " 'involute': 1,\n",
       " 'process': 53,\n",
       " 'gerhard': 1,\n",
       " 'remain': 8,\n",
       " 'debating': 4,\n",
       " 'skene': 1,\n",
       " 'spare': 1,\n",
       " 'rdf': 2,\n",
       " 'insist': 1,\n",
       " 'believed': 13,\n",
       " 'subdivision': 3,\n",
       " 'councils': 2,\n",
       " 'imitation': 1,\n",
       " 'cane': 2,\n",
       " 'banks': 6,\n",
       " 'gentlemen': 1,\n",
       " 'espn': 2,\n",
       " 'leitmotif': 3,\n",
       " 'dealt': 2,\n",
       " 'identifies': 5,\n",
       " 'wadsworth': 2,\n",
       " 'collisions': 2,\n",
       " 'cullen': 1,\n",
       " 'cosmonauts': 1,\n",
       " 'successful': 25,\n",
       " 'fascination': 1,\n",
       " 'winkler': 1,\n",
       " 'raids': 2,\n",
       " 'odds': 2,\n",
       " 'woodsman': 1,\n",
       " 'natural': 30,\n",
       " 'problem': 5,\n",
       " 'taurine': 1,\n",
       " 'tiers': 1,\n",
       " 'shelanu': 1,\n",
       " 'elections': 14,\n",
       " '270': 4,\n",
       " 'snicket': 11,\n",
       " 'botts': 1,\n",
       " 'samoa': 1,\n",
       " 'series': 197,\n",
       " 'egos': 1,\n",
       " 'kirk': 1,\n",
       " 'assisting': 1,\n",
       " 'rain': 4,\n",
       " 'models': 6,\n",
       " 'walden': 2,\n",
       " '1848': 3,\n",
       " 'landscape': 3,\n",
       " 'supplied': 1,\n",
       " 'prizes': 4,\n",
       " 'farther': 1,\n",
       " 'stella': 1,\n",
       " 'replaced': 17,\n",
       " 'zoey': 1,\n",
       " 'build': 8,\n",
       " 'renny': 1,\n",
       " 'image': 23,\n",
       " 'by': 1143,\n",
       " 'correct': 10,\n",
       " '57th': 1,\n",
       " 'violence': 4,\n",
       " 'ghanaian': 2,\n",
       " '1675': 1,\n",
       " 'arteries': 9,\n",
       " 'hawaiian': 1,\n",
       " 'saratoga': 1,\n",
       " 'sexton': 1,\n",
       " 'titanesses': 1,\n",
       " 'handle': 4,\n",
       " 'legged': 1,\n",
       " 'texaco': 6,\n",
       " 'hulu': 1,\n",
       " 'dumpsite': 1,\n",
       " 'vista': 1,\n",
       " 'departed': 3,\n",
       " 'domkirke': 2,\n",
       " 'burnett': 1,\n",
       " 'sailed': 1,\n",
       " '1971': 9,\n",
       " 'categories': 6,\n",
       " 'psychology': 4,\n",
       " 'ancestor': 5,\n",
       " 'collisional': 1,\n",
       " 'surpassed': 6,\n",
       " 'witherspoon': 2,\n",
       " 'exposition': 3,\n",
       " 'r': 25,\n",
       " 'fled': 1,\n",
       " 'inclusion': 3,\n",
       " '532': 2,\n",
       " 'lancelot': 2,\n",
       " '1968': 12,\n",
       " 'signs': 5,\n",
       " 'generating': 4,\n",
       " 'coatings': 1,\n",
       " 'esquire': 1,\n",
       " 'variation': 17,\n",
       " 'aldus': 2,\n",
       " 'newsome': 1,\n",
       " 'geologic': 1,\n",
       " 'there': 117,\n",
       " 'demoralized': 1,\n",
       " 'subset': 2,\n",
       " 'toulouse': 2,\n",
       " 'dominated': 8,\n",
       " 'hud': 1,\n",
       " 'johannes': 2,\n",
       " 'marriage': 18,\n",
       " 'ishbiliya': 1,\n",
       " 'stagnation': 2,\n",
       " 'artwork': 2,\n",
       " 'transformed': 5,\n",
       " 'true': 20,\n",
       " 'occur': 17,\n",
       " 'commonplace': 1,\n",
       " 'secretions': 2,\n",
       " 'clockwork': 1,\n",
       " 'superstructure': 1,\n",
       " 'steve': 7,\n",
       " 'charlaine': 1,\n",
       " 'precocious': 1,\n",
       " 'q780': 11,\n",
       " 'whist': 2,\n",
       " 'hamlets': 1,\n",
       " 'mostly': 16,\n",
       " 'hokkaido': 1,\n",
       " 'gb': 4,\n",
       " 'yam': 1,\n",
       " 'sloan': 6,\n",
       " 'policy': 14,\n",
       " 'elemental': 1,\n",
       " 'tremors': 1,\n",
       " 'acetylcholine': 4,\n",
       " 'responds': 1,\n",
       " 'guyton': 5,\n",
       " 'garrick': 1,\n",
       " 'inactivity': 1,\n",
       " 'poaching': 1,\n",
       " 'others': 32,\n",
       " 'expand': 3,\n",
       " 'shonda': 1,\n",
       " 'praises': 1,\n",
       " 'compute': 2,\n",
       " 'signing': 4,\n",
       " 'martinez': 3,\n",
       " 'recess': 1,\n",
       " 'indicates': 5,\n",
       " 'daily': 10,\n",
       " 'megatron': 1,\n",
       " 'reigns': 1,\n",
       " 'spc': 1,\n",
       " 'decided': 12,\n",
       " 'butter': 1,\n",
       " 'few': 31,\n",
       " 'carry': 15,\n",
       " 'marshaled': 1,\n",
       " 'moisture': 7,\n",
       " 'spectrum': 3,\n",
       " 'cranium': 1,\n",
       " 'elasticity': 3,\n",
       " 'monreale': 1,\n",
       " 'enemies': 5,\n",
       " 'palate': 1,\n",
       " 'things': 16,\n",
       " 'textiles': 1,\n",
       " 'autumn': 2,\n",
       " 'cdma': 2,\n",
       " 'mesoamerican': 1,\n",
       " 'fairs': 1,\n",
       " 'letterman': 2,\n",
       " 'latria': 1,\n",
       " 'criteria': 1,\n",
       " 'requirement': 3,\n",
       " 'particles': 9,\n",
       " 'balancing': 1,\n",
       " 'sample': 14,\n",
       " 'http': 3,\n",
       " 'accounted': 2,\n",
       " 'invertebrates': 2,\n",
       " 'deprecating': 1,\n",
       " 'bite': 1,\n",
       " 'validation': 1,\n",
       " 'cine': 1,\n",
       " 'bedrock': 1,\n",
       " 'nonconsensual': 1,\n",
       " 'klaus': 1,\n",
       " 'bon': 2,\n",
       " 'rubicam': 1,\n",
       " 'arachnids': 2,\n",
       " 'consul': 1,\n",
       " 'panther': 1,\n",
       " 'orlando': 2,\n",
       " 'directions': 4,\n",
       " 'dates': 4,\n",
       " 'stop': 9,\n",
       " 'asleep': 1,\n",
       " 'risks': 1,\n",
       " 'davis': 16,\n",
       " 'peripheral': 2,\n",
       " 'aug': 1,\n",
       " 'scrivener': 2,\n",
       " 'real': 32,\n",
       " 'inning': 1,\n",
       " 'afarensis': 1,\n",
       " 'distillery': 3,\n",
       " 'oust': 1,\n",
       " 'generosity': 1,\n",
       " 'middlefield': 1,\n",
       " 'climates': 1,\n",
       " 'enjoy': 4,\n",
       " 'photons': 1,\n",
       " 'dolphins': 4,\n",
       " 'having': 33,\n",
       " 'proceeds': 1,\n",
       " 'complicated': 3,\n",
       " 'participating': 2,\n",
       " 'tentatively': 1,\n",
       " 'dedication': 1,\n",
       " '1980s': 12,\n",
       " 'counterpart': 3,\n",
       " 'hode': 1,\n",
       " '1867': 4,\n",
       " 'collided': 1,\n",
       " 'bombings': 2,\n",
       " 'knosos': 1,\n",
       " 'cds': 1,\n",
       " 'snap': 1,\n",
       " 'centerpiece': 1,\n",
       " 'uses': 22,\n",
       " 'aesthetically': 2,\n",
       " 'albany': 6,\n",
       " 'usitatissimum': 1,\n",
       " '2003': 21,\n",
       " 'value': 10,\n",
       " 'official': 39,\n",
       " 'toshiba': 1,\n",
       " 'twarc': 1,\n",
       " 'betterment': 1,\n",
       " 'download': 1,\n",
       " 'corregidor': 1,\n",
       " 'litre': 2,\n",
       " 'protecting': 1,\n",
       " 'cutting': 3,\n",
       " 'detect': 2,\n",
       " '1493': 2,\n",
       " 'boulder': 1,\n",
       " 'monoacetylmorphine': 1,\n",
       " 'thirty': 6,\n",
       " '1944': 7,\n",
       " 'supremacy': 2,\n",
       " 'hellenistic': 1,\n",
       " 'newfoundland': 3,\n",
       " 'meaningful': 1,\n",
       " 'connections': 4,\n",
       " 'presentation': 2,\n",
       " 'comparisons': 2,\n",
       " 'waters': 4,\n",
       " 'rubbing': 1,\n",
       " 'japanese': 23,\n",
       " 'embodied': 1,\n",
       " 'brooke': 1,\n",
       " 'governing': 4,\n",
       " 'caulfield': 1,\n",
       " 'tagmata': 1,\n",
       " 'jean': 5,\n",
       " 'chordal': 2,\n",
       " 'trinity': 1,\n",
       " 'lear': 1,\n",
       " 'penicillin': 4,\n",
       " 'shenae': 2,\n",
       " 'compounded': 1,\n",
       " 'seven': 41,\n",
       " 'borrowed': 3,\n",
       " 'gone': 4,\n",
       " 'moons': 18,\n",
       " 'diet': 4,\n",
       " 'capita': 4,\n",
       " '10100': 1,\n",
       " 'architecture': 9,\n",
       " 'perspectives': 1,\n",
       " 'exhaustive': 1,\n",
       " 'decisive': 4,\n",
       " 'utilizes': 2,\n",
       " 'transactions': 4,\n",
       " 'meteorologist': 2,\n",
       " 'nominations': 6,\n",
       " 'distance': 7,\n",
       " 'cheques': 8,\n",
       " 'liquidize': 1,\n",
       " 'totally': 2,\n",
       " 'royce': 1,\n",
       " 'archduke': 1,\n",
       " 'bedding': 1,\n",
       " 'combination': 17,\n",
       " 'incisive': 1,\n",
       " 'succeeded': 4,\n",
       " 'became': 125,\n",
       " 'commander': 9,\n",
       " 'shipyards': 1,\n",
       " 'openly': 1,\n",
       " 'symbolized': 1,\n",
       " 'happy': 1,\n",
       " 'sweat': 1,\n",
       " 'jupiter': 2,\n",
       " 'phones': 4,\n",
       " 'graduate': 23,\n",
       " 'judaism': 2,\n",
       " '95': 7,\n",
       " 'chihuahua': 1,\n",
       " 'modality': 1,\n",
       " 'biostatistics': 1,\n",
       " 'papillae': 1,\n",
       " 'xr': 2,\n",
       " 'lists': 8,\n",
       " 'performers': 1,\n",
       " 'maj': 1,\n",
       " 'balkans': 1,\n",
       " 'highlight': 1,\n",
       " 'modernising': 1,\n",
       " 'jacket': 1,\n",
       " 'dreamed': 1,\n",
       " 'emptying': 1,\n",
       " 'predators': 3,\n",
       " 'outcrop': 1,\n",
       " 'connecting': 3,\n",
       " 'negligent': 1,\n",
       " 'jowers': 1,\n",
       " 'anniversary': 4,\n",
       " 'colts': 1,\n",
       " 'femoral': 1,\n",
       " 'sections': 14,\n",
       " 'rooms': 1,\n",
       " 'gamble': 1,\n",
       " 'hsa': 1,\n",
       " 'transportation': 5,\n",
       " 'pairs': 2,\n",
       " 'ian': 1,\n",
       " 'fire': 22,\n",
       " 'frequently': 22,\n",
       " 'welsh': 2,\n",
       " 'celebrities': 1,\n",
       " 'lindemulder': 1,\n",
       " 'henry': 18,\n",
       " 'flows': 3,\n",
       " 'legal': 32,\n",
       " 'wolfgang': 1,\n",
       " 'liberia': 1,\n",
       " 'hearst': 2,\n",
       " 'factual': 2,\n",
       " 'hunted': 1,\n",
       " '539': 1,\n",
       " 'legally': 5,\n",
       " 'intracranial': 3,\n",
       " '1975': 7,\n",
       " 'bridges': 6,\n",
       " 'note': 12,\n",
       " 'criminalizing': 1,\n",
       " 'mixes': 1,\n",
       " 'skilled': 1,\n",
       " 'cross': 11,\n",
       " 'metals': 2,\n",
       " 'rationing': 1,\n",
       " 'sizes': 4,\n",
       " 'delay': 2,\n",
       " 'partnerships': 1,\n",
       " 'organization': 27,\n",
       " 'perciform': 1,\n",
       " 'stefan': 1,\n",
       " 'conglomerate': 1,\n",
       " 'speaking': 9,\n",
       " 'sensation': 4,\n",
       " 'vassal': 1,\n",
       " 'multiplexer': 1,\n",
       " 'camps': 2,\n",
       " 'irritation': 1,\n",
       " 'anger': 1,\n",
       " 'witchcraft': 1,\n",
       " 'contemporary': 3,\n",
       " 'robin': 7,\n",
       " 'longfellow': 6,\n",
       " 'configurations': 1,\n",
       " 'shorthand': 1,\n",
       " 'notary': 18,\n",
       " 'ta': 1,\n",
       " 'burma': 1,\n",
       " 'drum': 1,\n",
       " 'kinships': 1,\n",
       " 'hitting': 4,\n",
       " 'save': 3,\n",
       " 'molecules': 8,\n",
       " 'combatant': 1,\n",
       " '86': 2,\n",
       " 'translation': 6,\n",
       " 'in': 3728,\n",
       " '1796': 1,\n",
       " 'somali': 1,\n",
       " 'aggregate': 1,\n",
       " 'correction': 1,\n",
       " 'copernican': 1,\n",
       " 'narratives': 3,\n",
       " 'developer': 2,\n",
       " 'nisou': 1,\n",
       " 'some': 178,\n",
       " 'caucasus': 1,\n",
       " 'diffusion': 1,\n",
       " 'appears': 14,\n",
       " 'civilian': 9,\n",
       " 'workout': 1,\n",
       " 'stuart': 1,\n",
       " 'owed': 1,\n",
       " 'transsexual': 4,\n",
       " 'veracruz': 2,\n",
       " 'spicy': 3,\n",
       " 'dealerships': 1,\n",
       " 'financially': 1,\n",
       " 'micropolitan': 2,\n",
       " 'vintage': 2,\n",
       " 'immigrant': 2,\n",
       " 'dissipating': 1,\n",
       " 'decades': 9,\n",
       " 'maturity': 3,\n",
       " 'transcellular': 1,\n",
       " 'brussels': 1,\n",
       " 'enslave': 1,\n",
       " 'informed': 2,\n",
       " 'controversial': 13,\n",
       " 'astrodome': 1,\n",
       " 'explore': 1,\n",
       " 'pilot': 9,\n",
       " 'champ': 1,\n",
       " 'bridge': 28,\n",
       " 'alleviate': 2,\n",
       " 'vieques': 1,\n",
       " 'quoted': 2,\n",
       " 'knox': 1,\n",
       " 'monitored': 3,\n",
       " 'cetana': 1,\n",
       " 'proper': 5,\n",
       " 'flew': 2,\n",
       " 'ruminants': 1,\n",
       " 'elective': 1,\n",
       " 'chinatown': 1,\n",
       " 'alligatoridae': 1,\n",
       " 'jodie': 2,\n",
       " 'predecessors': 1,\n",
       " 'dogs': 7,\n",
       " 'contention': 1,\n",
       " 'importantly': 2,\n",
       " 'background': 8,\n",
       " 'strategy': 5,\n",
       " 'reflecting': 4,\n",
       " 'contraception': 1,\n",
       " 'freak': 1,\n",
       " 'legs': 2,\n",
       " 'vocalist': 5,\n",
       " 'poured': 4,\n",
       " 'sing': 2,\n",
       " 'taburiente': 1,\n",
       " 'retirees': 1,\n",
       " 'umbilical': 1,\n",
       " 'said': 21,\n",
       " 'pagans': 1,\n",
       " 'mergers': 1,\n",
       " 'serpent': 2,\n",
       " 'colonies': 13,\n",
       " 'wienerle': 1,\n",
       " 'best': 123,\n",
       " 'willingly': 1,\n",
       " 'bright': 1,\n",
       " 'amino': 4,\n",
       " 'theological': 3,\n",
       " 'factorial': 1,\n",
       " 'cellphone': 1,\n",
       " 'suitable': 2,\n",
       " 'papers': 5,\n",
       " 'noun': 1,\n",
       " 'spanned': 2,\n",
       " 'adjoining': 1,\n",
       " 'errors': 2,\n",
       " 'maslin': 1,\n",
       " 'sexuality': 1,\n",
       " 'adjective': 3,\n",
       " 'zinser': 1,\n",
       " 'grapes': 1,\n",
       " 'incorrectly': 2,\n",
       " 'solomon': 3,\n",
       " 'drc': 1,\n",
       " 'welcome': 1,\n",
       " 'pattern': 7,\n",
       " 'tsao': 1,\n",
       " 'viewers': 22,\n",
       " 'ecoregion': 1,\n",
       " 'geldings': 1,\n",
       " 'institution': 5,\n",
       " 'coptic': 2,\n",
       " 'sexdecilliard': 1,\n",
       " 'morren': 1,\n",
       " 'convenient': 1,\n",
       " 'psychotropic': 1,\n",
       " '737': 1,\n",
       " 'hamburg': 1,\n",
       " 'premium': 7,\n",
       " 'johansson': 1,\n",
       " 'appendectomy': 1,\n",
       " 'watched': 15,\n",
       " 'upbringing': 2,\n",
       " 'awareness': 5,\n",
       " 'unattainable': 1,\n",
       " 'globalisation': 1,\n",
       " 'whiteley': 1,\n",
       " 'applying': 2,\n",
       " 'autocratic': 3,\n",
       " 'feather': 1,\n",
       " 'ranches': 1,\n",
       " 'thoroughly': 1,\n",
       " 'diagnostic': 2,\n",
       " 'champions': 7,\n",
       " 'ckf': 1,\n",
       " 'hallmark': 1,\n",
       " 'trinitarian': 1,\n",
       " 'syrinx': 1,\n",
       " 'homogeneous': 1,\n",
       " 'tetrapods': 1,\n",
       " 'passing': 5,\n",
       " 'wright': 9,\n",
       " 'clubs': 2,\n",
       " 'conifer': 1,\n",
       " 'generic': 1,\n",
       " 'wholly': 1,\n",
       " 'rk': 1,\n",
       " 'midwestern': 2,\n",
       " '1982': 8,\n",
       " '1888': 1,\n",
       " 'fighter': 6,\n",
       " 'convoy': 1,\n",
       " 'endowment': 1,\n",
       " 'bound': 1,\n",
       " 'icon': 7,\n",
       " 'carrier': 6,\n",
       " 'substances': 11,\n",
       " 'politics': 7,\n",
       " 'cite': 1,\n",
       " 'traits': 2,\n",
       " 'versa': 3,\n",
       " 'instant': 1,\n",
       " 'antoine': 3,\n",
       " 'racket': 1,\n",
       " 'mule': 3,\n",
       " 'smbus': 4,\n",
       " 'kneeling': 2,\n",
       " 'meristems': 1,\n",
       " 'dry': 7,\n",
       " 'q800': 1,\n",
       " 'awaiting': 1,\n",
       " 'outlets': 2,\n",
       " 'atop': 3,\n",
       " 'slipping': 1,\n",
       " 'button': 2,\n",
       " 'fit': 5,\n",
       " 'fault': 3,\n",
       " 'opened': 20,\n",
       " 'capulet': 2,\n",
       " 'bicolor': 1,\n",
       " 'singer': 20,\n",
       " 'texas': 57,\n",
       " 'insect': 1,\n",
       " 'belong': 4,\n",
       " 'holt': 1,\n",
       " 'delta': 2,\n",
       " 'jails': 4,\n",
       " 'shaoshan': 1,\n",
       " 'webb': 2,\n",
       " 'birds': 8,\n",
       " 'sinapis': 1,\n",
       " 'meteor': 3,\n",
       " 'denticles': 1,\n",
       " 'evaluation': 1,\n",
       " 'taraji': 1,\n",
       " 'attractive': 2,\n",
       " 'addiction': 3,\n",
       " 'stereo': 1,\n",
       " 'office': 28,\n",
       " 'intermittency': 1,\n",
       " 'reach': 9,\n",
       " 'folk': 8,\n",
       " 'skewed': 1,\n",
       " 'ny': 1,\n",
       " 'curve': 4,\n",
       " 'umts': 2,\n",
       " 'invading': 1,\n",
       " 'differentiation': 2,\n",
       " 'acting': 17,\n",
       " 'nationally': 5,\n",
       " 'tragic': 1,\n",
       " 'dispatchable': 1,\n",
       " 'unless': 4,\n",
       " 'cnn': 1,\n",
       " 'estonia': 1,\n",
       " 'noise': 1,\n",
       " 'dharma': 1,\n",
       " 'pennsylvania': 8,\n",
       " 'coins': 2,\n",
       " 'dales': 1,\n",
       " 'superstore': 1,\n",
       " 'waitress': 1,\n",
       " 'nougat': 1,\n",
       " 'wachowskis': 3,\n",
       " 'sookie': 1,\n",
       " 'recently': 7,\n",
       " 'amphetamines': 1,\n",
       " 'touched': 1,\n",
       " 'reminder': 1,\n",
       " 'troops': 13,\n",
       " 'barbra': 1,\n",
       " 'view': 14,\n",
       " 'installment': 3,\n",
       " 'practicing': 1,\n",
       " 'dose': 3,\n",
       " 'hittite': 1,\n",
       " 'outside': 20,\n",
       " 'davenant': 1,\n",
       " 'chemist': 1,\n",
       " 'dissenting': 2,\n",
       " 'guests': 2,\n",
       " 'controlled': 19,\n",
       " 'celebrations': 3,\n",
       " 'mathematica': 1,\n",
       " 'pierce': 4,\n",
       " 'technically': 4,\n",
       " 'abc': 18,\n",
       " 'township': 6,\n",
       " 'renunciation': 1,\n",
       " 'espa': 1,\n",
       " 'frederiksborg': 1,\n",
       " 'ethical': 3,\n",
       " 'grime': 1,\n",
       " 'campbell': 3,\n",
       " 'tn': 1,\n",
       " 'terminal': 1,\n",
       " 'nonprofit': 3,\n",
       " 'blu': 12,\n",
       " 'cameo': 3,\n",
       " 'optimus': 1,\n",
       " 'gould': 1,\n",
       " 'upheaval': 3,\n",
       " 'menstruation': 3,\n",
       " 'olfaction': 1,\n",
       " 'lighting': 4,\n",
       " 'initiatory': 1,\n",
       " 'rectum': 1,\n",
       " 'perceived': 6,\n",
       " 'agree': 1,\n",
       " 'guster': 1,\n",
       " 'atherosclerotic': 6,\n",
       " 'milk': 1,\n",
       " 'celebrity': 5,\n",
       " 'shallow': 2,\n",
       " 'seafloor': 6,\n",
       " 'plaque': 4,\n",
       " 'strong': 19,\n",
       " 'shue': 2,\n",
       " '1174': 1,\n",
       " 'lion': 1,\n",
       " 'separates': 3,\n",
       " 'virginia': 21,\n",
       " 'reichskanzler': 1,\n",
       " 'administrators': 2,\n",
       " 'roday': 1,\n",
       " 'therein': 3,\n",
       " 'colloquially': 7,\n",
       " 'dasypsyllus': 1,\n",
       " 'cleaned': 1,\n",
       " 'lights': 1,\n",
       " 'excavated': 1,\n",
       " 'swim': 2,\n",
       " 'philology': 1,\n",
       " 'memorial': 6,\n",
       " 'travelers': 1,\n",
       " 'embedded': 4,\n",
       " 'productivity': 2,\n",
       " '600': 4,\n",
       " 'viewpoint': 1,\n",
       " '9': 40,\n",
       " 'hydrogenated': 1,\n",
       " 'medial': 1,\n",
       " 'dairy': 3,\n",
       " 'citizenry': 1,\n",
       " 'lauded': 1,\n",
       " 'redeemer': 1,\n",
       " 'smoots': 1,\n",
       " 'sealings': 1,\n",
       " 'recruit': 1,\n",
       " 'buttons': 1,\n",
       " 'prescription': 1,\n",
       " 'rumble': 1,\n",
       " 'satisfaction': 1,\n",
       " 'flagged': 1,\n",
       " 'organic': 3,\n",
       " 'bissextile': 1,\n",
       " 'objects': 16,\n",
       " 'grants': 1,\n",
       " 'case': 21,\n",
       " 'steak': 7,\n",
       " 'gage': 2,\n",
       " 'cockney': 1,\n",
       " 'water': 82,\n",
       " 'plunger': 2,\n",
       " 'resuming': 1,\n",
       " 'descendant': 4,\n",
       " 'walt': 12,\n",
       " 'kevin': 8,\n",
       " 'gender': 7,\n",
       " 'eldest': 1,\n",
       " 'assassinations': 3,\n",
       " ...}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0f251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
