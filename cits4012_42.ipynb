{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023 CITS4012 Assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from optuna) (0.9.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from optuna) (1.11.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from optuna) (22.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from optuna) (1.23.5)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from optuna) (2.0.14)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.4.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tabulate in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from seaborn) (3.6.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (22.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from pandas>=0.25->seaborn) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib) (22.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nicho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install optuna\n",
    "%pip install tabulate\n",
    "%pip install seaborn\n",
    "%pip install matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.DataSet Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data\n",
    "train_data = pd.read_csv(\"WikiQA-train.tsv\", sep=\"\\t\")\n",
    "test_data = pd.read_csv(\"WikiQA-test.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the unique questions from the train and test data frames, including the documentID and the DocumentTitle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_documenttag(data):\n",
    "    qd = data[\n",
    "        [\"Question\", \"QuestionID\", \"DocumentID\", \"DocumentTitle\"]\n",
    "    ].drop_duplicates()\n",
    "    return qd\n",
    "\n",
    "\n",
    "train_question_doctag = get_questions_documenttag(train_data)\n",
    "test_question_doctag = get_questions_documenttag(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique questions\n",
    "train_questions = train_question_doctag[\"Question\"]\n",
    "test_questions = test_question_doctag[\"Question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique document ids\n",
    "train_docid = train_question_doctag[\"DocumentID\"]\n",
    "test_docid = test_question_doctag[\"DocumentID\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the answers to those questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(data, questions, documentids):\n",
    "    answers = []  # list of answers\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q]  # add the document id\n",
    "        df = data[data[\"Question\"] == question]\n",
    "        index = df.loc[df[\"Label\"] == 1][\"Sentence\"].index.values\n",
    "        if len(index) == 0:  # if no answer found\n",
    "            answers.append([question, doc_id, \"No answer\"])\n",
    "        else:  # if 1 answer found\n",
    "            answers.append([question, doc_id, df.loc[index[0], \"Sentence\"]])\n",
    "    return answers\n",
    "\n",
    "\n",
    "train_answers = pd.DataFrame(get_answers(train_data, train_questions, train_docid))\n",
    "test_answers = pd.DataFrame(get_answers(test_data, test_questions, test_docid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above get_answers returns train_answers and test_answers which, gives us in the following columns\n",
    "\n",
    "-   Question\n",
    "-   Related Document ID\n",
    "-   Answer (if no answer to that question, return no answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(data, questions, documentids):  # (done by Finn, tweaked by Dan)\n",
    "    documents = []\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q]  # add the document id\n",
    "        df = data[data[\"Question\"] == question]\n",
    "        sentences = df[\"Sentence\"].tolist()\n",
    "        for i in range(0, len(sentences) - 1):\n",
    "            sentences[i] = sentences[i] + \" \"\n",
    "        documents.append([doc_id, \"\".join(sentences)])\n",
    "    return documents\n",
    "\n",
    "\n",
    "train_documents = pd.DataFrame(\n",
    "    get_documents(train_data, train_questions, train_docid)\n",
    ")  # return the individual document in list\n",
    "test_documents = pd.DataFrame(\n",
    "    get_documents(test_data, test_questions, test_docid)\n",
    ")  # return the individual document in list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above train_documents and test_documents called from the get_documents gives us in the following columns\n",
    "\n",
    "-   Document ID\n",
    "-   Full Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming all the columns for more standardised access\n",
    "train_answers.columns = [\"Question\", \"DocumentID\", \"Answer\"]\n",
    "test_answers.columns = [\"Question\", \"DocumentID\", \"Answer\"]\n",
    "train_documents.columns = [\"DocumentID\", \"Document\"]\n",
    "test_documents.columns = [\"DocumentID\", \"Document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2117, 2117, 630, 630)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result is 2117, 2117, 630, 630\n",
    "\n",
    "len(train_answers), len(train_documents), len(test_answers), len(test_documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior to tagging, we should maybe clean the document and answers first:** (stopped here)\n",
    "\n",
    "Maybe?\n",
    "\n",
    "-   lowercase (might lose context, but we can use on questions)\n",
    "-   removing any punctuation or weird symbols (do)\n",
    "-   removal of stop words? (probably not)\n",
    "\n",
    "Make sure that the pre-processing is standardised to be the same throughout doc and ans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are just common English contractions. We used it in Lab 5 before!\n",
    "contraction_dict = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lower(text):\n",
    "    # Lowercase the text for question, answer and documents\n",
    "    text = text.lower()\n",
    "    for word, new_word in contraction_dict.items():\n",
    "        text = text.replace(word, new_word)  # dealing with contractions\n",
    "    pattern = r\"[^a-zA-Z0-9\\s]\"\n",
    "    cleaned_text = re.sub(pattern, \" \", text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "train_answers[[\"Question\", \"Answer\"]] = train_answers[[\"Question\", \"Answer\"]].applymap(\n",
    "    preprocess_lower\n",
    ")\n",
    "train_documents[\"Document\"] = train_documents[\"Document\"].apply(preprocess_lower)\n",
    "test_answers[[\"Question\", \"Answer\"]] = test_answers[[\"Question\", \"Answer\"]].applymap(\n",
    "    preprocess_lower\n",
    ")\n",
    "test_documents[\"Document\"] = test_documents[\"Document\"].apply(preprocess_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>a partly submerged glacier cave on perito more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>in physics   circular motion is a movement of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D5</td>\n",
       "      <td>apollo creed is a fictional character from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D6</td>\n",
       "      <td>in the united states  the title of federal jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D7</td>\n",
       "      <td>the beretta 21a bobcat is a small pocket sized...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>D2805</td>\n",
       "      <td>blue mountain state is an american comedy seri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>D2806</td>\n",
       "      <td>apple inc   formerly apple computer  inc   is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>D2807</td>\n",
       "      <td>section 8 housing in the south bronx section 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>D2808</td>\n",
       "      <td>restaurants categorized by type and informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>D2810</td>\n",
       "      <td>u s  federal reserve notes in the mid 1990s th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2117 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DocumentID                                           Document\n",
       "0            D1  a partly submerged glacier cave on perito more...\n",
       "1            D2  in physics   circular motion is a movement of ...\n",
       "2            D5  apollo creed is a fictional character from the...\n",
       "3            D6  in the united states  the title of federal jud...\n",
       "4            D7  the beretta 21a bobcat is a small pocket sized...\n",
       "...         ...                                                ...\n",
       "2112      D2805  blue mountain state is an american comedy seri...\n",
       "2113      D2806  apple inc   formerly apple computer  inc   is ...\n",
       "2114      D2807  section 8 housing in the south bronx section 8...\n",
       "2115      D2808  restaurants categorized by type and informatio...\n",
       "2116      D2810  u s  federal reserve notes in the mid 1990s th...\n",
       "\n",
       "[2117 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelling(documents, answers):\n",
    "    tagged_documents = []\n",
    "    for q in range(len(answers)):\n",
    "        tagged_document = []\n",
    "        qn = answers[\"Question\"].loc[q]\n",
    "        doc_id = answers[\"DocumentID\"].loc[q]\n",
    "        content = documents.loc[documents[\"DocumentID\"] == doc_id, \"Document\"].values[0]\n",
    "        answer = answers[\"Answer\"].loc[q]\n",
    "\n",
    "        if answer == \"no answer\":\n",
    "            tokens = word_tokenize(content)\n",
    "            for j in range(len(tokens)):\n",
    "                tagged_document.append(\"N\")  # none\n",
    "        else:\n",
    "            parts = content.partition(answer)\n",
    "            for j in range(len(parts)):\n",
    "                tokens = word_tokenize(parts[j])\n",
    "                if j == 1:\n",
    "                    tagged_document.append(\"S\")  # start of answer\n",
    "                    for k in range(len(tokens) - 2):\n",
    "                        tagged_document.append(\"I\")  # inside of answer\n",
    "                    tagged_document.append(\"E\")  # end of answer\n",
    "                else:\n",
    "                    for k in range(len(tokens)):\n",
    "                        tagged_document.append(\"N\")  # outside answer\n",
    "        tagged_documents.append(tagged_document)\n",
    "    return tagged_documents\n",
    "\n",
    "\n",
    "train_doc_ans_labels = labelling(train_documents, train_answers)\n",
    "test_doc_ans_labels = labelling(test_documents, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'egg']\n",
      "['N', 'roll']\n",
      "['N', 'is']\n",
      "['N', 'a']\n",
      "['N', 'term']\n",
      "['N', 'used']\n",
      "['N', 'for']\n",
      "['N', 'many']\n",
      "['N', 'different']\n",
      "['N', 'foods']\n",
      "['N', 'around']\n",
      "['N', 'the']\n",
      "['N', 'world']\n",
      "['S', '2']\n",
      "['I', 'egg']\n",
      "['I', 'roll']\n",
      "['I', 'varieties']\n",
      "['I', 'of']\n",
      "['I', 'egg']\n",
      "['I', 'rolls']\n",
      "['I', 'are']\n",
      "['I', 'found']\n",
      "['I', 'in']\n",
      "['I', 'mainland']\n",
      "['I', 'china']\n",
      "['I', 'many']\n",
      "['I', 'chinese']\n",
      "['I', 'speaking']\n",
      "['I', 'regions']\n",
      "['I', 'of']\n",
      "['I', 'asia']\n",
      "['I', 'and']\n",
      "['I', 'chinese']\n",
      "['I', 'immigrant']\n",
      "['I', 'communities']\n",
      "['I', 'around']\n",
      "['I', 'the']\n",
      "['E', 'world']\n",
      "['N', 'egg']\n",
      "['N', 'rolls']\n",
      "['N', 'as']\n",
      "['N', 'referred']\n",
      "['N', 'to']\n",
      "['N', 'in']\n",
      "['N', 'china']\n",
      "['N', 'in']\n",
      "['N', 'guangdong']\n",
      "['N', 'and']\n",
      "['N', 'hong']\n",
      "['N', 'kong']\n",
      "['N', 'egg']\n",
      "['N', 'roll']\n",
      "['N', 'usually']\n",
      "['N', 'refers']\n",
      "['N', 'to']\n",
      "['N', 'biscuit']\n",
      "['N', 'roll']\n",
      "['N', 'this']\n",
      "['N', 'is']\n",
      "['N', 'a']\n",
      "['N', 'type']\n",
      "['N', 'of']\n",
      "['N', 'biscuit']\n",
      "['N', 'the']\n",
      "['N', 'ingredient']\n",
      "['N', 'included']\n",
      "['N', 'egg']\n",
      "['N', 'flour']\n",
      "['N', 'and']\n",
      "['N', 'sugar']\n",
      "['N', 'egg']\n",
      "['N', 'roll']\n",
      "['N', 'also']\n",
      "['N', 'is']\n",
      "['N', 'a']\n",
      "['N', 'dish']\n",
      "['N', 'in']\n",
      "['N', 'canton']\n",
      "['N', 'and']\n",
      "['N', 'hong']\n",
      "['N', 'kong']\n",
      "['N', 'this']\n",
      "['N', 'is']\n",
      "['N', 'usually']\n",
      "['N', 'made']\n",
      "['N', 'with']\n",
      "['N', 'vegetable']\n",
      "['N', 'within']\n",
      "['N', 'china']\n",
      "['N', 'egg']\n",
      "['N', 'rolls']\n",
      "['N', 'are']\n",
      "['N', 'eaten']\n",
      "['N', 'predominantly']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'southeast']\n",
      "['N', 'and']\n",
      "['N', 'are']\n",
      "['N', 'not']\n",
      "['N', 'as']\n",
      "['N', 'commonly']\n",
      "['N', 'consumed']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'north']\n",
      "['N', 'and']\n",
      "['N', 'western']\n",
      "['N', 'parts']\n",
      "['N', 'of']\n",
      "['N', 'china']\n",
      "['N', 'in']\n",
      "['N', 'american']\n",
      "['N', 'chinese']\n",
      "['N', 'cuisine']\n",
      "['N', 'an']\n",
      "['N', 'egg']\n",
      "['N', 'roll']\n",
      "['N', 'is']\n",
      "['N', 'a']\n",
      "['N', 'savory']\n",
      "['N', 'dish']\n",
      "['N', 'typically']\n",
      "['N', 'served']\n",
      "['N', 'as']\n",
      "['N', 'an']\n",
      "['N', 'appetizer']\n",
      "['N', 'it']\n",
      "['N', 'is']\n",
      "['N', 'usually']\n",
      "['N', 'stuffed']\n",
      "['N', 'with']\n",
      "['N', 'chicken']\n",
      "['N', 'pork']\n",
      "['N', 'or']\n",
      "['N', 'shrimp']\n",
      "['N', 'cabbage']\n",
      "['N', 'carrots']\n",
      "['N', 'tomatoes']\n",
      "['N', 'bean']\n",
      "['N', 'sprouts']\n",
      "['N', 'and']\n",
      "['N', 'other']\n",
      "['N', 'vegetables']\n",
      "['N', 'and']\n",
      "['N', 'then']\n",
      "['N', 'deep']\n",
      "['N', 'fried']\n",
      "['N', 'this']\n",
      "['N', 'variety']\n",
      "['N', 'of']\n",
      "['N', 'the']\n",
      "['N', 'egg']\n",
      "['N', 'roll']\n",
      "['N', 'is']\n",
      "['N', 'very']\n",
      "['N', 'common']\n",
      "['N', 'and']\n",
      "['N', 'popular']\n",
      "['N', 'across']\n",
      "['N', 'even']\n",
      "['N', 'regional']\n",
      "['N', 'varieties']\n",
      "['N', 'of']\n",
      "['N', 'american']\n",
      "['N', 'chinese']\n",
      "['N', 'food']\n",
      "['N', 'and']\n",
      "['N', 'is']\n",
      "['N', 'often']\n",
      "['N', 'included']\n",
      "['N', 'as']\n",
      "['N', 'part']\n",
      "['N', 'of']\n",
      "['N', 'a']\n",
      "['N', 'combination']\n",
      "['N', 'platter']\n",
      "['N', 'in']\n",
      "['N', 'montreal']\n",
      "['N', 'canada']\n",
      "['N', 'open']\n",
      "['N', 'ended']\n",
      "['N', 'eggrolls']\n",
      "['N', 'are']\n",
      "['N', 'a']\n",
      "['N', 'jewish']\n",
      "['N', 'chinese']\n",
      "['N', 'fusion']\n",
      "['N', 'dish']\n",
      "['N', 'they']\n",
      "['N', 'can']\n",
      "['N', 'be']\n",
      "['N', 'either']\n",
      "['N', 'kosher']\n",
      "['N', 'certified']\n",
      "['N', 'or']\n",
      "['N', 'merely']\n",
      "['N', 'kosher']\n",
      "['N', 'style']\n",
      "2  egg roll    varieties of egg rolls are found in mainland china   many chinese speaking regions of asia  and chinese immigrant communities around the world \n"
     ]
    }
   ],
   "source": [
    "# check if tags are good\n",
    "def testing_tokens(ind, labels, documents, answers):\n",
    "    for i, j in zip(labels[ind], word_tokenize(documents[\"Document\"][ind])):\n",
    "        print([i, j])\n",
    "    print(answers[\"Answer\"][ind])\n",
    "\n",
    "\n",
    "testing_tokens(1000, train_doc_ans_labels, train_documents, train_answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaned Documents: train and test\n",
    "\n",
    "train_answers - contains the ['Question','DocumentID','Answer']\n",
    "\n",
    "train_documents - contains the ['DocumentID','Document']\n",
    "\n",
    "train_doc_ans_labels - contains a list of list of answer tags for each document,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare the document for word embeddings:\n",
    "train_doc_ques = pd.DataFrame(\n",
    "    {\"Document\": train_documents[\"Document\"], \"Question\": train_answers[\"Question\"]}\n",
    ")\n",
    "test_doc_ques = pd.DataFrame(\n",
    "    {\"Document\": test_documents[\"Document\"], \"Question\": test_answers[\"Question\"]}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.QA Model Implementation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "To use the CBOW model, we need the data in sentences. Extract this from the original dataset, don't use sent_tokenise, will mess with some of the fullstops, we want to maintain structure from above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokens(data):\n",
    "    sentence_list = []\n",
    "    for i in range(len(data)):\n",
    "        sentence_list.append(word_tokenize(data[i]))\n",
    "    return sentence_list\n",
    "\n",
    "\n",
    "train_doc_list = word_tokens(train_doc_ques[\"Document\"])\n",
    "train_ques_list = word_tokens(train_doc_ques[\"Question\"])\n",
    "test_doc_list = word_tokens(test_doc_ques[\"Document\"])\n",
    "test_ques_list = word_tokens(test_doc_ques[\"Question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = train_doc_list + train_ques_list + test_doc_list + test_ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trained, don't have to run this multiple times\n",
    "wc_cbow_model = Word2Vec(\n",
    "    sentences=combined_text,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=2,\n",
    "    epochs=30,\n",
    ")\n",
    "# wc_cbow_model.save(\"cbow.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement QA\n",
    "\n",
    "1. Word Embeddings, using CBOW\n",
    "2. Feature Extraction 1 - POS tags\n",
    "3. Feature Extraction 2 - TF-IDF\n",
    "4. Feature Extraction 3 - NER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this if model in directory\n",
    "wc_cbow_model = Word2Vec.load(\"./cbow.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(doc):\n",
    "    tokenized_doc = word_tokenize(doc)\n",
    "    embeddings = [wc_cbow_model.wv[word] for word in tokenized_doc]\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "train_doc_ques[\"Doc_Embeddings\"] = train_doc_ques[\"Document\"].apply(get_word_embeddings)\n",
    "train_doc_ques[\"Q_Embeddings\"] = train_doc_ques[\"Question\"].apply(get_word_embeddings)\n",
    "test_doc_ques[\"Doc_Embeddings\"] = test_doc_ques[\"Document\"].apply(get_word_embeddings)\n",
    "test_doc_ques[\"Q_Embeddings\"] = test_doc_ques[\"Question\"].apply(get_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_ques[\"Doc_Tokens\"] = train_doc_ques[\"Document\"].apply(word_tokenize)\n",
    "train_doc_ques[\"Q_Tokens\"] = train_doc_ques[\"Question\"].apply(word_tokenize)\n",
    "test_doc_ques[\"Doc_Tokens\"] = test_doc_ques[\"Document\"].apply(word_tokenize)\n",
    "test_doc_ques[\"Q_Tokens\"] = test_doc_ques[\"Question\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Max Seq Length is 924, Median is 182.0, Number of lines is 2117)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_length(column):\n",
    "    max_length = 0\n",
    "    lns = []\n",
    "    for i in range(len(column)):\n",
    "        lns.append(len(column[i]))\n",
    "        if len(column[i]) > max_length:\n",
    "            max_length = len(column[i])\n",
    "    return \"Max Seq Length is {}, Median is {}, Number of lines is {})\".format(\n",
    "        max_length, np.median(lns), len(lns)\n",
    "    )\n",
    "\n",
    "\n",
    "find_max_length(test_doc_ques[\"Doc_Embeddings\"])\n",
    "find_max_length(train_doc_ques[\"Doc_Embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_count(doc):\n",
    "    count = 0\n",
    "    for i in range(len(doc)):\n",
    "        if len(doc[\"Doc_Embeddings\"][i]) != len(doc[\"Doc_Tokens\"][i]):\n",
    "            count += 1\n",
    "        elif len(doc[\"Q_Embeddings\"][i]) != len(doc[\"Q_Tokens\"][i]):\n",
    "            count += 1\n",
    "        else:\n",
    "            continue\n",
    "    return count\n",
    "\n",
    "\n",
    "check_count(train_doc_ques)  # looks good"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, need to convert the POS tags, NER tags into embeddings. After this, pad the questions and answers to the max question/document length in the combined training and test set.\n",
    "\n",
    "### PoS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nicho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Apply the pos tags to the tokens\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# download the dependency and resource as required\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "train_doc_ques[\"Doc_POS\"] = train_doc_ques[\"Doc_Tokens\"].apply(pos_tag)\n",
    "train_doc_ques[\"Q_POS\"] = train_doc_ques[\"Q_Tokens\"].apply(pos_tag)\n",
    "test_doc_ques[\"Doc_POS\"] = test_doc_ques[\"Doc_Tokens\"].apply(pos_tag)\n",
    "test_doc_ques[\"Q_POS\"] = test_doc_ques[\"Q_Tokens\"].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how', 'WRB'),\n",
       " ('many', 'JJ'),\n",
       " ('schools', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('ten', 'NN')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the POS tags: # looks ok\n",
    "train_doc_ques[\"Q_POS\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$': 0,\n",
       " 'CC': 1,\n",
       " 'CD': 2,\n",
       " 'DT': 3,\n",
       " 'EX': 4,\n",
       " 'FW': 5,\n",
       " 'IN': 6,\n",
       " 'JJ': 7,\n",
       " 'JJR': 8,\n",
       " 'JJS': 9,\n",
       " 'MD': 10,\n",
       " 'NN': 11,\n",
       " 'NNP': 12,\n",
       " 'NNPS': 13,\n",
       " 'NNS': 14,\n",
       " 'PDT': 15,\n",
       " 'POS': 16,\n",
       " 'PRP': 17,\n",
       " 'PRP$': 18,\n",
       " 'RB': 19,\n",
       " 'RBR': 20,\n",
       " 'RBS': 21,\n",
       " 'RP': 22,\n",
       " 'SYM': 23,\n",
       " 'TO': 24,\n",
       " 'UH': 25,\n",
       " 'VB': 26,\n",
       " 'VBD': 27,\n",
       " 'VBG': 28,\n",
       " 'VBN': 29,\n",
       " 'VBP': 30,\n",
       " 'VBZ': 31,\n",
       " 'WDT': 32,\n",
       " 'WP': 33,\n",
       " 'WP$': 34,\n",
       " 'WRB': 35}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all unique POS Tags\n",
    "all_pos_tags = (\n",
    "    train_doc_ques[\"Doc_POS\"].tolist()\n",
    "    + test_doc_ques[\"Doc_POS\"].tolist()\n",
    "    + train_doc_ques[\"Q_POS\"].tolist()\n",
    "    + test_doc_ques[\"Q_POS\"].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "def get_unique_pos(data):\n",
    "    pos_tags = set()\n",
    "    for item in data:\n",
    "        for _, pos_tag in item:\n",
    "            pos_tags.add(pos_tag)\n",
    "\n",
    "    pos_tag_index = {tag: i for i, tag in enumerate(sorted(pos_tags))}\n",
    "    return pos_tag_index\n",
    "\n",
    "\n",
    "pos_iden = get_unique_pos(all_pos_tags)  # list of tags\n",
    "pos_iden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Tagging\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to run this:\n",
    "\n",
    "-   pip install spacy\n",
    "-   python -m spacy download en_core_web_sm\n",
    "\n",
    "If loaded for the first time, restart kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk using Spacy\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "# loading pre-trained model of NER\n",
    "# nlp = en_core_web_sm.load()\n",
    "# nlp.to_disk(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"./en_core_web_sm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tagging(texts):\n",
    "    tagged_texts = []\n",
    "    for text in texts:\n",
    "        doc = spacy.tokens.Doc(nlp.vocab, words=text)\n",
    "        nlp.get_pipe(\"ner\")(doc)\n",
    "        tagged_texts.append([(token.text, token.ent_type_) for token in doc])\n",
    "    return tagged_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will take a while...\n",
    "train_doc_ques[\"Doc_NER\"] = ner_tagging(train_doc_ques[\"Doc_Tokens\"])\n",
    "train_doc_ques[\"Q_NER\"] = ner_tagging(train_doc_ques[\"Q_Tokens\"])\n",
    "test_doc_ques[\"Doc_NER\"] = ner_tagging(test_doc_ques[\"Doc_Tokens\"])\n",
    "test_doc_ques[\"Q_NER\"] = ner_tagging(test_doc_ques[\"Q_Tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how', ''),\n",
       " ('much', ''),\n",
       " ('are', ''),\n",
       " ('the', ''),\n",
       " ('harry', 'WORK_OF_ART'),\n",
       " ('potter', 'WORK_OF_ART'),\n",
       " ('movies', 'WORK_OF_ART'),\n",
       " ('worth', 'WORK_OF_ART')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc_ques[\"Q_NER\"][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'CARDINAL': 1,\n",
       " 'DATE': 2,\n",
       " 'EVENT': 3,\n",
       " 'FAC': 4,\n",
       " 'GPE': 5,\n",
       " 'LANGUAGE': 6,\n",
       " 'LAW': 7,\n",
       " 'LOC': 8,\n",
       " 'MONEY': 9,\n",
       " 'NORP': 10,\n",
       " 'ORDINAL': 11,\n",
       " 'ORG': 12,\n",
       " 'PERCENT': 13,\n",
       " 'PERSON': 14,\n",
       " 'PRODUCT': 15,\n",
       " 'QUANTITY': 16,\n",
       " 'TIME': 17,\n",
       " 'WORK_OF_ART': 18}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar approach to the POS\n",
    "\n",
    "# Extract all unique POS Tags\n",
    "all_ner_tags = (\n",
    "    train_doc_ques[\"Doc_NER\"].tolist()\n",
    "    + test_doc_ques[\"Doc_NER\"].tolist()\n",
    "    + train_doc_ques[\"Q_NER\"].tolist()\n",
    "    + test_doc_ques[\"Q_NER\"].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "def get_unique_ner(data):\n",
    "    ner_tags = set()\n",
    "    for item in data:\n",
    "        for _, ner_tag in item:\n",
    "            ner_tags.add(ner_tag)\n",
    "\n",
    "    ner_tag_index = {tag: i for i, tag in enumerate(sorted(ner_tags))}\n",
    "    return ner_tag_index\n",
    "\n",
    "\n",
    "ner_iden = get_unique_pos(all_ner_tags)  # list of tags\n",
    "ner_iden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ohv dims\n",
    "ner_idx = ner_iden.values()\n",
    "aa = np.eye(max(ner_idx) + 1)\n",
    "# aa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "First, calculate the document frequency of each token in the entire corpus (training documents + testing documents). The result is a dictionary where each token is a key and its value is the document frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_frequency(corpus):\n",
    "    \"\"\"\n",
    "    Computes the document frequency for every token in the corpus.\n",
    "    Returns a dictionary {token: doc_freq, ...}\n",
    "    \"\"\"\n",
    "    document_frequency = {}\n",
    "    for document in corpus:\n",
    "        for token in np.unique(document):\n",
    "            try:\n",
    "                document_frequency[token] += 1\n",
    "            except:\n",
    "                document_frequency[token] = 1\n",
    "    return document_frequency\n",
    "\n",
    "\n",
    "train_corpus = (\n",
    "    train_doc_ques[\"Doc_Tokens\"].tolist() + train_doc_ques[\"Q_Tokens\"].tolist()\n",
    ")\n",
    "test_corpus = test_doc_ques[\"Doc_Tokens\"].tolist() + test_doc_ques[\"Q_Tokens\"].tolist()\n",
    "train_doc_freq = document_frequency(train_corpus)\n",
    "test_doc_freq = document_frequency(test_corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate TF-IDF using the document frequency from above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "def compute_tf_idf(corpus, doc_frequency):\n",
    "    \"\"\"\n",
    "    Computes the term frequency inverse document frequency for every token in every document in the corpus.\n",
    "    Returns a list the same shape as the list of tokenized documents except every token is replaced with the tf-idf\n",
    "    for that token.\n",
    "    \"\"\"\n",
    "    tf_idf = {}\n",
    "    tf_idf_list = []\n",
    "    N = len(doc_frequency)\n",
    "    doc_id = 0\n",
    "    for document in corpus:\n",
    "        tf_idf_doc = []\n",
    "        counter = Counter(document)\n",
    "        total_num_words = len(document)\n",
    "        for token in np.unique(document):\n",
    "            tf = counter[token] / total_num_words\n",
    "            df = doc_frequency[token]\n",
    "            idf = math.log(N / (df + 1)) + 1\n",
    "            tf_idf[doc_id, token] = tf * idf\n",
    "        for token in document:\n",
    "            tf_idf_doc.append(tf_idf[doc_id, token])\n",
    "        tf_idf_list.append(tf_idf_doc)\n",
    "        doc_id += 1\n",
    "    return tf_idf_list\n",
    "\n",
    "\n",
    "train_doc_ques[\"Doc_TFIDF\"] = compute_tf_idf(\n",
    "    train_doc_ques[\"Doc_Tokens\"].tolist(), train_doc_freq\n",
    ")\n",
    "train_doc_ques[\"Q_TFIDF\"] = compute_tf_idf(\n",
    "    train_doc_ques[\"Q_Tokens\"].tolist(), train_doc_freq\n",
    ")\n",
    "test_doc_ques[\"Doc_TFIDF\"] = compute_tf_idf(\n",
    "    test_doc_ques[\"Doc_Tokens\"].tolist(), test_doc_freq\n",
    ")\n",
    "test_doc_ques[\"Q_TFIDF\"] = compute_tf_idf(\n",
    "    test_doc_ques[\"Q_Tokens\"].tolist(), test_doc_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vectorize(\n",
    "    pos_tagger, ner_tagger, data\n",
    "):  # pass in the unique dict for ner or pos\n",
    "    pos_idx = pos_tagger.values()\n",
    "    pos_ohv = np.eye(max(pos_idx) + 1)  # create the ohv\n",
    "    ner_idx = ner_tagger.values()\n",
    "    ner_ohv = np.eye(max(ner_idx) + 1)\n",
    "\n",
    "    dpos_full_ohv, dner_full_ohv = [], []  # lists to append to\n",
    "    qpos_full_ohv, qner_full_ohv = [], []  # lists to append to\n",
    "\n",
    "    for item in data[\"Doc_POS\"]:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            pos_index_iden = pos_tagger[tag]\n",
    "            sent_ohv.append(pos_ohv[pos_index_iden])\n",
    "        dpos_full_ohv.append(sent_ohv)\n",
    "\n",
    "    for item in data[\"Q_POS\"]:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            pos_index_iden = pos_tagger[tag]\n",
    "            sent_ohv.append(pos_ohv[pos_index_iden])\n",
    "        qpos_full_ohv.append(sent_ohv)\n",
    "\n",
    "    for item in data[\"Doc_NER\"]:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            ner_index_iden = ner_tagger[tag]\n",
    "            sent_ohv.append(ner_ohv[ner_index_iden])\n",
    "        dner_full_ohv.append(sent_ohv)\n",
    "\n",
    "    for item in data[\"Q_NER\"]:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            ner_index_iden = ner_tagger[tag]\n",
    "            sent_ohv.append(ner_ohv[ner_index_iden])\n",
    "        qner_full_ohv.append(sent_ohv)\n",
    "\n",
    "    return (dpos_full_ohv, qpos_full_ohv, dner_full_ohv, qner_full_ohv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ohv for doc\n",
    "(\n",
    "    train_doc_pos_ohv,\n",
    "    train_q_pos_ohv,\n",
    "    train_doc_ner_ohv,\n",
    "    train_q_ner_ohv,\n",
    ") = one_hot_vectorize(pos_iden, ner_iden, train_doc_ques)\n",
    "test_doc_pos_ohv, test_q_pos_ohv, test_doc_ner_ohv, test_q_ner_ohv = one_hot_vectorize(\n",
    "    pos_iden, ner_iden, test_doc_ques\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dataframe to just tokens and embeddings:\n",
    "doc_emb_train = train_doc_ques[[\"Doc_Tokens\", \"Doc_Embeddings\", \"Doc_TFIDF\"]]\n",
    "doc_pos_ner = pd.DataFrame({\"Doc_POS\": train_doc_pos_ohv, \"Doc_NER\": train_doc_ner_ohv})\n",
    "doc_emb_train = pd.concat([doc_emb_train, doc_pos_ner], axis=1)\n",
    "\n",
    "q_emb_train = train_doc_ques[[\"Q_Tokens\", \"Q_Embeddings\", \"Q_TFIDF\"]]\n",
    "q_pos_ner = pd.DataFrame({\"Q_POS\": train_q_pos_ohv, \"Q_NER\": train_q_ner_ohv})\n",
    "q_emb_train = pd.concat([q_emb_train, q_pos_ner], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb_test = test_doc_ques[[\"Doc_Tokens\", \"Doc_Embeddings\", \"Doc_TFIDF\"]]\n",
    "doc_pos_ner = pd.DataFrame({\"Doc_POS\": test_doc_pos_ohv, \"Doc_NER\": test_doc_ner_ohv})\n",
    "doc_emb_test = pd.concat([doc_emb_test, doc_pos_ner], axis=1)\n",
    "\n",
    "q_emb_test = test_doc_ques[[\"Q_Tokens\", \"Q_Embeddings\", \"Q_TFIDF\"]]\n",
    "q_pos_ner = pd.DataFrame({\"Q_POS\": test_q_pos_ohv, \"Q_NER\": test_q_ner_ohv})\n",
    "q_emb_test = pd.concat([q_emb_test, q_pos_ner], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (Doc and Qn)\n",
    "\n",
    "The embeddings of the questions and answers of the train and test set can be found here:\n",
    "\n",
    "-   Train Document - doc_emb_train\n",
    "-   Train Q - q_emb_train\n",
    "-   Test Document - doc_emb_test\n",
    "-   Test Q - q_emb_test\n",
    "\n",
    "The max_document size is 1675 and max_question size is 23.\n",
    "Combine all the embeddings into a full array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_array(data, data_type=\"Document\"):\n",
    "    num_vec_length = 156\n",
    "    max_doc = 1675\n",
    "    max_qn = 23\n",
    "    zero_vec = np.zeros(156)\n",
    "\n",
    "    if data_type == \"Document\":\n",
    "        full_vec = []  # create a list for list of list for document\n",
    "        for dat in range(len(data)):  # go through each line\n",
    "            doc_ques = data.loc[dat]  # document data\n",
    "            v = []  # create list to each word\n",
    "            for j in range(len(doc_ques.iloc[0])):\n",
    "                vn = []  # list of concat word embeddings\n",
    "                vn.append(doc_ques.iloc[1][j].tolist())  # Word2Vec\n",
    "                vn.append(doc_ques.iloc[2][j])  # TF-IDF\n",
    "                vn.append(doc_ques.iloc[3][j].tolist())  # POS\n",
    "                vn.append(doc_ques.iloc[4][j].tolist())  # NER\n",
    "                flatten = [\n",
    "                    item\n",
    "                    for sublist in vn\n",
    "                    for item in (sublist if isinstance(sublist, list) else [sublist])\n",
    "                ]\n",
    "                v.append(flatten)\n",
    "            while len(v) < max_doc:\n",
    "                v.append(zero_vec)\n",
    "            full_vec.append(v)\n",
    "\n",
    "    if data_type == \"Question\":\n",
    "        full_vec = []  # create a list for list of list for document\n",
    "        for dat in range(len(data)):  # go through each line\n",
    "            doc_ques = data.loc[dat]  # document data\n",
    "            v = []  # create list to each word\n",
    "            for j in range(len(doc_ques.iloc[0])):\n",
    "                vn = []  # list of concat word embeddings\n",
    "                vn.append(doc_ques.iloc[1][j].tolist())  # Word2Vec\n",
    "                vn.append(doc_ques.iloc[2][j])  # TF-IDF\n",
    "                vn.append(doc_ques.iloc[3][j].tolist())  # POS\n",
    "                vn.append(doc_ques.iloc[4][j].tolist())  # NER\n",
    "                flatten = [\n",
    "                    item\n",
    "                    for sublist in vn\n",
    "                    for item in (sublist if isinstance(sublist, list) else [sublist])\n",
    "                ]\n",
    "                v.append(flatten)\n",
    "            while len(v) < max_qn:\n",
    "                v.append(zero_vec)\n",
    "            full_vec.append(v)\n",
    "    return full_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Test Documents to pass in, takes about a min\n",
    "final_doc_train = np.stack(full_array(doc_emb_train, data_type=\"Document\"))\n",
    "final_doc_test = np.stack(full_array(doc_emb_test, data_type=\"Document\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Test Questions to pass in, takes about a few seconds\n",
    "final_qn_train = np.stack(full_array(q_emb_train, data_type=\"Question\"))\n",
    "final_qn_test = np.stack(full_array(q_emb_test, data_type=\"Question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(labels):\n",
    "    check = []\n",
    "    for i in labels:\n",
    "        if len(i) < 1675:\n",
    "            while len(i) < 1675:\n",
    "                i.append(\"N\")\n",
    "            check.append(i)\n",
    "        else:\n",
    "            check.append(i)\n",
    "    return check\n",
    "\n",
    "\n",
    "tr_labels = np.array(convert_labels(train_doc_ans_labels))\n",
    "ts_labels = np.array(convert_labels(test_doc_ans_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we found that the sequence length of the documents can get quite large, which might introduce a lot of noise into the model with paddings. One alternative to reduce this noise is to perhaps truncate the sequences down to just over the median for the documents. We also need to do this for the outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Max Seq Length is 1675, Median is 181.0, Number of lines is 630)',\n",
       " 'Max Seq Length is 924, Median is 182.0, Number of lines is 2117)',\n",
       " 'Max Seq Length is 19, Median is 7.0, Number of lines is 630)',\n",
       " 'Max Seq Length is 23, Median is 7.0, Number of lines is 2117)')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_max_length(test_doc_ques[\"Doc_Embeddings\"]), find_max_length(\n",
    "    train_doc_ques[\"Doc_Embeddings\"]\n",
    "), find_max_length(test_doc_ques[\"Q_Embeddings\"]), find_max_length(\n",
    "    train_doc_ques[\"Q_Embeddings\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the embeddings and the labels to 200\n",
    "\n",
    "\n",
    "def truncate(data, labels, max_seq=200):\n",
    "    data = data[:, :max_seq, :]\n",
    "    labels = labels[:, :max_seq]\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "final_doc_train, tr_labels = truncate(final_doc_train, tr_labels)\n",
    "final_doc_test, ts_labels = truncate(final_doc_test, ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final prepared documents are found here:\n",
    "\n",
    "# final_doc_train, tr_labels - doc embeddings and output labels for training\n",
    "# final_doc_test, ts_labels - doc embeddings and output labels for testing\n",
    "# final_qn_train - question embeddings for training\n",
    "# final_qn_test - question embeddings for testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the arrays to a float32, then save those np arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce to float32\n",
    "final_doc_train = final_doc_train.astype(\"float32\")\n",
    "final_doc_test = final_doc_test.astype(\"float32\")\n",
    "final_qn_train = final_qn_train.astype(\"float32\")\n",
    "final_qn_test = final_qn_test.astype(\"float32\")\n",
    "\n",
    "# save the np.arrays\n",
    "np.save(\"./cleaneddata/final_doc_train.npy\", final_doc_train)\n",
    "np.save(\"./cleaneddata/final_doc_test.npy\", final_doc_test)\n",
    "np.save(\"./cleaneddata/final_qn_train.npy\", final_qn_train)\n",
    "np.save(\"./cleaneddata/final_qn_test.npy\", final_qn_test)\n",
    "np.save(\"./cleaneddata/tr_labels.npy\", tr_labels)\n",
    "np.save(\"./cleaneddata/ts_labels.npy\", ts_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Implementation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the data wrangling bit can be quite computationally intensive, so we aim to save the np arrays in our wd, then call it from there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# load in the np data from the cleaneddata folder\n",
    "final_doc_test = np.load(\"cleaneddata/final_doc_test.npy\")\n",
    "final_doc_train = np.load(\"cleaneddata/final_doc_train.npy\")\n",
    "final_qn_train = np.load(\"cleaneddata/final_qn_train.npy\")\n",
    "final_qn_test = np.load(\"cleaneddata/final_qn_test.npy\")\n",
    "tr_labels = np.load(\"cleaneddata/tr_labels.npy\")\n",
    "ts_labels = np.load(\"cleaneddata/ts_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(630, 200, 156) (2117, 200, 156) (2117, 23, 156) (630, 23, 156) (2117, 200) (630, 200)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of all the above\n",
    "print(\n",
    "    final_doc_test.shape,\n",
    "    final_doc_train.shape,\n",
    "    final_qn_train.shape,\n",
    "    final_qn_test.shape,\n",
    "    tr_labels.shape,\n",
    "    ts_labels.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numpy arrays to tensors\n",
    "final_doc_test = torch.from_numpy(final_doc_test).to(device=device, dtype=torch.float32)\n",
    "final_doc_train = torch.from_numpy(final_doc_train).to(\n",
    "    device=device, dtype=torch.float32\n",
    ")\n",
    "final_qn_train = torch.from_numpy(final_qn_train).to(device=device, dtype=torch.float32)\n",
    "final_qn_test = torch.from_numpy(final_qn_test).to(device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([630, 200, 156]) torch.Size([2117, 200, 156]) torch.Size([2117, 23, 156]) torch.Size([630, 23, 156])\n"
     ]
    }
   ],
   "source": [
    "# check the shapes of the tensors\n",
    "print(\n",
    "    final_doc_test.shape,\n",
    "    final_doc_train.shape,\n",
    "    final_qn_train.shape,\n",
    "    final_qn_test.shape,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cTNGfO0h9I3W"
   },
   "source": [
    "In the model input embedding Ablation study, we are given 3 variations of input embeddings to test. We will test 3 options:\n",
    "\n",
    "1. Word2Vec only # 100 dims\n",
    "2. Word2Vec + Tf-IDF # 101 dims\n",
    "3. Word2Vec + all features (TF-IDF, POS, NER) # 156 dims\n",
    "\n",
    "Since we are using tensors, we can use tensor slicing to take out the relevant features.\n",
    "Our tensor of embeddings are built as follows (w2v, TF-IDF, POS, NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensors(tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test, option=3):\n",
    "    if option == 3:\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test\n",
    "    elif option == 1:\n",
    "        tf_doc_train = tf_doc_train[:, :, :100]\n",
    "        tf_doc_test = tf_doc_test[:, :, :100]\n",
    "        tf_qn_train = tf_qn_train[:, :, :100]\n",
    "        tf_qn_test = tf_qn_test[:, :, :100]\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test\n",
    "    elif option == 2:\n",
    "        tf_doc_train = tf_doc_train[:, :, :101]\n",
    "        tf_doc_test = tf_doc_test[:, :, :101]\n",
    "        tf_qn_train = tf_qn_train[:, :, :101]\n",
    "        tf_qn_test = tf_qn_test[:, :, :101]\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from label to index\n",
    "label2index = {\"N\": 0, \"S\": 1, \"I\": 2, \"E\": 3}\n",
    "\n",
    "# Find the maximum length of the label lists\n",
    "max_len = final_doc_train.shape[1]\n",
    "\n",
    "# Create a tensor to hold the one-hot encoded labels\n",
    "train_labels = torch.zeros(\n",
    "    len(tr_labels), max_len, len(label2index), device=device, dtype=torch.float32\n",
    ")\n",
    "test_labels = torch.zeros(\n",
    "    len(ts_labels),\n",
    "    max_len,\n",
    "    len(label2index),\n",
    "    device=device,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Sets the first element of the third dimension of the target_labels tensor to 1\n",
    "train_labels[:, :, 0] = 1\n",
    "test_labels[:, :, 0] = 1\n",
    "\n",
    "# Iterate over the label lists and one-hot encode the labels\n",
    "for i, label_list in enumerate(tr_labels):\n",
    "    for j, label in enumerate(label_list):\n",
    "        index = label2index[label]\n",
    "        # Sets all elements of the target_labels tensor at position (i,j) to 0\n",
    "        train_labels[i, j] = 0\n",
    "        train_labels[i, j, index] = 1\n",
    "\n",
    "for i, label_list in enumerate(ts_labels):\n",
    "    for j, label in enumerate(label_list):\n",
    "        index = label2index[label]\n",
    "        # Sets all elements of the target_labels tensor at position (i,j) to 0\n",
    "        test_labels[i, j] = 0\n",
    "        test_labels[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Reshape the target labels tensor\n",
    "reshaped_target_labels = (\n",
    "    train_labels.view(-1, 4).cpu().numpy()\n",
    ")  # Assuming it's on the GPU\n",
    "\n",
    "# Flatten the reshaped target labels\n",
    "flattened_target_labels = reshaped_target_labels.argmax(axis=1)\n",
    "\n",
    "# Calculate the class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=[0, 1, 2, 3], y=flattened_target_labels\n",
    ")\n",
    "\n",
    "# Convert the class weights to a PyTorch tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing complete at this stage, we should check again the shapes of the tensors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from enum import Enum\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the model for the Document BiLSTM\n",
    "\n",
    "\n",
    "class DocumentBiRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super(DocumentBiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: Tensor):\n",
    "        input = input.unsqueeze(1)\n",
    "        output: Tensor\n",
    "        output, _ = self.lstm(input)\n",
    "        # print(\"document output shape: \", output.shape)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Architecture of the model for the Question BiLSTM\n",
    "\n",
    "\n",
    "class QuestionBiRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super(QuestionBiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: Tensor):\n",
    "        input = input.unsqueeze(1)\n",
    "        output, (hn, cn) = self.lstm(input)\n",
    "        forward_hn = hn[-2, :, :]\n",
    "        backward_hn = hn[-1, :, :]\n",
    "        hidden = torch.cat((forward_hn, backward_hn), dim=-1).unsqueeze(0)\n",
    "        # print(\"question hidden shape: \", hidden.shape)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "# attention methods\n",
    "class AttentionMethod(Enum):\n",
    "    DOT_PRODUCT = \"dot_product\"\n",
    "    SCALE_DOT_PRODUCT = \"scale_dot_product\"\n",
    "    COSINE_SIMILARITY = \"cosine_similarity\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "# Architecture of the model for the Attention Calculation\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ques_len,\n",
    "        hidden_size: int,\n",
    "        attention_method: Literal[\n",
    "            \"dot_product\",\n",
    "            \"scale_dot_product\",\n",
    "            \"cosine_similarity\",\n",
    "        ] = \"dot_product\",\n",
    "    ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.out = nn.Linear(ques_len, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_method = AttentionMethod(attention_method)\n",
    "\n",
    "    def forward(self, document_output, question_summary):\n",
    "        if self.attention_method == AttentionMethod.DOT_PRODUCT:\n",
    "            document_output = document_output.permute(\n",
    "                1, 0, 2\n",
    "            )  # torch.Size([200, 1, 16])\n",
    "            question_summary = question_summary.permute(\n",
    "                1, 2, 0\n",
    "            )  # torch.Size([1, 16, 1])\n",
    "\n",
    "            attention_scores = torch.bmm(document_output, question_summary).permute(\n",
    "                1, 0, 2\n",
    "            )\n",
    "            # get attention weights\n",
    "            attention_weights = nn.functional.softmax(attention_scores, dim=1)\n",
    "            # attention_scores = torch.bmm(document_output, question_summary) / np.sqrt(self.hidden_size)\n",
    "            # get context vector\n",
    "            context_scores = torch.bmm(\n",
    "                document_output.permute(1, 2, 0), attention_weights\n",
    "            ).permute(0, 2, 1)\n",
    "            return context_scores\n",
    "\n",
    "        elif self.attention_method == AttentionMethod.SCALE_DOT_PRODUCT:\n",
    "            document_output = document_output.permute(1, 0, 2)\n",
    "            question_summary = question_summary.permute(1, 2, 0)\n",
    "            attention_scores = torch.bmm(document_output, question_summary).permute(\n",
    "                1, 0, 2\n",
    "            ) / np.sqrt(self.hidden_size)\n",
    "            attention_weights = nn.functional.softmax(attention_scores, dim=1)\n",
    "            context_scores = torch.bmm(\n",
    "                document_output.permute(1, 2, 0), attention_weights\n",
    "            ).permute(0, 2, 1)\n",
    "            return context_scores\n",
    "\n",
    "        elif self.attention_method == AttentionMethod.COSINE_SIMILARITY:\n",
    "            document_output = document_output.permute(1, 0, 2)\n",
    "            question_summary = question_summary.permute(1, 2, 0)\n",
    "            question_summary = question_summary.squeeze(-1)\n",
    "            # cosine similarity attention:\n",
    "            cos_sim = F.cosine_similarity(\n",
    "                document_output, question_summary.unsqueeze(0), dim=-1\n",
    "            ).T.unsqueeze(1)\n",
    "            attention_weights = nn.functional.softmax(cos_sim, dim=1)\n",
    "            context_scores = torch.bmm(\n",
    "                document_output.permute(1, 2, 0), attention_weights\n",
    "            ).permute(0, 2, 1)\n",
    "            return context_scores\n",
    "\n",
    "\n",
    "# Architecture of the model for the Attention Weighted Document Representation a.k.a ReadingComprehension\n",
    "class ReadingComprehensionModel(nn.Module):\n",
    "    def __init__(self, document_rnn, question_rnn, attention, hidden_size, output_size):\n",
    "        super(ReadingComprehensionModel, self).__init__()\n",
    "        self.document_rnn = document_rnn\n",
    "        self.question_rnn = question_rnn\n",
    "        self.attention = attention\n",
    "        self.linear = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.linear2 = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def predict_label(self, attention_output):\n",
    "        attention_output = torch.squeeze(attention_output, 1)\n",
    "        # pass to linear\n",
    "        pred_weights = self.linear(attention_output)\n",
    "        pred_weights = self.linear2(pred_weights)\n",
    "        # get the softmax\n",
    "        # pred_weights = nn.functional.softmax(pred_weights, dim=1)\n",
    "        return pred_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "\n",
    "\n",
    "def trainIter(\n",
    "    model,\n",
    "    document_inputs,\n",
    "    question_inputs,\n",
    "    target_labels,\n",
    "    num_epochs,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    verbose=True,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = 0\n",
    "        for document_input, question_input, target_label in zip(\n",
    "            document_inputs, question_inputs, target_labels\n",
    "        ):\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            document_output = model.document_rnn(document_input)\n",
    "            question_summary = model.question_rnn(question_input)\n",
    "\n",
    "            attention_output = model.attention(document_output, question_summary)\n",
    "\n",
    "            token_label_logits = model.predict_label(attention_output).to(device)\n",
    "\n",
    "            # print(\"token label logits shape: \", token_label_logits)\n",
    "            # print(\"target label shape: \", target_label.shape)\n",
    "            # print(\"token label logits: \", token_label_logits)\n",
    "\n",
    "            # print(token_label_logits[0])\n",
    "            # print(target_label[0])\n",
    "            # raise TypeError(\"stop\")\n",
    "\n",
    "            loss += criterion(token_label_logits, target_label)\n",
    "            optimizer.zero_grad()\n",
    "            # print(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = loss.item() / len(document_inputs)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalutation of the model\n",
    "\n",
    "START_LABEL = 1\n",
    "END_LABEL = 3\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model,\n",
    "    document_inputs,\n",
    "    question_inputs,\n",
    "    target_labels,\n",
    "    criterion,\n",
    "    output_dict=False,\n",
    "    verbose=True,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        for document_input, question_input, target_label in zip(\n",
    "            document_inputs, question_inputs, target_labels\n",
    "        ):\n",
    "            document_output = model.document_rnn(document_input)\n",
    "            question_summary = model.question_rnn(question_input)\n",
    "            attention_output = model.attention(document_output, question_summary)\n",
    "            token_label_logits = model.predict_label(attention_output).to(device)\n",
    "            loss += criterion(token_label_logits, target_label)\n",
    "\n",
    "            # print(token_label_logits)\n",
    "\n",
    "            predictions = token_label_logits.argmax(dim=-1).cpu().numpy()\n",
    "            targets = target_label.argmax(dim=-1).cpu().numpy()\n",
    "            # print(predictions == 1)\n",
    "\n",
    "            if any(targets == START_LABEL) and any(targets == END_LABEL):\n",
    "                # Find indices of start and end tokens\n",
    "                start_token_idx = np.where(targets == START_LABEL)[0]\n",
    "                end_token_idx = np.where(targets == END_LABEL)[0]\n",
    "\n",
    "                # print(\"target: \", targets[start_token_idx[0] : end_token_idx[0] + 1])\n",
    "                # print(\n",
    "                #    \"prediction: \",\n",
    "                #    predictions[start_token_idx[0] : end_token_idx[0] + 1],\n",
    "                # )\n",
    "                # print()\n",
    "\n",
    "                # Take slice of predictions and target_labels for sentence tokens\n",
    "                sentence_prediction = predictions[\n",
    "                    start_token_idx[0] : end_token_idx[0] + 1\n",
    "                ]\n",
    "                sentence_target = targets[start_token_idx[0] : end_token_idx[0] + 1]\n",
    "\n",
    "                all_predictions.extend(sentence_prediction)\n",
    "                all_targets.extend(sentence_target)\n",
    "            else:\n",
    "                # Use the whole document since there is no answer\n",
    "                all_predictions.extend(predictions)\n",
    "                all_targets.extend(targets)\n",
    "\n",
    "        # print(all_predictions)\n",
    "        # print(all_targets)\n",
    "\n",
    "        avg_loss = loss.item() / len(document_inputs)\n",
    "        cr = classification_report(\n",
    "            all_targets, all_predictions, output_dict=output_dict\n",
    "        )\n",
    "\n",
    "        return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior to training\n",
    "as_doc_train, as_doc_test, as_qn_train, as_qn_test = convert_tensors(\n",
    "    final_doc_train, final_doc_test, final_qn_train, final_qn_test, 3\n",
    ")\n",
    "# if not running any ablation, use the free up space by deleting np arrays:\n",
    "# del final_doc_test, final_doc_train, final_qn_train, final_qn_test, tr_labels, ts_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of the training\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "def train(\n",
    "    as_doc_train=as_doc_train,\n",
    "    as_qn_train=as_qn_train,\n",
    "    train_labels=train_labels,\n",
    "    hidden_size=64,\n",
    "    epochs=10,\n",
    "    learning_rate=0.01,\n",
    "    num_layers=1,\n",
    "    token_labels=4,\n",
    "    attention_method: Literal[\n",
    "        \"dot_product\",\n",
    "        \"scale_dot_product\",\n",
    "        \"cosine_similarity\",\n",
    "    ] = \"dot_product\",\n",
    "    verbose=True,\n",
    "):\n",
    "    # note the names of the tensors are changed to:\n",
    "    # as_doc_train, as_doc_test, as_qn_train, as_qn_test, train_labels, test_labels are called before in the ablation part\n",
    "    # to avoid confusion with the original tensors\n",
    "\n",
    "    # as_doc_train, as_doc_test, as_qn_train, as_qn_test\n",
    "\n",
    "    document_num_embeddings = as_doc_train.shape[2]\n",
    "    question_num_embeddings = as_qn_train.shape[2]\n",
    "    ques_len = as_qn_train.shape[1]\n",
    "\n",
    "    document_rnn = DocumentBiRNN(\n",
    "        hidden_size=hidden_size,\n",
    "        input_size=document_num_embeddings,\n",
    "        num_layers=num_layers,\n",
    "    ).to(device)\n",
    "    question_rnn = QuestionBiRNN(\n",
    "        input_size=question_num_embeddings,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "    ).to(device)\n",
    "    attention = Attention(ques_len, hidden_size, attention_method).to(device)\n",
    "    reading_comp = ReadingComprehensionModel(\n",
    "        document_rnn,\n",
    "        question_rnn,\n",
    "        attention,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=token_labels,\n",
    "    ).to(device)\n",
    "    reading_comp_optimizer = optim.AdamW(reading_comp.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        weight=class_weights\n",
    "    )  # to account for imbalanced class weights\n",
    "\n",
    "    trainIter(\n",
    "        reading_comp,\n",
    "        as_doc_train,\n",
    "        as_qn_train,\n",
    "        train_labels,\n",
    "        epochs,\n",
    "        criterion,\n",
    "        reading_comp_optimizer,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    return reading_comp, criterion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Model Testing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Input Embedding Ablation Study\n",
    "\n",
    "The above model used the full context vector with all word embeddings taken (Word2Vec, POS, NER, TF-IDF). In this section, we want to study the results of:\n",
    "\n",
    "1. Word2Vec Word embeddings only\n",
    "2. Word2Vec + TF-IDF\n",
    "3. Full vector, which we have ran the results above\n",
    "\n",
    "As we have seen from the above study that the best attention was the dot product, we will standardise across the 3 experiments using this. Additionally, we use the same hyperparameters as above in the Attention Study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2117, 200, 100]) torch.Size([630, 200, 100]) torch.Size([2117, 23, 100]) torch.Size([630, 23, 100])\n"
     ]
    }
   ],
   "source": [
    "# Word embeds only, important to run this as it also makes sure that the input size is correct\n",
    "as_doc_train, as_doc_test, as_qn_train, as_qn_test = convert_tensors(\n",
    "    final_doc_train, final_doc_test, final_qn_train, final_qn_test, 1\n",
    ")\n",
    "# check the tensor sizes\n",
    "print(as_doc_train.shape, as_doc_test.shape, as_qn_train.shape, as_qn_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec Word Embeddings Ablation Study**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3892\n",
      "Epoch 2/10, Loss: 1.3099\n",
      "Epoch 3/10, Loss: 1.1735\n",
      "Epoch 4/10, Loss: 1.0318\n",
      "Epoch 5/10, Loss: 0.8983\n",
      "Epoch 6/10, Loss: 0.8239\n",
      "Epoch 7/10, Loss: 0.9530\n",
      "Epoch 8/10, Loss: 0.8242\n",
      "Epoch 9/10, Loss: 0.7999\n",
      "Epoch 10/10, Loss: 0.7199\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Model evaluation for Word Embeds only\n",
    "# testing with cosine similarity attention\n",
    "model_path_word = \"./pytorch/word_embeds_model.pt\"\n",
    "\n",
    "if os.path.exists(model_path_word):\n",
    "    reading_comp_word = torch.load(model_path_word)\n",
    "    criterion_word = nn.CrossEntropyLoss(\n",
    "        weight=class_weights\n",
    "    )  # to account for imbalanced class weights\n",
    "else:\n",
    "    reading_comp_word, criterion_word = train(\n",
    "        as_doc_train=as_doc_train,\n",
    "        as_qn_train=as_qn_train,\n",
    "        train_labels=train_labels,\n",
    "        attention_method=\"dot_product\",\n",
    "    )\n",
    "    torch.save(reading_comp_word, model_path_word)\n",
    "\n",
    "word_train_report, word_test_report = evaluate(\n",
    "    reading_comp_word, as_doc_train, as_qn_train, train_labels, criterion_word\n",
    "), evaluate(reading_comp_word, as_doc_test, as_qn_test, test_labels, criterion_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.37      0.54    260691\n",
      "           1       0.04      0.89      0.07       826\n",
      "           2       0.11      0.78      0.20     19947\n",
      "           3       0.02      0.88      0.05       812\n",
      "\n",
      "    accuracy                           0.40    282276\n",
      "   macro avg       0.29      0.73      0.21    282276\n",
      "weighted avg       0.92      0.40      0.51    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.35      0.51     80177\n",
      "           1       0.03      0.77      0.06       230\n",
      "           2       0.10      0.77      0.17      5295\n",
      "           3       0.02      0.78      0.04       229\n",
      "\n",
      "    accuracy                           0.37     85931\n",
      "   macro avg       0.28      0.67      0.19     85931\n",
      "weighted avg       0.92      0.37      0.49     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation for train and test set\n",
    "print(\"Evaluation on train set\")\n",
    "print(word_train_report)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Evaluation on test set\")\n",
    "print(word_test_report)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec + TF-IDF**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2117, 200, 101]) torch.Size([630, 200, 101]) torch.Size([2117, 23, 101]) torch.Size([630, 23, 101])\n"
     ]
    }
   ],
   "source": [
    "# Word embeds + TF-IDF, important to run this as it also makes sure that the input size is correct\n",
    "as_doc_train, as_doc_test, as_qn_train, as_qn_test = convert_tensors(\n",
    "    final_doc_train, final_doc_test, final_qn_train, final_qn_test, 2\n",
    ")\n",
    "# check the tensor sizes\n",
    "print(as_doc_train.shape, as_doc_test.shape, as_qn_train.shape, as_qn_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with cosine similarity attention\n",
    "model_path_tfidf = \"./pytorch/word_embeds_tfidf_model.pt\"\n",
    "\n",
    "if os.path.exists(model_path_tfidf):\n",
    "    reading_comp_tfidf = torch.load(model_path_tfidf)\n",
    "    criterion_tfidf = nn.CrossEntropyLoss(\n",
    "        weight=class_weights\n",
    "    )  # to account for imbalanced class weights\n",
    "else:\n",
    "    reading_comp_tfidf, criterion_tfidf = train(\n",
    "        as_doc_train=as_doc_train,\n",
    "        as_qn_train=as_qn_train,\n",
    "        train_labels=train_labels,\n",
    "        attention_method=\"dot_product\",\n",
    "    )\n",
    "    torch.save(reading_comp_tfidf, model_path_tfidf)\n",
    "\n",
    "tfidf_train_report, tfidf_test_report = evaluate(\n",
    "    reading_comp_tfidf, as_doc_train, as_qn_train, train_labels, criterion_tfidf\n",
    "), evaluate(reading_comp_tfidf, as_doc_test, as_qn_test, test_labels, criterion_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.55      0.71    260691\n",
      "           1       0.05      0.86      0.09       826\n",
      "           2       0.15      0.65      0.24     19947\n",
      "           3       0.02      0.93      0.05       812\n",
      "\n",
      "    accuracy                           0.56    282276\n",
      "   macro avg       0.30      0.75      0.27    282276\n",
      "weighted avg       0.91      0.56      0.67    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.53      0.69     80177\n",
      "           1       0.04      0.79      0.08       230\n",
      "           2       0.12      0.62      0.20      5295\n",
      "           3       0.02      0.83      0.04       229\n",
      "\n",
      "    accuracy                           0.54     85931\n",
      "   macro avg       0.29      0.69      0.25     85931\n",
      "weighted avg       0.92      0.54      0.65     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation for train and test set\n",
    "print(\"Evaluation on train set\")\n",
    "print(tfidf_train_report)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Evaluation on test set\")\n",
    "print(tfidf_test_report)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Embeddings Ablation Study Table**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Input Embeddings              Precision    Recall    F1-score\n",
      "--------------------------  -----------  --------  ----------\n",
      "Word2Vec                       0.285386  0.702875    0.193889\n",
      "Word2Vec, TF-IDF               0.295407  0.724489    0.250477\n",
      "Word2Vec, POS, NER, TF-IDF     0.286895  0.722808    0.177364\n",
      "----------------------------------------------------------\n",
      "Test Data:\n",
      "Input Embeddings              Precision    Recall    F1-score\n",
      "--------------------------  -----------  --------  ----------\n",
      "Word2Vec                       0.278921  0.652814    0.177574\n",
      "Word2Vec, TF-IDF               0.285429  0.652082    0.22768\n",
      "Word2Vec, POS, NER, TF-IDF     0.281041  0.686869    0.161531\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "tfidf_train_report, tfidf_test_report = evaluate(\n",
    "    reading_comp_tfidf,\n",
    "    as_doc_train,\n",
    "    as_qn_train,\n",
    "    train_labels,\n",
    "    criterion_tfidf,\n",
    "    output_dict=True,\n",
    "    verbose=False,\n",
    "), evaluate(\n",
    "    reading_comp_tfidf,\n",
    "    as_doc_test,\n",
    "    as_qn_test,\n",
    "    test_labels,\n",
    "    criterion_tfidf,\n",
    "    output_dict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "as_doc_train, as_doc_test, as_qn_train, as_qn_test = convert_tensors(\n",
    "    final_doc_train, final_doc_test, final_qn_train, final_qn_test, 1\n",
    ")\n",
    "\n",
    "word_train_report, word_test_report = evaluate(\n",
    "    reading_comp_word,\n",
    "    as_doc_train,\n",
    "    as_qn_train,\n",
    "    train_labels,\n",
    "    criterion_word,\n",
    "    output_dict=True,\n",
    "    verbose=False,\n",
    "), evaluate(\n",
    "    reading_comp_word,\n",
    "    as_doc_test,\n",
    "    as_qn_test,\n",
    "    test_labels,\n",
    "    criterion_word,\n",
    "    output_dict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "as_doc_train, as_doc_test, as_qn_train, as_qn_test = convert_tensors(\n",
    "    final_doc_train, final_doc_test, final_qn_train, final_qn_test, 3\n",
    ")\n",
    "\n",
    "model_path_dot = \"./pytorch/dot_product_model.pt\"\n",
    "\n",
    "if os.path.exists(model_path_dot):\n",
    "    reading_comp_dot = torch.load(model_path_dot)\n",
    "    criterion_dot = nn.CrossEntropyLoss(\n",
    "        weight=class_weights\n",
    "    )  # to account for imbalanced class weights\n",
    "else:\n",
    "    reading_comp_dot, criterion_dot = train(attention_method=\"dot_product\")\n",
    "    torch.save(reading_comp_dot, model_path_dot)\n",
    "\n",
    "# Model evaluation\n",
    "train_report, test_report = evaluate(\n",
    "    reading_comp_dot, as_doc_train, as_qn_train, train_labels, criterion_dot\n",
    "), evaluate(reading_comp_dot, as_doc_test, as_qn_test, test_labels, criterion_dot)\n",
    "\n",
    "# Model evaluation\n",
    "train_report, test_report = evaluate(\n",
    "    reading_comp_dot,\n",
    "    as_doc_train,\n",
    "    as_qn_train,\n",
    "    train_labels,\n",
    "    criterion_dot,\n",
    "    output_dict=True,\n",
    "    verbose=False,\n",
    "), evaluate(\n",
    "    reading_comp_dot,\n",
    "    as_doc_test,\n",
    "    as_qn_test,\n",
    "    test_labels,\n",
    "    criterion_dot,\n",
    "    output_dict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Assuming you have already trained and evaluated models for all 3 attention methods\n",
    "# and have obtained the classification reports for each method\n",
    "reports = {\n",
    "    \"Word2Vec\": {\n",
    "        \"train\": word_train_report,\n",
    "        \"test\": word_test_report,\n",
    "    },\n",
    "    \"Word2Vec, TF-IDF\": {\n",
    "        \"train\": tfidf_train_report,\n",
    "        \"test\": tfidf_test_report,\n",
    "    },\n",
    "    \"Word2Vec, POS, NER, TF-IDF\": {\"train\": train_report, \"test\": test_report},\n",
    "}\n",
    "\n",
    "# Create tables for train and test data\n",
    "for data_type in [\"train\", \"test\"]:\n",
    "    table = []\n",
    "    headers = [\"Input Embeddings\", \"Precision\", \"Recall\", \"F1-score\"]\n",
    "    for method, report in reports.items():\n",
    "        precision = report[data_type][\"macro avg\"][\"precision\"]\n",
    "        recall = report[data_type][\"macro avg\"][\"recall\"]\n",
    "        f1_score = report[data_type][\"macro avg\"][\"f1-score\"]\n",
    "        table.append([method, precision, recall, f1_score])\n",
    "    print(f\"{data_type.capitalize()} Data:\")\n",
    "    print(tabulate(table, headers=headers))\n",
    "    print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Attention Ablation Study\n",
    "\n",
    "In this section, we study 3 different type of attention mechanisms between the question model and the document model. We ensured that the 3 attention mechanisms are ran on the same model hyperparameters, so as to keep things interpretable and standardized across the study.\n",
    "\n",
    "The hyperparameters of the training model are as follows:\n",
    "\n",
    "-   RNN (Bi-LSTM) Hidden Size: 64,\n",
    "-   Number of epochs: 10,\n",
    "-   Learning Rate: 0.01,\n",
    "-   Number of RNN (Bi-LSTM) layers: 1\n",
    "\n",
    "**Attention Ablation Study - Dot Product**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_dot = \"./pytorch/dot_product_model.pt\"\n",
    "\n",
    "if os.path.exists(model_path_dot):\n",
    "    reading_comp_dot = torch.load(model_path_dot)\n",
    "    criterion_dot = nn.CrossEntropyLoss(\n",
    "        weight=class_weights\n",
    "    )  # to account for imbalanced class weights\n",
    "else:\n",
    "    reading_comp_dot, criterion_dot = train(attention_method=\"dot_product\")\n",
    "    torch.save(reading_comp_dot, model_path_dot)\n",
    "\n",
    "# Model evaluation\n",
    "train_report, test_report = evaluate(\n",
    "    reading_comp_dot, as_doc_train, as_qn_train, train_labels, criterion_dot\n",
    "), evaluate(reading_comp_dot, as_doc_test, as_qn_test, test_labels, criterion_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.28      0.44    260691\n",
      "           1       0.03      0.92      0.05       826\n",
      "           2       0.11      0.73      0.19     19947\n",
      "           3       0.02      0.96      0.03       812\n",
      "\n",
      "    accuracy                           0.32    282276\n",
      "   macro avg       0.29      0.72      0.18    282276\n",
      "weighted avg       0.93      0.32      0.42    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.26      0.41     80177\n",
      "           1       0.02      0.84      0.04       230\n",
      "           2       0.09      0.73      0.16      5295\n",
      "           3       0.02      0.92      0.03       229\n",
      "\n",
      "    accuracy                           0.29     85931\n",
      "   macro avg       0.28      0.69      0.16     85931\n",
      "weighted avg       0.94      0.29      0.39     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation for train and test set\n",
    "print(\"Evaluation on train set\")\n",
    "print(train_report)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Evaluation on test set\")\n",
    "print(test_report)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Ablation Study - Scaled Dot Product**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with scaled dot product attention\n",
    "model_path_scaled = \"./pytorch/scale_dot_product_model.pt\"\n",
    "\n",
    "if os.path.exists(model_path_scaled):\n",
    "    reading_comp_scaled = torch.load(model_path_scaled)\n",
    "    criterion_scaled = nn.CrossEntropyLoss(\n",
    "        weight=class_weights\n",
    "    )  # to account for imbalanced class weights\n",
    "else:\n",
    "    reading_comp_scaled, criterion_scaled = train(attention_method=\"scale_dot_product\")\n",
    "    torch.save(reading_comp_scaled, model_path_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.34      0.50    260691\n",
      "           1       0.02      0.92      0.03       826\n",
      "           2       0.11      0.65      0.19     19947\n",
      "           3       0.02      0.90      0.05       812\n",
      "\n",
      "    accuracy                           0.36    282276\n",
      "   macro avg       0.29      0.70      0.19    282276\n",
      "weighted avg       0.92      0.36      0.48    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.32      0.48     80177\n",
      "           1       0.01      0.84      0.03       230\n",
      "           2       0.10      0.65      0.17      5295\n",
      "           3       0.02      0.80      0.04       229\n",
      "\n",
      "    accuracy                           0.34     85931\n",
      "   macro avg       0.28      0.65      0.18     85931\n",
      "weighted avg       0.93      0.34      0.46     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "scaled_train_report, scaled_test_report = evaluate(\n",
    "    reading_comp_scaled, as_doc_train, as_qn_train, train_labels, criterion_scaled\n",
    "), evaluate(reading_comp_scaled, as_doc_test, as_qn_test, test_labels, criterion_scaled)\n",
    "\n",
    "# model evaluation for train and test set\n",
    "print(\"Evaluation on train set\")\n",
    "print(scaled_train_report)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Evaluation on test set\")\n",
    "print(scaled_test_report)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Ablation Study - Cosine Similarity**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with cosine similarity attention\n",
    "model_path_cosine = \"./pytorch/cosine_similarity_model.pt\"\n",
    "\n",
    "if os.path.exists(model_path_cosine):\n",
    "    reading_comp_cosine = torch.load(model_path_cosine)\n",
    "    criterion_cosine = nn.CrossEntropyLoss(\n",
    "        weight=class_weights\n",
    "    )  # to account for imbalanced class weights\n",
    "else:\n",
    "    reading_comp_cosine, criterion_cosine = train(attention_method=\"cosine_similarity\")\n",
    "    torch.save(reading_comp_cosine, model_path_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.48      0.64    260691\n",
      "           1       0.04      0.87      0.07       826\n",
      "           2       0.13      0.76      0.22     19947\n",
      "           3       0.04      0.80      0.07       812\n",
      "\n",
      "    accuracy                           0.50    282276\n",
      "   macro avg       0.30      0.72      0.25    282276\n",
      "weighted avg       0.91      0.50      0.61    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.45      0.61     80177\n",
      "           1       0.03      0.77      0.06       230\n",
      "           2       0.10      0.75      0.18      5295\n",
      "           3       0.03      0.64      0.06       229\n",
      "\n",
      "    accuracy                           0.47     85931\n",
      "   macro avg       0.29      0.65      0.23     85931\n",
      "weighted avg       0.92      0.47      0.58     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# model evaluation for train and test set\n",
    "cosine_train_report, cosine_test_report = evaluate(\n",
    "    reading_comp_cosine, as_doc_train, as_qn_train, train_labels, criterion_cosine\n",
    "), evaluate(reading_comp_cosine, as_doc_test, as_qn_test, test_labels, criterion_cosine)\n",
    "\n",
    "print(\"Evaluation on train set\")\n",
    "print(cosine_train_report)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Evaluation on test set\")\n",
    "print(cosine_test_report)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Methods Table**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Attention Method      Precision    Recall    F1-score\n",
      "------------------  -----------  --------  ----------\n",
      "dot_product            0.286895  0.722808    0.177364\n",
      "scaled_dot_product     0.285386  0.702875    0.193889\n",
      "cosine_similarity      0.295407  0.724489    0.250477\n",
      "----------------------------------------------------------\n",
      "Test Data:\n",
      "Attention Method      Precision    Recall    F1-score\n",
      "------------------  -----------  --------  ----------\n",
      "dot_product            0.281041  0.686869    0.161531\n",
      "scaled_dot_product     0.278921  0.652814    0.177574\n",
      "cosine_similarity      0.285429  0.652082    0.22768\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Model evaluation\n",
    "train_report, test_report = evaluate(\n",
    "    reading_comp_dot,\n",
    "    as_doc_train,\n",
    "    as_qn_train,\n",
    "    train_labels,\n",
    "    criterion_dot,\n",
    "    output_dict=True,\n",
    "    verbose=False,\n",
    "), evaluate(\n",
    "    reading_comp_dot,\n",
    "    as_doc_test,\n",
    "    as_qn_test,\n",
    "    test_labels,\n",
    "    criterion_dot,\n",
    "    output_dict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "scaled_train_report, scaled_test_report = evaluate(\n",
    "    reading_comp_scaled,\n",
    "    as_doc_train,\n",
    "    as_qn_train,\n",
    "    train_labels,\n",
    "    criterion_scaled,\n",
    "    output_dict=True,\n",
    "    verbose=False,\n",
    "), evaluate(\n",
    "    reading_comp_scaled,\n",
    "    as_doc_test,\n",
    "    as_qn_test,\n",
    "    test_labels,\n",
    "    criterion_scaled,\n",
    "    output_dict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "cosine_train_report, cosine_test_report = evaluate(\n",
    "    reading_comp_cosine,\n",
    "    as_doc_train,\n",
    "    as_qn_train,\n",
    "    train_labels,\n",
    "    criterion_cosine,\n",
    "    output_dict=True,\n",
    "    verbose=False,\n",
    "), evaluate(\n",
    "    reading_comp_cosine,\n",
    "    as_doc_test,\n",
    "    as_qn_test,\n",
    "    test_labels,\n",
    "    criterion_cosine,\n",
    "    output_dict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Assuming you have already trained and evaluated models for all 3 attention methods\n",
    "# and have obtained the classification reports for each method\n",
    "reports = {\n",
    "    \"dot_product\": {\"train\": train_report, \"test\": test_report},\n",
    "    \"scaled_dot_product\": {\n",
    "        \"train\": scaled_train_report,\n",
    "        \"test\": scaled_test_report,\n",
    "    },\n",
    "    \"cosine_similarity\": {\n",
    "        \"train\": cosine_train_report,\n",
    "        \"test\": cosine_test_report,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create tables for train and test data\n",
    "for data_type in [\"train\", \"test\"]:\n",
    "    table = []\n",
    "    headers = [\"Attention Method\", \"Precision\", \"Recall\", \"F1-score\"]\n",
    "    for method, report in reports.items():\n",
    "        precision = report[data_type][\"macro avg\"][\"precision\"]\n",
    "        recall = report[data_type][\"macro avg\"][\"recall\"]\n",
    "        f1_score = report[data_type][\"macro avg\"][\"f1-score\"]\n",
    "        table.append([method, precision, recall, f1_score])\n",
    "    print(f\"{data_type.capitalize()} Data:\")\n",
    "    print(tabulate(table, headers=headers))\n",
    "    print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Hyper Parameter Testing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will fine tune the hyperparameter of the model to see if we can get better results. We will be testing the following hyperparameter: learning rate [0.00001, 0.0001, 0.001, 0.01, 0.1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-20 15:15:50,488]\u001b[0m Using an existing study with name 'reading_comprehension' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study reading_comprehension already exists in storage. Skipping optimization.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import json\n",
    "\n",
    "\n",
    "mean_cv_precision_scores = {}\n",
    "mean_cv_recall_scores = {}\n",
    "mean_cv_f1_scores = {}\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to search over\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "\n",
    "    # Create a KFold object for cross-validation\n",
    "    kf = KFold(n_splits=3)\n",
    "\n",
    "    # Initialize a list to store the cross-validation scores\n",
    "    cv_precision_scores = []\n",
    "    cv_recall_scores = []\n",
    "    cv_f1_scores = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, val_index in kf.split(as_doc_train):\n",
    "        # Split the data into training and validation sets\n",
    "        doc_train, doc_val = as_doc_train[train_index], as_doc_train[val_index]\n",
    "        que_train, que_val = as_qn_train[train_index], as_qn_train[val_index]\n",
    "        label_train, label_val = train_labels[train_index], train_labels[val_index]\n",
    "\n",
    "        # Train the model on the training set\n",
    "        reading_comp_dot, criterion_dot = train(\n",
    "            doc_train,\n",
    "            que_train,\n",
    "            label_train,\n",
    "            attention_method=\"dot_product\",\n",
    "            learning_rate=learning_rate,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        val_report = evaluate(\n",
    "            reading_comp_dot,\n",
    "            doc_val,\n",
    "            que_val,\n",
    "            label_val,\n",
    "            criterion_dot,\n",
    "            output_dict=True,\n",
    "            verbose=False,\n",
    "        )\n",
    "        # Store the validation score\n",
    "        cv_precision_scores.append(val_report[\"macro avg\"][\"precision\"])\n",
    "        cv_recall_scores.append(val_report[\"macro avg\"][\"recall\"])\n",
    "        cv_f1_scores.append(val_report[\"macro avg\"][\"f1-score\"])\n",
    "    print(f\"Learning rate: {learning_rate:.4f}, F1: {np.mean(cv_f1_scores):.4f}\")\n",
    "\n",
    "    mean_cv_precision_scores[learning_rate] = np.mean(cv_precision_scores)\n",
    "    mean_cv_recall_scores[learning_rate] = np.mean(cv_recall_scores)\n",
    "    mean_cv_f1_scores[learning_rate] = np.mean(cv_f1_scores)\n",
    "\n",
    "    # Return the average cross-validation score\n",
    "    return np.mean(cv_f1_scores)\n",
    "\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study_name = \"reading_comprehension\"  # Unique identifier of the study.\n",
    "storage_name = f\"sqlite:///./optuna/{study_name}.db\"\n",
    "sampler = optuna.samplers.GridSampler({\"learning_rate\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]})\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=storage_name,\n",
    "    load_if_exists=True,\n",
    "    direction=\"maximize\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "# Check if the study is new\n",
    "if len(study.trials) == 0:\n",
    "    # Optimize only for new study\n",
    "    study.optimize(objective, n_trials=5, n_jobs=1, show_progress_bar=True)\n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(\"./optuna/precision.json\", \"w\") as f:\n",
    "        json.dump(mean_cv_precision_scores, f)\n",
    "    with open(\"./optuna/recall.json\", \"w\") as f:\n",
    "        json.dump(mean_cv_recall_scores, f)\n",
    "    with open(\"./optuna/f1.json\", \"w\") as f:\n",
    "        json.dump(mean_cv_f1_scores, f)\n",
    "else:\n",
    "    print(f\"Study {study_name} already exists in storage. Skipping optimization.\")\n",
    "    with open(\"./optuna/precision.json\", \"r\") as f:\n",
    "        mean_cv_precision_scores = json.load(f)\n",
    "    with open(\"./optuna/recall.json\", \"r\") as f:\n",
    "        mean_cv_recall_scores = json.load(f)\n",
    "    with open(\"./optuna/f1.json\", \"r\") as f:\n",
    "        mean_cv_f1_scores = json.load(f)\n",
    "    # Create new dictionaries with the keys converted to integers\n",
    "    new_mean_cv_precision_scores = {}\n",
    "    new_mean_cv_recall_scores = {}\n",
    "    new_mean_cv_f1_scores = {}\n",
    "\n",
    "    for key, value in mean_cv_precision_scores.items():\n",
    "        new_mean_cv_precision_scores[float(key)] = value\n",
    "    for key, value in mean_cv_recall_scores.items():\n",
    "        new_mean_cv_recall_scores[float(key)] = value\n",
    "    for key, value in mean_cv_f1_scores.items():\n",
    "        new_mean_cv_f1_scores[float(key)] = value\n",
    "\n",
    "    # Update the original dictionaries with the new dictionaries\n",
    "    mean_cv_precision_scores = new_mean_cv_precision_scores\n",
    "    mean_cv_recall_scores = new_mean_cv_recall_scores\n",
    "    mean_cv_f1_scores = new_mean_cv_f1_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Best Hyperparameter**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph of Precision, Recall and F1 score vs Learning Rate**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "\n",
    "def plot():\n",
    "    if mean_cv_precision_scores == {} or mean_cv_recall_scores == {} or mean_cv_f1_scores == {}:\n",
    "        print(\"Please delete the database and run the objective function again to get the scores.\")\n",
    "        return\n",
    "    # Assuming you have already optimized the study and have obtained the mean cross-validation scores\n",
    "    # for precision, recall, and F1-score for each learning rate\n",
    "    learning_rates = [1e-1, 1e-2,1e-3, 1e-4, 1e-5]\n",
    "    # Extract the scores from the dictionaries in the order of the learning rates\n",
    "    precision_scores_plt = [mean_cv_precision_scores[lr] for lr in learning_rates]\n",
    "    recall_scores_plt = [mean_cv_recall_scores[lr] for lr in learning_rates]\n",
    "    f1_scores_plt = [mean_cv_f1_scores[lr] for lr in learning_rates]\n",
    "\n",
    "    # Create a figure with a single subplot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot the precision scores\n",
    "    ax.plot(learning_rates, precision_scores_plt, label=\"Precision\")\n",
    "\n",
    "    # Plot the recall scores\n",
    "    ax.plot(learning_rates, recall_scores_plt, label=\"Recall\")\n",
    "\n",
    "    # Plot the F1 scores\n",
    "    ax.plot(learning_rates, f1_scores_plt, label=\"F1-score\")\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    ax.set_xlabel(\"Learning Rate\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_title(\"Precision, Recall, and F1-score vs. Learning Rate\")\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
