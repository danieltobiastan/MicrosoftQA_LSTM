{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Implementation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the data wrangling bit can be quite computationally intensive, and\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# load in the np data from the cleaneddata folder\n",
    "final_doc_test = np.load(\"cleaneddata/final_doc_test.npy\")\n",
    "final_doc_train = np.load(\"cleaneddata/final_doc_train.npy\")\n",
    "final_qn_train = np.load(\"cleaneddata/final_qn_train.npy\")\n",
    "final_qn_test = np.load(\"cleaneddata/final_qn_test.npy\")\n",
    "tr_labels = np.load(\"cleaneddata/tr_labels.npy\")\n",
    "ts_labels = np.load(\"cleaneddata/ts_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(630, 200, 156) (2117, 200, 156) (2117, 23, 156) (630, 23, 156) (2117, 200) (630, 200)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of all the above\n",
    "print(\n",
    "    final_doc_test.shape,\n",
    "    final_doc_train.shape,\n",
    "    final_qn_train.shape,\n",
    "    final_qn_test.shape,\n",
    "    tr_labels.shape,\n",
    "    ts_labels.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numpy arrays to tensors\n",
    "final_doc_test = torch.from_numpy(final_doc_test).to(device=device, dtype=torch.float32)\n",
    "final_doc_train = torch.from_numpy(final_doc_train).to(\n",
    "    device=device, dtype=torch.float32\n",
    ")\n",
    "final_qn_train = torch.from_numpy(final_qn_train).to(device=device, dtype=torch.float32)\n",
    "final_qn_test = torch.from_numpy(final_qn_test).to(device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([630, 200, 156]) torch.Size([2117, 200, 156]) torch.Size([2117, 23, 156]) torch.Size([630, 23, 156])\n"
     ]
    }
   ],
   "source": [
    "# check the shapes of the tensors\n",
    "print(\n",
    "    final_doc_test.shape,\n",
    "    final_doc_train.shape,\n",
    "    final_qn_train.shape,\n",
    "    final_qn_test.shape,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Embedding Ablation Study**\n",
    "\n",
    "In the model input embedding Ablation study, we are given 3 variations of input embeddings to test. We will test 3 options:\n",
    "\n",
    "1. Word2Vec only # 100 dims\n",
    "2. Word2Vec + Tf-IDF # 101 dims\n",
    "3. Word2Vec + all features (TF-IDF, POS, NER) # 156 dims\n",
    "\n",
    "Since we are using tensors, we can use tensor slicing to take out the relevant features.\n",
    "Our tensor of embeddings are built as follows (w2v, TF-IDF, POS, NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensors(tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test, option=3):\n",
    "    if option == 3:\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test\n",
    "    elif option == 1:\n",
    "        tf_doc_train = tf_doc_train[:, :, :100]\n",
    "        tf_doc_test = tf_doc_test[:, :, :100]\n",
    "        tf_qn_train = tf_qn_train[:, :, :100]\n",
    "        tf_qn_test = tf_qn_test[:, :, :100]\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test\n",
    "    elif option == 2:\n",
    "        tf_doc_train = tf_doc_train[:, :, :101]\n",
    "        tf_doc_test = tf_doc_test[:, :, :101]\n",
    "        tf_qn_train = tf_qn_train[:, :, :101]\n",
    "        tf_qn_test = tf_qn_test[:, :, :101]\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from label to index\n",
    "label2index = {\"N\": 0, \"S\": 1, \"I\": 2, \"E\": 3}\n",
    "\n",
    "# Find the maximum length of the label lists\n",
    "max_len = final_doc_train.shape[1]\n",
    "\n",
    "# Create a tensor to hold the one-hot encoded labels\n",
    "train_labels = torch.zeros(\n",
    "    len(tr_labels), max_len, len(label2index), device=device, dtype=torch.float32\n",
    ")\n",
    "test_labels = torch.zeros(\n",
    "    len(ts_labels),\n",
    "    max_len,\n",
    "    len(label2index),\n",
    "    device=device,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Sets the first element of the third dimension of the target_labels tensor to 1\n",
    "train_labels[:, :, 0] = 1\n",
    "test_labels[:, :, 0] = 1\n",
    "\n",
    "# Iterate over the label lists and one-hot encode the labels\n",
    "for i, label_list in enumerate(tr_labels):\n",
    "    for j, label in enumerate(label_list):\n",
    "        index = label2index[label]\n",
    "        # Sets all elements of the target_labels tensor at position (i,j) to 0\n",
    "        train_labels[i, j] = 0\n",
    "        train_labels[i, j, index] = 1\n",
    "\n",
    "for i, label_list in enumerate(ts_labels):\n",
    "    for j, label in enumerate(label_list):\n",
    "        index = label2index[label]\n",
    "        # Sets all elements of the target_labels tensor at position (i,j) to 0\n",
    "        test_labels[i, j] = 0\n",
    "        test_labels[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Reshape the target labels tensor\n",
    "reshaped_target_labels = (\n",
    "    train_labels.view(-1, 4).cpu().numpy()\n",
    ")  # Assuming it's on the GPU\n",
    "\n",
    "# Flatten the reshaped target labels\n",
    "flattened_target_labels = reshaped_target_labels.argmax(axis=1)\n",
    "\n",
    "# Calculate the class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=[0, 1, 2, 3], y=flattened_target_labels\n",
    ")\n",
    "\n",
    "# Convert the class weights to a PyTorch tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing complete at this stage, we should check again the shapes of the tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior to training\n",
    "as_doc_train, as_doc_test, as_qn_train, as_qn_test = convert_tensors(\n",
    "    final_doc_train, final_doc_test, final_qn_train, final_qn_test, 3\n",
    ")\n",
    "# if not running any ablation, use the free up space by deleting np arrays:\n",
    "# del final_doc_test, final_doc_train, final_qn_train, final_qn_test, tr_labels, ts_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2117, 200, 156]) torch.Size([630, 200, 156]) torch.Size([2117, 23, 156]) torch.Size([630, 23, 156]) torch.Size([2117, 200, 4]) torch.Size([630, 200, 4])\n"
     ]
    }
   ],
   "source": [
    "# check final tensor shapes of all the tensors, note we use ablation study tensors here, change option to 3 if using full tensors, or 2 if using 101\n",
    "print(\n",
    "    as_doc_train.shape,\n",
    "    as_doc_test.shape,\n",
    "    as_qn_train.shape,\n",
    "    as_qn_test.shape,\n",
    "    train_labels.shape,\n",
    "    test_labels.shape,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from enum import Enum\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the model for the Document BiLSTM\n",
    "\n",
    "\n",
    "class DocumentBiRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super(DocumentBiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: Tensor):\n",
    "        input = input.unsqueeze(1)\n",
    "        output: Tensor\n",
    "        output, _ = self.lstm(input)\n",
    "        # print(\"document output shape: \", output.shape)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Architecture of the model for the Question BiLSTM\n",
    "\n",
    "\n",
    "class QuestionBiRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super(QuestionBiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: Tensor):\n",
    "        input = input.unsqueeze(1)\n",
    "        output, (hn, cn) = self.lstm(input)\n",
    "        forward_hn = hn[-2, :, :]\n",
    "        backward_hn = hn[-1, :, :]\n",
    "        hidden = torch.cat((forward_hn, backward_hn), dim=-1).unsqueeze(0)\n",
    "        # print(\"question hidden shape: \", hidden.shape)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "# Architecture of the model for the Attention Calculation\n",
    "\n",
    "\n",
    "class AttentionMethod(Enum):\n",
    "    DOT_PRODUCT = \"dot_product\"\n",
    "    SCALE_DOT_PRODUCT = \"scale_dot_product\"\n",
    "    COSINE_SIMILARITY = \"cosine_similarity\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        attention_method: Literal[\n",
    "            \"dot_product\",\n",
    "            \"scale_dot_product\",\n",
    "            \"cosine_similarity\",\n",
    "        ] = \"dot_product\",\n",
    "    ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_method = AttentionMethod(attention_method)\n",
    "\n",
    "    def forward(self, document_output, question_summary):\n",
    "        if self.attention_method == AttentionMethod.DOT_PRODUCT:\n",
    "            document_output = document_output.permute(\n",
    "                1, 0, 2\n",
    "            )  # torch.Size([200, 1, 16])\n",
    "            question_summary = question_summary.permute(\n",
    "                1, 2, 0\n",
    "            )  # torch.Size([1, 1, 16])\n",
    "            # [1, 200, 16], [1, 16, 1] -> [1, 200, 1]\n",
    "            attention_scores = torch.bmm(document_output, question_summary).squeeze(0)\n",
    "            # print(\"attention scores: \", attention_scores)\n",
    "            # print(\"attention scores shape: \", attention_scores.shape)\n",
    "            return attention_scores\n",
    "\n",
    "        elif self.attention_method == AttentionMethod.COSINE_SIMILARITY:\n",
    "            cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
    "            # torch.Size([200, 16])\n",
    "            document_output = document_output.squeeze(1)\n",
    "            # torch.Size([16])\n",
    "            question_summary = question_summary.squeeze(0).squeeze(0)\n",
    "\n",
    "            # [200]\n",
    "            attention_scores = cosine_similarity(\n",
    "                document_output, question_summary\n",
    "            ).unsqueeze(-1)\n",
    "\n",
    "            return attention_scores\n",
    "\n",
    "        else:\n",
    "            document_output = document_output.permute(\n",
    "                1, 0, 2\n",
    "            )  # torch.Size([200, 1, 16])\n",
    "            question_summary = question_summary.permute(\n",
    "                1, 2, 0\n",
    "            )  # torch.Size([1, 1, 16])\n",
    "            # [1, 200, 16], [1, 16, 1] -> [1, 200, 1]\n",
    "            attention_scores = torch.bmm(document_output, question_summary) / np.sqrt(\n",
    "                self.hidden_size\n",
    "            )\n",
    "            attention_scores = attention_scores.squeeze(0)\n",
    "            # print(\"attention scores: \", attention_scores)\n",
    "            # print(\"attention scores shape: \", attention_scores.shape)\n",
    "            return attention_scores\n",
    "\n",
    "\n",
    "# Architecture of the model for the Attention Weighted Document Representation a.k.a ReadingComprehension\n",
    "\n",
    "\n",
    "class ReadingComprehensionModel(nn.Module):\n",
    "    def __init__(self, document_rnn, question_rnn, attention, hidden_size, output_size):\n",
    "        super(ReadingComprehensionModel, self).__init__()\n",
    "        self.document_rnn = document_rnn\n",
    "        self.question_rnn = question_rnn\n",
    "        self.attention = attention\n",
    "        self.linear = nn.Linear(1, output_size)\n",
    "\n",
    "    def predict_label(self, attention_output):\n",
    "        pred = self.linear(attention_output)\n",
    "        # print(\"prediction shape: \", pred.shape)\n",
    "        pred_weights = nn.functional.softmax(pred, dim=1)\n",
    "        # print(\"prediction weights shape: \", pred_weights.shape)\n",
    "        # shape of the context vector: (batch_size, 1, hidden_size)\n",
    "        return pred_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "\n",
    "\n",
    "def trainIter(\n",
    "    model,\n",
    "    document_inputs,\n",
    "    question_inputs,\n",
    "    target_labels,\n",
    "    num_epochs,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = 0\n",
    "        for document_input, question_input, target_label in zip(\n",
    "            document_inputs, question_inputs, target_labels\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            document_output = model.document_rnn(document_input)\n",
    "            question_summary = model.question_rnn(question_input)\n",
    "\n",
    "            attention_output = model.attention(document_output, question_summary)\n",
    "\n",
    "            token_label_logits = model.predict_label(attention_output).to(device)\n",
    "\n",
    "            # print(\"token label logits shape: \", token_label_logits.shape)\n",
    "            # print(\"target label shape: \", target_label.shape)\n",
    "            # print(\"token label logits: \", token_label_logits)\n",
    "\n",
    "            # print(token_label_logits[0])\n",
    "            # print(target_label[0])\n",
    "            # raise TypeError(\"stop\")\n",
    "\n",
    "            loss += criterion(token_label_logits, target_label)\n",
    "            # print(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = loss.item() / len(document_inputs)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalutation of the model\n",
    "\n",
    "START_LABEL = 1\n",
    "END_LABEL = 3\n",
    "\n",
    "\n",
    "def evaluate(model, document_inputs, question_inputs, target_labels, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        for document_input, question_input, target_label in zip(\n",
    "            document_inputs, question_inputs, target_labels\n",
    "        ):\n",
    "            document_output = model.document_rnn(document_input)\n",
    "            question_summary = model.question_rnn(question_input)\n",
    "            attention_output = model.attention(document_output, question_summary)\n",
    "            token_label_logits = model.predict_label(attention_output).to(device)\n",
    "            loss += criterion(token_label_logits, target_label)\n",
    "\n",
    "            # print(token_label_logits)\n",
    "\n",
    "            predictions = token_label_logits.argmax(dim=-1).cpu().numpy()\n",
    "            targets = target_label.argmax(dim=-1).cpu().numpy()\n",
    "            # print(predictions == 1)\n",
    "\n",
    "            if any(targets == START_LABEL) and any(targets == END_LABEL):\n",
    "                # Find indices of start and end tokens\n",
    "                start_token_idx = np.where(targets == START_LABEL)[0]\n",
    "                end_token_idx = np.where(targets == END_LABEL)[0]\n",
    "\n",
    "                # print(\"target: \", targets[start_token_idx[0] : end_token_idx[0] + 1])\n",
    "                # print(\n",
    "                #     \"prediction: \",\n",
    "                #     predictions[start_token_idx[0] : end_token_idx[0] + 1],\n",
    "                # )\n",
    "                # print()\n",
    "\n",
    "                # Take slice of predictions and target_labels for sentence tokens\n",
    "                sentence_prediction = predictions[\n",
    "                    start_token_idx[0] : end_token_idx[0] + 1\n",
    "                ]\n",
    "                sentence_target = targets[start_token_idx[0] : end_token_idx[0] + 1]\n",
    "\n",
    "                all_predictions.extend(sentence_prediction)\n",
    "                all_targets.extend(sentence_target)\n",
    "\n",
    "            else:\n",
    "                # Use the whole document since there is no answer\n",
    "                all_predictions.extend(predictions)\n",
    "                all_targets.extend(targets)\n",
    "\n",
    "        # print(all_predictions)\n",
    "        # print(all_targets)\n",
    "\n",
    "        # avg_loss = loss.item() / len(document_inputs)\n",
    "        # accuracy = accuracy_score(all_targets, all_predictions)\n",
    "        # precision = precision_score(all_targets, all_predictions, average=\"macro\")\n",
    "        # recall = recall_score(all_targets, all_predictions, average=\"macro\")\n",
    "        # f1 = f1_score(all_targets, all_predictions, average=\"macro\")\n",
    "        cr = classification_report(all_targets, all_predictions)\n",
    "\n",
    "        # print(\n",
    "        #     f\"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\"\n",
    "        # )\n",
    "\n",
    "        return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Reshape the target labels tensor\n",
    "reshaped_target_labels = (\n",
    "    train_labels.view(-1, 4).cpu().numpy()\n",
    ")  # Assuming it's on the GPU\n",
    "\n",
    "# Flatten the reshaped target labels\n",
    "flattened_target_labels = reshaped_target_labels.argmax(axis=1)\n",
    "\n",
    "\n",
    "# Calculate the class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=[0, 1, 2, 3], y=flattened_target_labels\n",
    ")\n",
    "\n",
    "# Convert the class weights to a PyTorch tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of the training\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "def train(\n",
    "    hidden_size=128,\n",
    "    epochs=10,\n",
    "    learning_rate=0.05,\n",
    "    num_layers=1,\n",
    "    token_labels=4,\n",
    "    attention_method: Literal[\n",
    "        \"dot_product\",\n",
    "        \"scale_dot_product\",\n",
    "        \"cosine_similarity\",\n",
    "    ] = \"dot_product\",\n",
    "):\n",
    "    # note the names of the tensors are changed to:\n",
    "    # as_doc_train, as_doc_test, as_qn_train, as_qn_test, train_labels, test_labels are called before in the ablation part\n",
    "    # to avoid confusion with the original tensors\n",
    "\n",
    "    # as_doc_train, as_doc_test, as_qn_train, as_qn_test\n",
    "\n",
    "    document_num_embeddings = as_doc_train.shape[2]\n",
    "    question_num_embeddings = as_qn_train.shape[2]\n",
    "\n",
    "    document_rnn = DocumentBiRNN(\n",
    "        hidden_size=hidden_size,\n",
    "        input_size=document_num_embeddings,\n",
    "        num_layers=num_layers,\n",
    "    ).to(device)\n",
    "    question_rnn = QuestionBiRNN(\n",
    "        input_size=question_num_embeddings,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "    ).to(device)\n",
    "    attention = Attention(hidden_size, attention_method).to(device)\n",
    "    reading_comp = ReadingComprehensionModel(\n",
    "        document_rnn,\n",
    "        question_rnn,\n",
    "        attention,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=token_labels,\n",
    "    ).to(device)\n",
    "    reading_comp_optimizer = optim.AdamW(reading_comp.parameters(), lr=learning_rate)\n",
    "\n",
    "    # class_weights = torch.tensor([1.0, 20.0, 10.0, 20.0], dtype=torch.float32).to(\n",
    "    #     device\n",
    "    # )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # # Create TensorDatasets from your data tensors\n",
    "    # dataset = TensorDataset(tf_final_doc_train, tf_final_qn_train)\n",
    "\n",
    "    # # Define batch size and create DataLoader\n",
    "    # batch_size = 32\n",
    "    # dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # note the names of the tensors are changed to:\n",
    "    # as_doc_train, as_doc_test, as_qn_train, as_qn_test, train_labels, test_labels\n",
    "\n",
    "    trainIter(\n",
    "        reading_comp,\n",
    "        as_doc_train,\n",
    "        as_qn_train,\n",
    "        train_labels,\n",
    "        epochs,\n",
    "        criterion,\n",
    "        reading_comp_optimizer,\n",
    "    )\n",
    "\n",
    "    return reading_comp, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.0819\n",
      "Epoch 2/10, Loss: 1.6655\n",
      "Epoch 3/10, Loss: 1.6621\n",
      "Epoch 4/10, Loss: 1.6621\n",
      "Epoch 5/10, Loss: 1.6621\n",
      "Epoch 6/10, Loss: 1.6621\n",
      "Epoch 7/10, Loss: 1.6621\n",
      "Epoch 8/10, Loss: 1.6621\n",
      "Epoch 9/10, Loss: 1.6621\n",
      "Epoch 10/10, Loss: 1.6621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96    260691\n",
      "           1       0.00      0.00      0.00       826\n",
      "           2       0.00      0.00      0.00     19947\n",
      "           3       0.00      0.00      0.00       812\n",
      "\n",
      "    accuracy                           0.92    282276\n",
      "   macro avg       0.23      0.25      0.24    282276\n",
      "weighted avg       0.85      0.92      0.89    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97     80177\n",
      "           1       0.00      0.00      0.00       230\n",
      "           2       0.00      0.00      0.00      5295\n",
      "           3       0.00      0.00      0.00       229\n",
      "\n",
      "    accuracy                           0.93     85931\n",
      "   macro avg       0.23      0.25      0.24     85931\n",
      "weighted avg       0.87      0.93      0.90     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "reading_comp_dot, criterion_dot = train(attention_method=\"dot_product\")\n",
    "\n",
    "# Model evaluation dot product # ignore if rerun\n",
    "train_report_dot, test_report_dot = evaluate(\n",
    "    reading_comp_dot, as_doc_train, as_qn_train, train_labels, criterion_dot\n",
    "), evaluate(reading_comp_dot, as_doc_test, as_qn_test, test_labels, criterion_dot)\n",
    "\n",
    "# Model evaluation for train and test set\n",
    "print(\"Evaluation on train set\")\n",
    "print(train_report_dot)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Evaluation on test set\")\n",
    "print(test_report_dot)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3886\n",
      "Epoch 2/10, Loss: 1.3828\n",
      "Epoch 3/10, Loss: 1.3835\n",
      "Epoch 4/10, Loss: 1.3813\n",
      "Epoch 5/10, Loss: 1.3758\n",
      "Epoch 6/10, Loss: 1.3686\n",
      "Epoch 7/10, Loss: 1.3637\n",
      "Epoch 8/10, Loss: 1.3588\n",
      "Epoch 9/10, Loss: 1.3528\n",
      "Epoch 10/10, Loss: 1.3473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00    260691\n",
      "           1       0.00      0.00      0.00       826\n",
      "           2       0.12      0.80      0.21     19947\n",
      "           3       0.00      0.88      0.01       812\n",
      "\n",
      "    accuracy                           0.06    282276\n",
      "   macro avg       0.03      0.42      0.05    282276\n",
      "weighted avg       0.01      0.06      0.01    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     80177\n",
      "           1       0.00      0.00      0.00       230\n",
      "           2       0.10      0.79      0.17      5295\n",
      "           3       0.00      0.81      0.01       229\n",
      "\n",
      "    accuracy                           0.05     85931\n",
      "   macro avg       0.03      0.40      0.05     85931\n",
      "weighted avg       0.01      0.05      0.01     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "reading_comp_dot, criterion_dot = train(attention_method=\"cosine_similarity\")\n",
    "\n",
    "# Model evaluation dot product # ignore if rerun\n",
    "train_report_dot, test_report_dot = evaluate(\n",
    "    reading_comp_dot, as_doc_train, as_qn_train, train_labels, criterion_dot\n",
    "), evaluate(reading_comp_dot, as_doc_test, as_qn_test, test_labels, criterion_dot)\n",
    "\n",
    "# Model evaluation for train and test set\n",
    "print(\"Evaluation on train set\")\n",
    "print(train_report_dot)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Evaluation on test set\")\n",
    "print(test_report_dot)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3908\n",
      "Epoch 2/10, Loss: 1.4861\n",
      "Epoch 3/10, Loss: 1.4936\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m reading_comp_dot, criterion_dot \u001b[39m=\u001b[39m train(attention_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdot_product\u001b[39;49m\u001b[39m\"\u001b[39;49m, num_layers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39m# Model evaluation dot product # ignore if rerun\u001b[39;00m\n\u001b[0;32m      4\u001b[0m train_report_dot, test_report_dot \u001b[39m=\u001b[39m evaluate(\n\u001b[0;32m      5\u001b[0m     reading_comp_dot, as_doc_train, as_qn_train, train_labels, criterion_dot\n\u001b[0;32m      6\u001b[0m ), evaluate(reading_comp_dot, as_doc_test, as_qn_test, test_labels, criterion_dot)\n",
      "Cell \u001b[1;32mIn[114], line 58\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(hidden_size, epochs, learning_rate, num_layers, token_labels, attention_method)\u001b[0m\n\u001b[0;32m     46\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(weight\u001b[39m=\u001b[39mclass_weights)\n\u001b[0;32m     48\u001b[0m \u001b[39m# # Create TensorDatasets from your data tensors\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m# dataset = TensorDataset(tf_final_doc_train, tf_final_qn_train)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# note the names of the tensors are changed to:\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# as_doc_train, as_doc_test, as_qn_train, as_qn_test, train_labels, test_labels\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m trainIter(\n\u001b[0;32m     59\u001b[0m     reading_comp,\n\u001b[0;32m     60\u001b[0m     as_doc_train,\n\u001b[0;32m     61\u001b[0m     as_qn_train,\n\u001b[0;32m     62\u001b[0m     train_labels,\n\u001b[0;32m     63\u001b[0m     epochs,\n\u001b[0;32m     64\u001b[0m     criterion,\n\u001b[0;32m     65\u001b[0m     reading_comp_optimizer,\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m reading_comp, criterion\n",
      "Cell \u001b[1;32mIn[93], line 39\u001b[0m, in \u001b[0;36mtrainIter\u001b[1;34m(model, document_inputs, question_inputs, target_labels, num_epochs, criterion, optimizer)\u001b[0m\n\u001b[0;32m     36\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m criterion(token_label_logits, target_label)\n\u001b[0;32m     37\u001b[0m     \u001b[39m# print(loss)\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     40\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     42\u001b[0m avg_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(document_inputs)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reading_comp_dot, criterion_dot = train(attention_method=\"dot_product\", num_layers=2)\n",
    "\n",
    "# Model evaluation dot product # ignore if rerun\n",
    "train_report_dot, test_report_dot = evaluate(\n",
    "    reading_comp_dot, as_doc_train, as_qn_train, train_labels, criterion_dot\n",
    "), evaluate(reading_comp_dot, as_doc_test, as_qn_test, test_labels, criterion_dot)\n",
    "\n",
    "# Model evaluation for train and test set\n",
    "print(\"Evaluation on train set\")\n",
    "print(train_report_dot)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Evaluation on test set\")\n",
    "print(test_report_dot)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3931\n",
      "Epoch 2/10, Loss: 1.3705\n",
      "Epoch 3/10, Loss: 1.3809\n",
      "Epoch 4/10, Loss: 1.3576\n",
      "Epoch 5/10, Loss: 1.3389\n",
      "Epoch 6/10, Loss: 1.3212\n",
      "Epoch 7/10, Loss: 1.3019\n",
      "Epoch 8/10, Loss: 1.2871\n",
      "Epoch 9/10, Loss: 1.2681\n",
      "Epoch 10/10, Loss: 1.2517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.69      0.80    260691\n",
      "           1       0.01      0.92      0.02       826\n",
      "           2       0.11      0.14      0.12     19947\n",
      "           3       0.00      0.00      0.00       812\n",
      "\n",
      "    accuracy                           0.65    282276\n",
      "   macro avg       0.27      0.44      0.24    282276\n",
      "weighted avg       0.88      0.65      0.74    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.67      0.79     80177\n",
      "           1       0.01      0.88      0.02       230\n",
      "           2       0.08      0.14      0.10      5295\n",
      "           3       0.00      0.00      0.00       229\n",
      "\n",
      "    accuracy                           0.64     85931\n",
      "   macro avg       0.26      0.42      0.23     85931\n",
      "weighted avg       0.89      0.64      0.74     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nicho\\.pyenv\\pyenv-win\\versions\\miniconda3-py310_23.1.0-1\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "reading_comp_scale, criterion_scale = train(attention_method=\"scale_dot_product\")\n",
    "\n",
    "# Model evaluation dot product # ignore if rerun\n",
    "train_report_scale, test_report_scale = evaluate(\n",
    "    reading_comp_scale, as_doc_train, as_qn_train, train_labels, criterion_scale\n",
    "), evaluate(reading_comp_scale, as_doc_test, as_qn_test, test_labels, criterion_scale)\n",
    "\n",
    "# Model evaluation for train and test set\n",
    "print(\"Evaluation on train set\")\n",
    "print(train_report_scale)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Evaluation on test set\")\n",
    "print(test_report_scale)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
