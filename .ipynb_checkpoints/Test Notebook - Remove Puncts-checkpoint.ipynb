{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528971ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3cb696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data\n",
    "train_data = pd.read_csv('WikiQA-train.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('WikiQA-test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad5982",
   "metadata": {},
   "source": [
    "Extract the unique questions from the train and test data frames, including the documentID and the DocumentTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034c4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_documenttag(data):\n",
    "    qd = data[['Question', 'QuestionID', 'DocumentID','DocumentTitle']].drop_duplicates()\n",
    "    return qd\n",
    "train_question_doctag = get_questions_documenttag(train_data)\n",
    "test_question_doctag = get_questions_documenttag(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368895a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique questions\n",
    "train_questions = train_question_doctag['Question']\n",
    "test_questions = test_question_doctag['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f8f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique document ids\n",
    "train_docid = train_question_doctag['DocumentID']\n",
    "test_docid = test_question_doctag['DocumentID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf4306",
   "metadata": {},
   "source": [
    "Extract the answers to those questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e2d6a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(data, questions, documentids): \n",
    "    answers = [] # list of answers\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q] # add the document id\n",
    "        df = data[data['Question'] == question]\n",
    "        index = df.loc[df['Label'] == 1]['Sentence'].index.values\n",
    "        if len(index) == 0: # if no answer found\n",
    "            answers.append([question, doc_id, 'No answer'])\n",
    "        else: # if 1 answer found\n",
    "            answers.append([question, doc_id, df.loc[index[0], \"Sentence\"]])\n",
    "    return answers\n",
    "\n",
    "train_answers = pd.DataFrame(get_answers(train_data, train_questions, train_docid))\n",
    "test_answers = pd.DataFrame(get_answers(test_data, test_questions, test_docid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e5527b",
   "metadata": {},
   "source": [
    "The above get_answers returns train_answers and test_answers which, gives us in the following columns\n",
    "- Question\n",
    "- Related Document ID\n",
    "- Answer (if no answer to that question, return no answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ac4567fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(data, questions, documentids): # (done by Finn, tweaked by Dan)\n",
    "    documents = []\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q] # add the document id\n",
    "        df = data[data['Question'] == question]\n",
    "        sentences = df['Sentence'].tolist()\n",
    "        for i in range(0, len(sentences) - 1):\n",
    "            sentences[i] = sentences[i] + ' '\n",
    "        documents.append([doc_id,''.join(sentences)])\n",
    "    return documents\n",
    "\n",
    "train_documents = pd.DataFrame(get_documents(train_data, train_questions, train_docid)) # return the individual document in list\n",
    "test_documents = pd.DataFrame(get_documents(test_data, test_questions, test_docid)) # return the individual document in list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa7704",
   "metadata": {},
   "source": [
    "The above train_documents and test_documents called from the get_documents gives us in the following columns\n",
    "- Document ID\n",
    "- Full Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0432bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming all the columns for more standardised access\n",
    "train_answers.columns = ['Question','DocumentID','Answer']\n",
    "test_answers.columns = ['Question','DocumentID','Answer']\n",
    "train_documents.columns = ['DocumentID','Document']\n",
    "test_documents.columns = ['DocumentID','Document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "763141d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2117, 2117, 630, 630)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result is 2117, 2117, 630, 630\n",
    "\n",
    "len(train_answers),len(train_documents), len(test_answers),len(test_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5edcdf6",
   "metadata": {},
   "source": [
    "**Prior to tagging, we should maybe clean the document and answers first:** (stopped here)\n",
    "\n",
    "Maybe? \n",
    "- lowercase (might lose context, but we can use on questions)\n",
    "- removing any punctuation or weird symbols (do)\n",
    "- removal of stop words? (probably not)\n",
    "\n",
    "Make sure that the pre-processing is standardised to be the same throughout doc and ans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c5fddce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lower(text):\n",
    "    # Lowercase the text for question, answer and documents\n",
    "    text = text.lower()\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "train_answers[['Question', 'Answer']] = train_answers[['Question', 'Answer']].applymap(preprocess_lower)\n",
    "train_documents['Document'] = train_documents['Document'].apply(preprocess_lower)\n",
    "test_answers[['Question', 'Answer']] = test_answers[['Question', 'Answer']].applymap(preprocess_lower)\n",
    "test_documents['Document'] = test_documents['Document'].apply(preprocess_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3328fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelling(documents, answers):\n",
    "    tagged_documents = []\n",
    "    for q in range(len(answers)):\n",
    "        tagged_document = []\n",
    "        qn = answers['Question'].loc[q]\n",
    "        doc_id = answers['DocumentID'].loc[q]\n",
    "        content = documents.loc[documents['DocumentID'] == doc_id,'Document'].values[0]\n",
    "        answer = answers['Answer'].loc[q]\n",
    "\n",
    "        if answer == 'no answer':\n",
    "            tokens = word_tokenize(content)\n",
    "            for j in range(len(tokens)):\n",
    "                tagged_document.append('N') # none \n",
    "        else:\n",
    "            parts = content.partition(answer)\n",
    "            for j in range(len(parts)):\n",
    "                tokens = word_tokenize(parts[j])\n",
    "                if j == 1:\n",
    "                    tagged_document.append('S') # start of answer\n",
    "                    for k in range(len(tokens) - 2):\n",
    "                        tagged_document.append('I') # inside of answer\n",
    "                    tagged_document.append('E') # end of answer\n",
    "                else:\n",
    "                    for k in range(len(tokens)):\n",
    "                        tagged_document.append('N') # outside answer\n",
    "        tagged_documents.append(tagged_document)\n",
    "    return(tagged_documents)\n",
    "\n",
    "train_doc_ans_labels = labelling(train_documents, train_answers)\n",
    "test_doc_ans_labels = labelling(test_documents, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3fc6f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'the']\n",
      "['N', 'pineapple']\n",
      "['N', 'ananas']\n",
      "['N', 'comosus']\n",
      "['N', 'is']\n",
      "['N', 'a']\n",
      "['N', 'tropical']\n",
      "['N', 'plant']\n",
      "['N', 'with']\n",
      "['N', 'edible']\n",
      "['N', 'multiple']\n",
      "['N', 'fruit']\n",
      "['N', 'consisting']\n",
      "['N', 'of']\n",
      "['N', 'coalesced']\n",
      "['N', 'berries']\n",
      "['N', 'and']\n",
      "['N', 'the']\n",
      "['N', 'most']\n",
      "['N', 'economically']\n",
      "['N', 'significant']\n",
      "['N', 'plant']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'bromeliaceae']\n",
      "['N', 'family']\n",
      "['S', 'pineapples']\n",
      "['I', 'may']\n",
      "['I', 'be']\n",
      "['I', 'cultivated']\n",
      "['I', 'from']\n",
      "['I', 'a']\n",
      "['I', 'crown']\n",
      "['I', 'cutting']\n",
      "['I', 'of']\n",
      "['I', 'the']\n",
      "['I', 'fruit']\n",
      "['I', 'possibly']\n",
      "['I', 'flowering']\n",
      "['I', 'in']\n",
      "['I', '20']\n",
      "['I', '24']\n",
      "['I', 'months']\n",
      "['I', 'and']\n",
      "['I', 'fruiting']\n",
      "['I', 'in']\n",
      "['I', 'the']\n",
      "['I', 'following']\n",
      "['I', 'six']\n",
      "['E', 'months']\n",
      "['N', 'pineapple']\n",
      "['N', 'does']\n",
      "['N', 'not']\n",
      "['N', 'ripen']\n",
      "['N', 'significantly']\n",
      "['N', 'post']\n",
      "['N', 'harvest']\n",
      "['N', 'pineapples']\n",
      "['N', 'are']\n",
      "['N', 'consumed']\n",
      "['N', 'fresh']\n",
      "['N', 'cooked']\n",
      "['N', 'juiced']\n",
      "['N', 'and']\n",
      "['N', 'preserved']\n",
      "['N', 'and']\n",
      "['N', 'are']\n",
      "['N', 'found']\n",
      "['N', 'in']\n",
      "['N', 'a']\n",
      "['N', 'wide']\n",
      "['N', 'array']\n",
      "['N', 'of']\n",
      "['N', 'cuisines']\n",
      "['N', 'in']\n",
      "['N', 'addition']\n",
      "['N', 'to']\n",
      "['N', 'consumption']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'philippines']\n",
      "['N', 'the']\n",
      "['N', 'pineapple']\n",
      "['N', 's']\n",
      "['N', 'leaves']\n",
      "['N', 'are']\n",
      "['N', 'used']\n",
      "['N', 'to']\n",
      "['N', 'produce']\n",
      "['N', 'the']\n",
      "['N', 'textile']\n",
      "['N', 'fiber']\n",
      "['N', 'pi']\n",
      "['N', 'a']\n",
      "['N', 'employed']\n",
      "['N', 'as']\n",
      "['N', 'a']\n",
      "['N', 'component']\n",
      "['N', 'of']\n",
      "['N', 'wall']\n",
      "['N', 'paper']\n",
      "['N', 'and']\n",
      "['N', 'furnishings']\n",
      "['N', 'amongst']\n",
      "['N', 'other']\n",
      "['N', 'uses']\n",
      "pineapples may be cultivated from a crown cutting of the fruit  possibly flowering in 20 24 months and fruiting in the following six months \n"
     ]
    }
   ],
   "source": [
    "# check if tags are good\n",
    "def testing_tokens(ind, labels, documents, answers):\n",
    "    for i,j in zip(labels[ind],word_tokenize(documents['Document'][ind])):\n",
    "        print([i,j])\n",
    "    print(answers['Answer'][ind])\n",
    "testing_tokens(144, train_doc_ans_labels, train_documents, train_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a294cdb",
   "metadata": {},
   "source": [
    "Cleaned Documents: train and test\n",
    "\n",
    "train_answers - contains the ['Question','DocumentID','Answer'] \n",
    "\n",
    "train_documents - contains the ['DocumentID','Document']\n",
    "\n",
    "train_doc_ans_labels - contains a list of list of answer tags for each document, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "83ed3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare the document for word embeddings:\n",
    "train_doc_ques = pd.DataFrame({'Document': train_documents['Document'],\n",
    "                               'Question': train_answers['Question']})\n",
    "test_doc_ques = pd.DataFrame({'Document': test_documents['Document'],\n",
    "                               'Question': test_answers['Question']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a01e5",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "To use the CBOW model, we need the data in sentences. Extract this from the original dataset, don't use sent_tokenise, will mess with some of the fullstops, we want to maintain structure from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "99ba3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokens(data):\n",
    "    sentence_list = []\n",
    "    for i in range(len(data)):\n",
    "        sentence_list.append(word_tokenize(data[i]))\n",
    "    return(sentence_list)\n",
    "train_doc_list = word_tokens(train_doc_ques['Document'])\n",
    "train_ques_list = word_tokens(train_doc_ques['Question'])\n",
    "test_doc_list = word_tokens(test_doc_ques['Document'])\n",
    "test_ques_list = word_tokens(test_doc_ques['Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1c491320",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = train_doc_list + train_ques_list + test_doc_list + test_ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "acb5de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_cbow_model = Word2Vec(sentences=combined_text, vector_size=100, window=5, min_count=1, workers=2, epochs=30)\n",
    "wc_cbow_model.save(\"cbow.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8cdd66",
   "metadata": {},
   "source": [
    "To implement QA\n",
    "\n",
    "1. Word Embeddings, using CBOW\n",
    "2. Feature Extraction 1 - POS tags\n",
    "3. Feature Extraction 2 - TF-IDF \n",
    "4. Feature Extraction 3 - NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ee6b4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(doc):\n",
    "    tokenized_doc = word_tokenize(doc)\n",
    "    embeddings = [wc_cbow_model.wv[word] for word in tokenized_doc]\n",
    "    return embeddings\n",
    "\n",
    "train_doc_ques['Doc_Embeddings'] = train_doc_ques['Document'].apply(get_word_embeddings)\n",
    "train_doc_ques['Q_Embeddings'] = train_doc_ques['Question'].apply(get_word_embeddings)\n",
    "test_doc_ques['Doc_Embeddings'] = test_doc_ques['Document'].apply(get_word_embeddings)\n",
    "test_doc_ques['Q_Embeddings'] = test_doc_ques['Question'].apply(get_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7da9a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_ques['Doc_Tokens'] = train_doc_ques['Document'].apply(word_tokenize)\n",
    "train_doc_ques['Q_Tokens'] =  train_doc_ques['Question'].apply(word_tokenize)\n",
    "test_doc_ques['Doc_Tokens'] = test_doc_ques['Document'].apply(word_tokenize)\n",
    "test_doc_ques['Q_Tokens'] = test_doc_ques['Question'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "cdf6d45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_count(doc):\n",
    "    count = 0\n",
    "    for i in range(len(doc)):\n",
    "        if len(doc['Doc_Embeddings'][i]) != len(doc['Doc_Tokens'][i]):\n",
    "            count += 1\n",
    "        elif len(doc['Q_Embeddings'][i]) != len(doc['Q_Tokens'][i]):\n",
    "            count += 1\n",
    "        else:\n",
    "            continue\n",
    "    return(count)\n",
    "        \n",
    "check_count(train_doc_ques) # looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46810415",
   "metadata": {},
   "source": [
    "Note, need to convert the POS tags, NER tags into embeddings. After this, pad the questions and answers to the max question/document length in the combined training and test set.\n",
    "\n",
    "### PoS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ea21daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Apply the pos tags to the tokens \n",
    "from nltk.tag import pos_tag\n",
    "# download the dependency and resource as required\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "train_doc_ques['Doc_POS'] = train_doc_ques['Doc_Tokens'].apply(pos_tag)\n",
    "train_doc_ques['Q_POS'] =  train_doc_ques['Q_Tokens'].apply(pos_tag)\n",
    "test_doc_ques['Doc_POS'] = test_doc_ques['Doc_Tokens'].apply(pos_tag)\n",
    "test_doc_ques['Q_POS'] = test_doc_ques['Q_Tokens'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e62df735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how', 'WRB'),\n",
       " ('african', 'JJ'),\n",
       " ('americans', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('immigrated', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('us', 'PRP')]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the POS tags: # looks ok\n",
    "test_doc_ques['Q_POS'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cfdef90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$': 0,\n",
       " 'CC': 1,\n",
       " 'CD': 2,\n",
       " 'DT': 3,\n",
       " 'EX': 4,\n",
       " 'FW': 5,\n",
       " 'IN': 6,\n",
       " 'JJ': 7,\n",
       " 'JJR': 8,\n",
       " 'JJS': 9,\n",
       " 'MD': 10,\n",
       " 'NN': 11,\n",
       " 'NNP': 12,\n",
       " 'NNPS': 13,\n",
       " 'NNS': 14,\n",
       " 'PDT': 15,\n",
       " 'POS': 16,\n",
       " 'PRP': 17,\n",
       " 'PRP$': 18,\n",
       " 'RB': 19,\n",
       " 'RBR': 20,\n",
       " 'RBS': 21,\n",
       " 'RP': 22,\n",
       " 'SYM': 23,\n",
       " 'TO': 24,\n",
       " 'UH': 25,\n",
       " 'VB': 26,\n",
       " 'VBD': 27,\n",
       " 'VBG': 28,\n",
       " 'VBN': 29,\n",
       " 'VBP': 30,\n",
       " 'VBZ': 31,\n",
       " 'WDT': 32,\n",
       " 'WP': 33,\n",
       " 'WP$': 34,\n",
       " 'WRB': 35}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all unique POS Tags\n",
    "all_pos_tags = train_doc_ques['Doc_POS'].tolist() + test_doc_ques['Doc_POS'].tolist() + train_doc_ques['Q_POS'].tolist() + test_doc_ques['Q_POS'].tolist()\n",
    "\n",
    "def get_unique_pos(data):\n",
    "    pos_tags = set()\n",
    "    for item in data:\n",
    "        for _,pos_tag in item:\n",
    "            pos_tags.add(pos_tag)\n",
    "\n",
    "    pos_tag_index = {tag: i for i, tag in enumerate(sorted(pos_tags))}\n",
    "    return pos_tag_index\n",
    "\n",
    "pos_iden = get_unique_pos(all_pos_tags) # list of tags\n",
    "pos_iden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c06ede",
   "metadata": {},
   "source": [
    "### NER Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b9a90",
   "metadata": {},
   "source": [
    "### Steps to run this:\n",
    "\n",
    "- pip install spacy \n",
    "- python -m spacy download en_core_web_sm\n",
    "\n",
    "If loaded for the first time, restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "89121aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk using Spacy\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "# loading pre-trained model of NER\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ab395e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tagging(texts):\n",
    "    tagged_texts = []\n",
    "    for text in texts:\n",
    "        doc = spacy.tokens.Doc(nlp.vocab, words=text)\n",
    "        nlp.get_pipe(\"ner\")(doc)\n",
    "        tagged_texts.append([(token.text, token.ent_type_) for token in doc])\n",
    "    return tagged_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "946fb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will take a while...\n",
    "train_doc_ques['Doc_NER'] = ner_tagging(train_doc_ques['Doc_Tokens'])\n",
    "train_doc_ques['Q_NER'] = ner_tagging(train_doc_ques['Q_Tokens'])\n",
    "test_doc_ques['Doc_NER'] = ner_tagging(test_doc_ques['Doc_Tokens'])\n",
    "test_doc_ques['Q_NER'] = ner_tagging(test_doc_ques['Q_Tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "25cba08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'CARDINAL': 1,\n",
       " 'DATE': 2,\n",
       " 'EVENT': 3,\n",
       " 'FAC': 4,\n",
       " 'GPE': 5,\n",
       " 'LANGUAGE': 6,\n",
       " 'LAW': 7,\n",
       " 'LOC': 8,\n",
       " 'MONEY': 9,\n",
       " 'NORP': 10,\n",
       " 'ORDINAL': 11,\n",
       " 'ORG': 12,\n",
       " 'PERCENT': 13,\n",
       " 'PERSON': 14,\n",
       " 'PRODUCT': 15,\n",
       " 'QUANTITY': 16,\n",
       " 'TIME': 17,\n",
       " 'WORK_OF_ART': 18}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar approach to the POS\n",
    "\n",
    "# Extract all unique POS Tags\n",
    "all_ner_tags = train_doc_ques['Doc_NER'].tolist() + test_doc_ques['Doc_NER'].tolist() + train_doc_ques['Q_NER'].tolist() + test_doc_ques['Q_NER'].tolist()\n",
    "\n",
    "def get_unique_ner(data):\n",
    "    ner_tags = set()\n",
    "    for item in data:\n",
    "        for _,ner_tag in item:\n",
    "            ner_tags.add(ner_tag)\n",
    "\n",
    "    ner_tag_index = {tag: i for i, tag in enumerate(sorted(ner_tags))}\n",
    "    return ner_tag_index\n",
    "\n",
    "ner_iden = get_unique_pos(all_ner_tags) # list of tags\n",
    "ner_iden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6d28ac77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_idx = ner_iden.values()\n",
    "aa = np.eye(max(ner_idx) + 1)\n",
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "60b20141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vectorize(pos_tagger, ner_tagger, data): # pass in the unique dict for ner or pos\n",
    "    pos_idx = pos_tagger.values()\n",
    "    pos_ohv = np.eye(max(pos_idx) + 1) # create the ohv\n",
    "    ner_idx = ner_tagger.values()\n",
    "    ner_ohv = np.eye(max(ner_idx) + 1)\n",
    "    \n",
    "    dpos_full_ohv, dner_full_ohv = [], [] # lists to append to \n",
    "    qpos_full_ohv, qner_full_ohv = [], [] # lists to append to\n",
    "\n",
    "    for item in data['Doc_POS']:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            pos_index_iden = pos_tagger[tag]\n",
    "            sent_ohv.append(pos_ohv[pos_index_iden])\n",
    "        dpos_full_ohv.append(sent_ohv)\n",
    "    \n",
    "    for item in data['Q_POS']:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            pos_index_iden = pos_tagger[tag]\n",
    "            sent_ohv.append(pos_ohv[pos_index_iden])\n",
    "        qpos_full_ohv.append(sent_ohv)\n",
    "    \n",
    "    for item in data['Doc_NER']:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            ner_index_iden = ner_tagger[tag]\n",
    "            sent_ohv.append(ner_ohv[ner_index_iden])\n",
    "        dner_full_ohv.append(sent_ohv)\n",
    "    \n",
    "    for item in data['Q_NER']:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            ner_index_iden = ner_tagger[tag]\n",
    "            sent_ohv.append(ner_ohv[ner_index_iden])\n",
    "        qner_full_ohv.append(sent_ohv)\n",
    "    \n",
    "    return(dpos_full_ohv, qpos_full_ohv, dner_full_ohv, qner_full_ohv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c766e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ohv for doc\n",
    "train_doc_pos_ohv, train_q_pos_ohv, train_doc_ner_ohv, train_q_ner_ohv = one_hot_vectorize(pos_iden, ner_iden, train_doc_ques)\n",
    "test_doc_pos_ohv, test_q_pos_ohv, test_doc_ner_ohv, test_q_ner_ohv = one_hot_vectorize(pos_iden, ner_iden, test_doc_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0ffd8910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sidney',\n",
       " 'patrick',\n",
       " 'crosby',\n",
       " 'ons',\n",
       " 'born',\n",
       " 'august',\n",
       " '7',\n",
       " '1987',\n",
       " 'is',\n",
       " 'a',\n",
       " 'canadian',\n",
       " 'professional',\n",
       " 'ice',\n",
       " 'hockey',\n",
       " 'player',\n",
       " 'who',\n",
       " 'is',\n",
       " 'captain',\n",
       " 'of',\n",
       " 'the',\n",
       " 'pittsburgh',\n",
       " 'penguins',\n",
       " 'of',\n",
       " 'the',\n",
       " 'national',\n",
       " 'hockey',\n",
       " 'league',\n",
       " 'nhl',\n",
       " 'crosby',\n",
       " 'was',\n",
       " 'drafted',\n",
       " 'first',\n",
       " 'overall',\n",
       " 'by',\n",
       " 'the',\n",
       " 'penguins',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'quebec',\n",
       " 'major',\n",
       " 'junior',\n",
       " 'hockey',\n",
       " 'league',\n",
       " 'qmjhl',\n",
       " 'during',\n",
       " 'his',\n",
       " 'two',\n",
       " 'year',\n",
       " 'major',\n",
       " 'junior',\n",
       " 'career',\n",
       " 'with',\n",
       " 'the',\n",
       " 'rimouski',\n",
       " 'oc',\n",
       " 'anic',\n",
       " 'he',\n",
       " 'earned',\n",
       " 'back',\n",
       " 'to',\n",
       " 'back',\n",
       " 'chl',\n",
       " 'player',\n",
       " 'of',\n",
       " 'the',\n",
       " 'year',\n",
       " 'awards',\n",
       " 'and',\n",
       " 'led',\n",
       " 'his',\n",
       " 'club',\n",
       " 'to',\n",
       " 'the',\n",
       " '2005',\n",
       " 'memorial',\n",
       " 'cup',\n",
       " 'final',\n",
       " 'nicknamed',\n",
       " 'the',\n",
       " 'next',\n",
       " 'one',\n",
       " 'he',\n",
       " 'was',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'highly',\n",
       " 'regarded',\n",
       " 'draft',\n",
       " 'picks',\n",
       " 'in',\n",
       " 'hockey',\n",
       " 'history',\n",
       " 'leading',\n",
       " 'many',\n",
       " 'to',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " '2005',\n",
       " 'draft',\n",
       " 'lottery',\n",
       " 'as',\n",
       " 'the',\n",
       " 'sidney',\n",
       " 'crosby',\n",
       " 'sweepstakes',\n",
       " 'in',\n",
       " 'his',\n",
       " 'first',\n",
       " 'nhl',\n",
       " 'season',\n",
       " 'crosby',\n",
       " 'finished',\n",
       " 'sixth',\n",
       " 'in',\n",
       " 'league',\n",
       " 'scoring',\n",
       " 'with',\n",
       " '102',\n",
       " 'points',\n",
       " '39',\n",
       " 'goals',\n",
       " '63',\n",
       " 'assists',\n",
       " 'and',\n",
       " 'was',\n",
       " 'a',\n",
       " 'runner',\n",
       " 'up',\n",
       " 'for',\n",
       " 'the',\n",
       " 'calder',\n",
       " 'memorial',\n",
       " 'trophy',\n",
       " 'won',\n",
       " 'by',\n",
       " 'alexander',\n",
       " 'ovechkin',\n",
       " 'by',\n",
       " 'his',\n",
       " 'second',\n",
       " 'season',\n",
       " 'he',\n",
       " 'led',\n",
       " 'the',\n",
       " 'nhl',\n",
       " 'with',\n",
       " '120',\n",
       " 'points',\n",
       " '36',\n",
       " 'goals',\n",
       " '84',\n",
       " 'assists',\n",
       " 'to',\n",
       " 'capture',\n",
       " 'the',\n",
       " 'art',\n",
       " 'ross',\n",
       " 'trophy',\n",
       " 'becoming',\n",
       " 'the',\n",
       " 'youngest',\n",
       " 'player',\n",
       " 'and',\n",
       " 'the',\n",
       " 'only',\n",
       " 'teenager',\n",
       " 'to',\n",
       " 'win',\n",
       " 'a',\n",
       " 'scoring',\n",
       " 'title',\n",
       " 'in',\n",
       " 'any',\n",
       " 'major',\n",
       " 'north',\n",
       " 'american',\n",
       " 'sports',\n",
       " 'league',\n",
       " 'that',\n",
       " 'same',\n",
       " 'season',\n",
       " 'crosby',\n",
       " 'also',\n",
       " 'won',\n",
       " 'the',\n",
       " 'hart',\n",
       " 'memorial',\n",
       " 'trophy',\n",
       " 'as',\n",
       " 'the',\n",
       " 'professional',\n",
       " 'hockey',\n",
       " 'writers',\n",
       " 'association',\n",
       " 's',\n",
       " 'choice',\n",
       " 'for',\n",
       " 'most',\n",
       " 'valuable',\n",
       " 'player',\n",
       " 'and',\n",
       " 'the',\n",
       " 'lester',\n",
       " 'b',\n",
       " 'pearson',\n",
       " 'award',\n",
       " 'as',\n",
       " 'the',\n",
       " 'nhl',\n",
       " 'players',\n",
       " 'association',\n",
       " 's',\n",
       " 'choice',\n",
       " 'for',\n",
       " 'most',\n",
       " 'outstanding',\n",
       " 'player',\n",
       " 'becoming',\n",
       " 'the',\n",
       " 'seventh',\n",
       " 'player',\n",
       " 'in',\n",
       " 'nhl',\n",
       " 'history',\n",
       " 'to',\n",
       " 'earn',\n",
       " 'all',\n",
       " 'three',\n",
       " 'awards',\n",
       " 'in',\n",
       " 'one',\n",
       " 'year',\n",
       " 'crosby',\n",
       " 'started',\n",
       " 'the',\n",
       " '2007',\n",
       " '08',\n",
       " 'season',\n",
       " 'with',\n",
       " 'the',\n",
       " 'team',\n",
       " 's',\n",
       " 'captaincy',\n",
       " 'and',\n",
       " 'subsequently',\n",
       " 'led',\n",
       " 'them',\n",
       " 'to',\n",
       " 'the',\n",
       " '2008',\n",
       " 'stanley',\n",
       " 'cup',\n",
       " 'finals',\n",
       " 'where',\n",
       " 'they',\n",
       " 'were',\n",
       " 'defeated',\n",
       " 'by',\n",
       " 'the',\n",
       " 'detroit',\n",
       " 'red',\n",
       " 'wings',\n",
       " 'in',\n",
       " 'six',\n",
       " 'games',\n",
       " 'the',\n",
       " 'penguins',\n",
       " 'returned',\n",
       " 'to',\n",
       " 'the',\n",
       " 'finals',\n",
       " 'against',\n",
       " 'detroit',\n",
       " 'the',\n",
       " 'following',\n",
       " 'year',\n",
       " 'and',\n",
       " 'won',\n",
       " 'in',\n",
       " 'seven',\n",
       " 'games',\n",
       " 'crosby',\n",
       " 'became',\n",
       " 'the',\n",
       " 'youngest',\n",
       " 'captain',\n",
       " 'in',\n",
       " 'nhl',\n",
       " 'history',\n",
       " 'to',\n",
       " 'win',\n",
       " 'the',\n",
       " 'stanley',\n",
       " 'cup',\n",
       " 'in',\n",
       " 'the',\n",
       " '2009',\n",
       " '10',\n",
       " 'season',\n",
       " 'crosby',\n",
       " 'scored',\n",
       " 'a',\n",
       " 'career',\n",
       " 'high',\n",
       " '51',\n",
       " 'goals',\n",
       " 'tying',\n",
       " 'him',\n",
       " 'with',\n",
       " 'steven',\n",
       " 'stamkos',\n",
       " 'for',\n",
       " 'the',\n",
       " 'rocket',\n",
       " 'richard',\n",
       " 'trophy',\n",
       " 'as',\n",
       " 'the',\n",
       " 'league',\n",
       " 'leader',\n",
       " 'with',\n",
       " '58',\n",
       " 'assists',\n",
       " 'he',\n",
       " 'totaled',\n",
       " '109',\n",
       " 'points',\n",
       " 'second',\n",
       " 'in',\n",
       " 'the',\n",
       " 'nhl',\n",
       " 'during',\n",
       " 'the',\n",
       " 'off',\n",
       " 'season',\n",
       " 'crosby',\n",
       " 'received',\n",
       " 'the',\n",
       " 'mark',\n",
       " 'messier',\n",
       " 'leadership',\n",
       " 'award',\n",
       " 'in',\n",
       " '2010',\n",
       " '11',\n",
       " 'crosby',\n",
       " 'sustained',\n",
       " 'a',\n",
       " 'concussion',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " 'of',\n",
       " 'hits',\n",
       " 'to',\n",
       " 'the',\n",
       " 'head',\n",
       " 'in',\n",
       " 'back',\n",
       " 'to',\n",
       " 'back',\n",
       " 'games',\n",
       " 'the',\n",
       " 'injury',\n",
       " 'left',\n",
       " 'him',\n",
       " 'sidelined',\n",
       " 'for',\n",
       " 'ten',\n",
       " 'and',\n",
       " 'a',\n",
       " 'half',\n",
       " 'months',\n",
       " 'however',\n",
       " 'after',\n",
       " 'playing',\n",
       " 'eight',\n",
       " 'games',\n",
       " 'in',\n",
       " 'the',\n",
       " '2011',\n",
       " '12',\n",
       " 'season',\n",
       " 'crosby',\n",
       " 's',\n",
       " 'concussion',\n",
       " 'like',\n",
       " 'symptoms',\n",
       " 'returned',\n",
       " 'in',\n",
       " 'december',\n",
       " '2011',\n",
       " 'and',\n",
       " 'he',\n",
       " 'did',\n",
       " 'not',\n",
       " 'return',\n",
       " 'until',\n",
       " 'mid',\n",
       " 'march',\n",
       " '2012',\n",
       " 'internationally',\n",
       " 'crosby',\n",
       " 'has',\n",
       " 'represented',\n",
       " 'canada',\n",
       " 'in',\n",
       " 'numerous',\n",
       " 'tournaments',\n",
       " 'for',\n",
       " 'the',\n",
       " 'country',\n",
       " 's',\n",
       " 'junior',\n",
       " 'and',\n",
       " 'men',\n",
       " 's',\n",
       " 'teams',\n",
       " 'after',\n",
       " 'competing',\n",
       " 'in',\n",
       " 'the',\n",
       " '2003',\n",
       " 'u',\n",
       " '18',\n",
       " 'junior',\n",
       " 'world',\n",
       " 'cup',\n",
       " 'he',\n",
       " 'represented',\n",
       " 'canada',\n",
       " 'in',\n",
       " 'back',\n",
       " 'to',\n",
       " 'back',\n",
       " 'iihf',\n",
       " 'world',\n",
       " 'u20',\n",
       " 'championships',\n",
       " 'winning',\n",
       " 'silver',\n",
       " 'in',\n",
       " '2004',\n",
       " 'and',\n",
       " 'gold',\n",
       " 'in',\n",
       " '2005',\n",
       " 'at',\n",
       " 'the',\n",
       " '2006',\n",
       " 'iihf',\n",
       " 'world',\n",
       " 'championship',\n",
       " 'he',\n",
       " 'led',\n",
       " 'the',\n",
       " 'tournament',\n",
       " 'in',\n",
       " 'scoring',\n",
       " 'while',\n",
       " 'also',\n",
       " 'earning',\n",
       " 'top',\n",
       " 'forward',\n",
       " 'and',\n",
       " 'all',\n",
       " 'star',\n",
       " 'team',\n",
       " 'honours',\n",
       " 'four',\n",
       " 'years',\n",
       " 'later',\n",
       " 'crosby',\n",
       " 'was',\n",
       " 'named',\n",
       " 'to',\n",
       " 'team',\n",
       " 'canada',\n",
       " 'for',\n",
       " 'the',\n",
       " '2010',\n",
       " 'winter',\n",
       " 'olympics',\n",
       " 'in',\n",
       " 'vancouver',\n",
       " 'playing',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " 'in',\n",
       " 'the',\n",
       " 'gold',\n",
       " 'medal',\n",
       " 'game',\n",
       " 'he',\n",
       " 'scored',\n",
       " 'the',\n",
       " 'game',\n",
       " 'winning',\n",
       " 'goal',\n",
       " 'in',\n",
       " 'overtime']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc_ques[:300]['Doc_Tokens'][290]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "68e412d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dataframe to just tokens and embeddings:\n",
    "doc_emb_train = train_doc_ques[['Doc_Tokens','Doc_Embeddings']]\n",
    "doc_pos_ner = pd.DataFrame({'Doc_POS':train_doc_pos_ohv,\n",
    "              'Doc_NER':train_doc_ner_ohv})\n",
    "doc_emb_train = pd.concat([doc_emb_train, doc_pos_ner], axis=1)\n",
    "\n",
    "q_emb_train = train_doc_ques[['Q_Tokens','Q_Embeddings']]\n",
    "q_pos_ner = pd.DataFrame({'Q_POS':train_q_pos_ohv,\n",
    "              'Q_NER':train_q_ner_ohv})\n",
    "q_emb_train = pd.concat([q_emb_train, q_pos_ner], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6d10ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb_test = test_doc_ques[['Doc_Tokens','Doc_Embeddings']]\n",
    "doc_pos_ner = pd.DataFrame({'Doc_POS':test_doc_pos_ohv,\n",
    "              'Doc_NER':test_doc_ner_ohv})\n",
    "doc_emb_test = pd.concat([doc_emb_test, doc_pos_ner], axis=1)\n",
    "\n",
    "q_emb_test = test_doc_ques[['Q_Tokens','Q_Embeddings']]\n",
    "q_pos_ner = pd.DataFrame({'Q_POS':test_q_pos_ohv,\n",
    "              'Q_NER':test_q_ner_ohv})\n",
    "q_emb_test = pd.concat([q_emb_test, q_pos_ner], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad89b3",
   "metadata": {},
   "source": [
    "### Word Embeddings (Doc and Qn)\n",
    "- Still have to add TF-IDF.\n",
    "\n",
    "The embeddings of the questions and answers of the train and test set can be found here:\n",
    "\n",
    "- Train Document - doc_emb_train\n",
    "- Train Q - q_emb_train\n",
    "- Test Document - doc_emb_test\n",
    "- Test Q - q_emb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d1a059d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q_Tokens</th>\n",
       "      <th>Q_Embeddings</th>\n",
       "      <th>Q_POS</th>\n",
       "      <th>Q_NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[how, african, americans, were, immigrated, to...</td>\n",
       "      <td>[[-1.2972423, 0.61705947, -2.289424, 1.9959564...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[how, large, were, early, jails]</td>\n",
       "      <td>[[-1.2972423, 0.61705947, -2.289424, 1.9959564...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[how, a, water, pump, works]</td>\n",
       "      <td>[[-1.2972423, 0.61705947, -2.289424, 1.9959564...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[how, old, was, sue, lyon, when, she, made, lo...</td>\n",
       "      <td>[[-1.2972423, 0.61705947, -2.289424, 1.9959564...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[how, are, antibodies, used, in]</td>\n",
       "      <td>[[-1.2972423, 0.61705947, -2.289424, 1.9959564...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>[where, is, the, brisket, from]</td>\n",
       "      <td>[[-0.59588027, -1.4281421, 0.5773438, 1.183183...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>[what, is, arm, chipset]</td>\n",
       "      <td>[[-1.1253945, 0.032915913, -1.9591076, -2.8268...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>[what, is, the, life, span, of, june, bugs]</td>\n",
       "      <td>[[-1.1253945, 0.032915913, -1.9591076, -2.8268...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>[who, is, the, youngest, female, to, give, bir...</td>\n",
       "      <td>[[-1.9893861, 2.442811, 0.9638027, 0.842218, -...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>[what, is, an, open, mare]</td>\n",
       "      <td>[[-1.1253945, 0.032915913, -1.9591076, -2.8268...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Q_Tokens  \\\n",
       "0    [how, african, americans, were, immigrated, to...   \n",
       "1                     [how, large, were, early, jails]   \n",
       "2                         [how, a, water, pump, works]   \n",
       "3    [how, old, was, sue, lyon, when, she, made, lo...   \n",
       "4                     [how, are, antibodies, used, in]   \n",
       "..                                                 ...   \n",
       "625                    [where, is, the, brisket, from]   \n",
       "626                           [what, is, arm, chipset]   \n",
       "627        [what, is, the, life, span, of, june, bugs]   \n",
       "628  [who, is, the, youngest, female, to, give, bir...   \n",
       "629                         [what, is, an, open, mare]   \n",
       "\n",
       "                                          Q_Embeddings  \\\n",
       "0    [[-1.2972423, 0.61705947, -2.289424, 1.9959564...   \n",
       "1    [[-1.2972423, 0.61705947, -2.289424, 1.9959564...   \n",
       "2    [[-1.2972423, 0.61705947, -2.289424, 1.9959564...   \n",
       "3    [[-1.2972423, 0.61705947, -2.289424, 1.9959564...   \n",
       "4    [[-1.2972423, 0.61705947, -2.289424, 1.9959564...   \n",
       "..                                                 ...   \n",
       "625  [[-0.59588027, -1.4281421, 0.5773438, 1.183183...   \n",
       "626  [[-1.1253945, 0.032915913, -1.9591076, -2.8268...   \n",
       "627  [[-1.1253945, 0.032915913, -1.9591076, -2.8268...   \n",
       "628  [[-1.9893861, 2.442811, 0.9638027, 0.842218, -...   \n",
       "629  [[-1.1253945, 0.032915913, -1.9591076, -2.8268...   \n",
       "\n",
       "                                                 Q_POS  \\\n",
       "0    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "4    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "..                                                 ...   \n",
       "625  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "626  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "627  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "628  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "629  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                                 Q_NER  \n",
       "0    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "..                                                 ...  \n",
       "625  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "626  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "627  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "628  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "629  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "\n",
       "[630 rows x 4 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_emb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "de8ada31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1675"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max_length of the document\n",
    "max_len_doc = 0\n",
    "for i in doc_emb_train['Doc_Tokens']:\n",
    "    if len(i) > max_len_doc:\n",
    "        max_len_doc = len(i)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "for i in doc_emb_test['Doc_Tokens']:\n",
    "    if len(i) > max_len_doc:\n",
    "        max_len_doc = len(i)\n",
    "    else:\n",
    "        continue\n",
    "max_len_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "56ced293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max_length of question\n",
    "max_len_qn = 0\n",
    "for i in q_emb_train['Q_Tokens']:\n",
    "    if len(i) > max_len_qn:\n",
    "        max_len_qn = len(i)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "for i in q_emb_test['Q_Tokens']:\n",
    "    if len(i) > max_len_qn:\n",
    "        max_len_qn = len(i)\n",
    "    else:\n",
    "        continue\n",
    "max_len_qn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a494fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = train_doc_ques[['Doc_Tokens', 'Q_Tokens']]\n",
    "test_corpus = test_doc_ques[['Doc_Tokens', 'Q_Tokens']]\n",
    "\n",
    "# Flatten the lists in each row and concatenate them\n",
    "def get_squeeze(corpus):\n",
    "    combined_list = []\n",
    "    for _, row in corpus.iterrows():\n",
    "        combined_list.extend(row['Doc_Tokens'])\n",
    "        combined_list.extend(row['Q_Tokens'])\n",
    "    return(combined_list)\n",
    "df_training = get_squeeze(training_corpus)\n",
    "df_test = get_squeeze(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "5540e712",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize a default dictionary with int\n",
    "def diction(df):\n",
    "    keys = dict.fromkeys(set(df),0)\n",
    "    for token in df:\n",
    "        keys[token] += 1\n",
    "    return keys\n",
    "d_train, d_test = diction(df_training), diction(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "23048853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16299"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF-oriented function:\n",
    "def tfidf_wikisearch(documents):\n",
    "  ## Write your function code body from here (** the following comments are only for instructional reference)\n",
    "  \n",
    "  N = len(documents)\n",
    "  print(\"Total docs in documents: {}\".format(N))\n",
    "  docs = []\n",
    "\n",
    "  # Add the three additional stop words, you can also do it out of this function\n",
    "  sww = sw.words()\n",
    "  sww.extend(['brisbane', 'melbourne', 'sydney'])\n",
    "\n",
    "  # process the document\n",
    "  for item in range(N):\n",
    "    cleaned_page = re.sub(r'[^\\w\\s]','', documents[item])\n",
    "    tokenized_page = word_tokenize(cleaned_page)\n",
    "    lower_tokens = [t.lower() for t in tokenized_page]\n",
    "    tokenized_doc = [w for w in lower_tokens if not w in sww]\n",
    "    docs.append(tokenized_doc)\n",
    "\n",
    "  # calculate the TF-IDF values for each unique words\n",
    "  DF = {}\n",
    "  for page in docs:\n",
    "    # get each unique word in the doc - and count the number of occurrences in the document\n",
    "    for term in np.unique(page):\n",
    "        try:\n",
    "            DF[term] +=1\n",
    "        except:\n",
    "            DF[term] =1\n",
    "\n",
    "  doc_id = 0\n",
    "  tf_idf = {} # dictionary of words and weights (tf-idf)\n",
    "\n",
    "  for tk_doc in docs:\n",
    "    # init counter\n",
    "    count = Counter(tk_doc)\n",
    "    # count the total number of words in the document\n",
    "    total_words = len(tk_doc)\n",
    "    # get each unique word in the document\n",
    "    for word in np.unique(tk_doc):\n",
    "      # Calculate TF, DF, IDF and TF-IDF\n",
    "      tf = count[word]/total_words # occurance in document\n",
    "      df = DF[word] # occurance in corpus\n",
    "      idf = math.log(N/(df+1))+1\n",
    "      tf_idf[doc_id, word] = tf*idf\n",
    "    doc_id += 1\n",
    "\n",
    "  # sorting the words based on the TF-IDF value and get the word with top-1 TF-IDF value\n",
    "  sorted_dict = sorted(tf_idf.items(), key=lambda x: x[1], reverse=True)\n",
    "  top = sorted_dict[0]\n",
    "\n",
    "  # search the wiki page for this word and print out the page content\n",
    "  top_word = top[0][1]\n",
    "  top_page = wikipedia.page(top_word).content\n",
    "  print(\"The word with the highest TF-IDF score is : '{}' - {}. \\n\".format(top_word,top[1]))\n",
    "  \n",
    "  return(top_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189c22d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
