{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the data wrangling bit can be quite computationally intensive, and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#load in the np data from the cleaneddata folder\n",
    "final_doc_test = np.load('cleaneddata/final_doc_test.npy')\n",
    "final_doc_train = np.load('cleaneddata/final_doc_train.npy')\n",
    "final_qn_train = np.load('cleaneddata/final_qn_train.npy')\n",
    "final_qn_test = np.load('cleaneddata/final_qn_test.npy')\n",
    "tr_labels = np.load('cleaneddata/tr_labels.npy')\n",
    "ts_labels = np.load('cleaneddata/ts_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(630, 200, 156) (2117, 200, 156) (2117, 23, 156) (630, 23, 156) (2117, 200) (630, 200)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of all the above\n",
    "print(final_doc_test.shape, final_doc_train.shape, final_qn_train.shape, final_qn_test.shape, tr_labels.shape, ts_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numpy arrays to tensors\n",
    "final_doc_test = torch.from_numpy(final_doc_test)\n",
    "final_doc_train = torch.from_numpy(final_doc_train)\n",
    "final_qn_train = torch.from_numpy(final_qn_train)\n",
    "final_qn_test = torch.from_numpy(final_qn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([630, 200, 156]) torch.Size([2117, 200, 156]) torch.Size([2117, 23, 156]) torch.Size([630, 23, 156])\n"
     ]
    }
   ],
   "source": [
    "# check the shapes of the tensors\n",
    "print(final_doc_test.shape, final_doc_train.shape, final_qn_train.shape, final_qn_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Embedding Ablation Study**\n",
    "\n",
    "In the model input embedding Ablation study, we are given 3 variations of input embeddings to test. We will test 3 options:\n",
    "\n",
    "1. Word2Vec only # 100 dims\n",
    "2. Word2Vec + Tf-IDF # 101 dims\n",
    "3. Word2Vec + all features (TF-IDF, POS, NER) # 156 dims\n",
    "\n",
    "Since we are using tensors, we can use tensor slicing to take out the relevant features.\n",
    "Our tensor of embeddings are built as follows (w2v, TF-IDF, POS, NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensors(tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test, option=3):\n",
    "    if option == 3:\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test\n",
    "    elif option == 1:\n",
    "        tf_doc_train = tf_doc_train[:, :, :100]\n",
    "        tf_doc_test = tf_doc_test[:, :, :100]\n",
    "        tf_qn_train = tf_qn_train[:, :, :100]\n",
    "        tf_qn_test = tf_qn_test[:, :, :100]\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test\n",
    "    elif option == 2:\n",
    "        tf_doc_train = tf_doc_train[:, :, :101]\n",
    "        tf_doc_test = tf_doc_test[:, :, :101]\n",
    "        tf_qn_train = tf_qn_train[:, :, :101]\n",
    "        tf_qn_test = tf_qn_test[:, :, :101]\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from label to index\n",
    "label2index = {\"N\": 0, \"S\": 1, \"I\": 2, \"E\": 3}\n",
    "\n",
    "# Find the maximum length of the label lists\n",
    "max_len = final_doc_train.shape[1]\n",
    "\n",
    "# Create a tensor to hold the one-hot encoded labels\n",
    "train_labels = torch.zeros(\n",
    "    len(tr_labels), max_len, len(label2index), device=device, dtype=torch.float32\n",
    ")\n",
    "test_labels = torch.zeros(\n",
    "    len(ts_labels),\n",
    "    max_len,\n",
    "    len(label2index),\n",
    "    device=device,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Iterate over the label lists and one-hot encode the labels\n",
    "for i, label_list in enumerate(tr_labels):\n",
    "    for j, label in enumerate(label_list):\n",
    "        index = label2index[label]\n",
    "        # Sets all elements of the target_labels tensor at position (i,j) to 0\n",
    "        train_labels[i, j] = 0\n",
    "        train_labels[i, j, index] = 1\n",
    "\n",
    "for i, label_list in enumerate(ts_labels):\n",
    "    for j, label in enumerate(label_list):\n",
    "        index = label2index[label]\n",
    "        # Sets all elements of the target_labels tensor at position (i,j) to 0\n",
    "        test_labels[i, j] = 0\n",
    "        test_labels[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Reshape the target labels tensor\n",
    "reshaped_target_labels = (\n",
    "    train_labels.view(-1, 4).cpu().numpy()\n",
    ")  # Assuming it's on the GPU\n",
    "\n",
    "# Flatten the reshaped target labels\n",
    "flattened_target_labels = reshaped_target_labels.argmax(axis=1)\n",
    "\n",
    "# Calculate the class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=[0, 1, 2, 3], y=flattened_target_labels\n",
    ")\n",
    "\n",
    "# Convert the class weights to a PyTorch tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing complete at this stage, we should check again the shapes of the tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import Tensor\n",
    "from enum import Enum\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the model for the Document BiLSTM\n",
    "\n",
    "class DocumentBiRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super(DocumentBiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: Tensor):\n",
    "        input = input.unsqueeze(1)\n",
    "        output: Tensor\n",
    "        output, _ = self.lstm(input)\n",
    "        # print(\"document output shape: \", output.shape)\n",
    "        return output\n",
    "    \n",
    "# Architecture of the model for the Question BiLSTM\n",
    "\n",
    "class QuestionBiRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super(QuestionBiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: Tensor):\n",
    "        input = input.unsqueeze(1)\n",
    "        output, (hn, cn) = self.lstm(input)\n",
    "        forward_hn = hn[-2, :, :]\n",
    "        backward_hn = hn[-1, :, :]\n",
    "        hidden = torch.cat((forward_hn, backward_hn), dim=-1).unsqueeze(0)\n",
    "        # print(\"question hidden shape: \", hidden.shape)\n",
    "        return hidden\n",
    "    \n",
    "# attention methods\n",
    "class AttentionMethod(Enum):\n",
    "    DOT_PRODUCT = \"dot_product\"\n",
    "    SCALE_DOT_PRODUCT = \"scale_dot_product\"\n",
    "    COSINE_SIMILARITY = \"cosine_similarity\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "   \n",
    "    \n",
    "# Architecture of the model for the Attention Calculation\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, ques_len, hidden_size:int, attention_method: Literal[\n",
    "            \"dot_product\",\n",
    "            \"scale_dot_product\",\n",
    "            \"cosine_similarity\",\n",
    "        ] = \"dot_product\",):\n",
    "        super(Attention, self).__init__()\n",
    "        self.out = nn.Linear(ques_len, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_method = AttentionMethod(attention_method)\n",
    "        \n",
    "    def forward(self, document_output, question_summary):\n",
    "        if self.attention_method == AttentionMethod.DOT_PRODUCT:\n",
    "            document_output = document_output.permute(1, 0, 2)  # torch.Size([200, 1, 16])\n",
    "            question_summary = question_summary.permute(1, 2, 0)  # torch.Size([1, 16, 1])\n",
    "\n",
    "            attention_scores = torch.bmm(document_output, question_summary).permute(1, 0, 2)\n",
    "            # get attention weights\n",
    "            attention_weights = nn.functional.softmax(attention_scores, dim=1)\n",
    "             #attention_scores = torch.bmm(document_output, question_summary) / np.sqrt(self.hidden_size)\n",
    "            # get context vector\n",
    "            context_scores = torch.bmm(document_output.permute(1, 2, 0), attention_weights).permute(0, 2, 1)\n",
    "            return context_scores\n",
    "        \n",
    "        elif self.attention_method == AttentionMethod.SCALE_DOT_PRODUCT:\n",
    "            document_output = document_output.permute(1, 0, 2)\n",
    "            question_summary = question_summary.permute(1, 2, 0)\n",
    "            attention_scores = torch.bmm(document_output, question_summary).permute(1,0,2) / np.sqrt(self.hidden_size)\n",
    "            attention_weights = nn.functional.softmax(attention_scores, dim=1)\n",
    "            context_scores = torch.bmm(document_output.permute(1, 2, 0), attention_weights).permute(0, 2, 1)\n",
    "            return context_scores\n",
    "        \n",
    "        elif self.attention_method == AttentionMethod.COSINE_SIMILARITY:\n",
    "            document_output = document_output.permute(1, 0, 2)\n",
    "            question_summary = question_summary.permute(1, 2, 0)\n",
    "            question_summary = question_summary.squeeze(-1)\n",
    "            # cosine similarity attention:\n",
    "            cos_sim = F.cosine_similarity(document_output, question_summary.unsqueeze(0), dim=-1).T.unsqueeze(1)\n",
    "            attention_weights = nn.functional.softmax(cos_sim, dim=1)\n",
    "            context_scores = torch.bmm(document_output.permute(1, 2, 0), attention_weights).permute(0, 2, 1)\n",
    "            return context_scores\n",
    "\n",
    "\n",
    "# Architecture of the model for the Attention Weighted Document Representation a.k.a ReadingComprehension\n",
    "class ReadingComprehensionModel(nn.Module):\n",
    "    def __init__(self, document_rnn, question_rnn, attention, hidden_size, output_size):\n",
    "        super(ReadingComprehensionModel, self).__init__()\n",
    "        self.document_rnn = document_rnn\n",
    "        self.question_rnn = question_rnn\n",
    "        self.attention = attention\n",
    "        self.linear = nn.Linear(hidden_size*2, hidden_size*2)\n",
    "        self.linear2 = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def predict_label(self, attention_output):\n",
    "        attention_output = torch.squeeze(attention_output,1)\n",
    "        # pass to linear\n",
    "        pred_weights = self.linear(attention_output)\n",
    "        pred_weights = self.linear2(pred_weights)\n",
    "        # get the softmax\n",
    "        #pred_weights = nn.functional.softmax(pred_weights, dim=1)\n",
    "        return pred_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "\n",
    "def trainIter(\n",
    "    model,\n",
    "    document_inputs,\n",
    "    question_inputs,\n",
    "    target_labels,\n",
    "    num_epochs,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = 0\n",
    "        for document_input, question_input, target_label in zip(\n",
    "            document_inputs, question_inputs, target_labels\n",
    "        ):\n",
    "            #optimizer.zero_grad()\n",
    "\n",
    "            document_output = model.document_rnn(document_input)\n",
    "            question_summary = model.question_rnn(question_input)\n",
    "\n",
    "            attention_output = model.attention(document_output, question_summary)\n",
    "\n",
    "            token_label_logits = model.predict_label(attention_output).to(device)\n",
    "\n",
    "            #print(\"token label logits shape: \", token_label_logits)\n",
    "            # print(\"target label shape: \", target_label.shape)\n",
    "            # print(\"token label logits: \", token_label_logits)\n",
    "\n",
    "            # print(token_label_logits[0])\n",
    "            # print(target_label[0])\n",
    "            # raise TypeError(\"stop\")\n",
    "\n",
    "            loss += criterion(token_label_logits, target_label)\n",
    "            optimizer.zero_grad()\n",
    "            # print(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = loss.item() / len(document_inputs)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalutation of the model\n",
    "\n",
    "START_LABEL = 1\n",
    "END_LABEL = 3\n",
    "\n",
    "\n",
    "def evaluate(model, document_inputs, question_inputs, target_labels, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        for document_input, question_input, target_label in zip(\n",
    "            document_inputs, question_inputs, target_labels\n",
    "        ):\n",
    "            document_output = model.document_rnn(document_input)\n",
    "            question_summary = model.question_rnn(question_input)\n",
    "            attention_output = model.attention(document_output, question_summary)\n",
    "            token_label_logits = model.predict_label(attention_output).to(device)\n",
    "            loss += criterion(token_label_logits, target_label)\n",
    "\n",
    "            # print(token_label_logits)\n",
    "\n",
    "            predictions = token_label_logits.argmax(dim=-1).cpu().numpy()\n",
    "            targets = target_label.argmax(dim=-1).cpu().numpy()\n",
    "            # print(predictions == 1)\n",
    "\n",
    "            if any(targets == START_LABEL) and any(targets == END_LABEL):\n",
    "                # Find indices of start and end tokens\n",
    "                start_token_idx = np.where(targets == START_LABEL)[0]\n",
    "                end_token_idx = np.where(targets == END_LABEL)[0]\n",
    "\n",
    "                #print(\"target: \", targets[start_token_idx[0] : end_token_idx[0] + 1])\n",
    "                #print(\n",
    "                #    \"prediction: \",\n",
    "                #    predictions[start_token_idx[0] : end_token_idx[0] + 1],\n",
    "                #)\n",
    "                #print()\n",
    "\n",
    "                # Take slice of predictions and target_labels for sentence tokens\n",
    "                sentence_prediction = predictions[\n",
    "                    start_token_idx[0] : end_token_idx[0] + 1\n",
    "                ]\n",
    "                sentence_target = targets[start_token_idx[0] : end_token_idx[0] + 1]\n",
    "\n",
    "                all_predictions.extend(sentence_prediction)\n",
    "                all_targets.extend(sentence_target)\n",
    "            else:\n",
    "                # Use the whole document since there is no answer\n",
    "                all_predictions.extend(predictions)\n",
    "                all_targets.extend(targets)\n",
    "\n",
    "        # print(all_predictions)\n",
    "        # print(all_targets)\n",
    "\n",
    "        avg_loss = loss.item() / len(document_inputs)\n",
    "        accuracy = accuracy_score(all_targets, all_predictions)\n",
    "        precision = precision_score(all_targets, all_predictions, average=\"macro\")\n",
    "        recall = recall_score(all_targets, all_predictions, average=\"macro\")\n",
    "        f1 = f1_score(all_targets, all_predictions, average=\"macro\")\n",
    "        cr = classification_report(all_targets, all_predictions)\n",
    "\n",
    "        print(\n",
    "            f\"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior to training\n",
    "as_doc_train, as_doc_test, as_qn_train, as_qn_test = convert_tensors(final_doc_train, final_doc_test, final_qn_train, final_qn_test, 3)\n",
    "# if not running any ablation, use the free up space by deleting np arrays:\n",
    "#del final_doc_test, final_doc_train, final_qn_train, final_qn_test, tr_labels, ts_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of the training\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "def train(\n",
    "    hidden_size = 64,\n",
    "    epochs = 10,\n",
    "    learning_rate = 0.01,\n",
    "    num_layers = 1,\n",
    "    token_labels = 4,\n",
    "    attention_method: Literal[\n",
    "            \"dot_product\",\n",
    "            \"scale_dot_product\",\n",
    "            \"cosine_similarity\",\n",
    "        ] = \"dot_product\",\n",
    "    ):\n",
    "\n",
    "    # note the names of the tensors are changed to:\n",
    "    # as_doc_train, as_doc_test, as_qn_train, as_qn_test, train_labels, test_labels are called before in the ablation part\n",
    "    # to avoid confusion with the original tensors\n",
    "\n",
    "    #as_doc_train, as_doc_test, as_qn_train, as_qn_test\n",
    "\n",
    "    document_num_embeddings = as_doc_train.shape[2]\n",
    "    question_num_embeddings = as_qn_train.shape[2]\n",
    "    ques_len = as_qn_train.shape[1]\n",
    "\n",
    "    document_rnn = DocumentBiRNN(\n",
    "        hidden_size=hidden_size, input_size=document_num_embeddings, num_layers=num_layers,\n",
    "    ).to(device)\n",
    "    question_rnn = QuestionBiRNN(\n",
    "        input_size=question_num_embeddings,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "    ).to(device)\n",
    "    attention = Attention(ques_len, hidden_size, attention_method).to(device)\n",
    "    reading_comp = ReadingComprehensionModel(\n",
    "        document_rnn,\n",
    "        question_rnn,\n",
    "        attention,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=token_labels,\n",
    "    ).to(device)\n",
    "    reading_comp_optimizer = optim.AdamW(reading_comp.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights) # to account for imbalanced class weights\n",
    "\n",
    "    trainIter(\n",
    "        reading_comp,\n",
    "        as_doc_train,\n",
    "        as_qn_train,\n",
    "        train_labels,\n",
    "        epochs,\n",
    "        criterion,\n",
    "        reading_comp_optimizer,\n",
    "    )\n",
    "\n",
    "    return reading_comp, criterion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Attention Ablation Study\n",
    "\n",
    "In this section, we study 3 different type of attention mechanisms between the question model and the document model. We ensured that the 3 attention mechanisms are ran on the same model hyperparameters, so as to keep things interpretable and standardized across the study. \n",
    "\n",
    "The hyperparameters of the training model are as follows:\n",
    "- RNN (Bi-LSTM) Hidden Size: 64,\n",
    "- Number of epochs: 10,\n",
    "- Learning Rate: 0.01,\n",
    "- Number of RNN (Bi-LSTM) layers: 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Ablation Study - Dot Product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3877\n",
      "Epoch 2/10, Loss: 1.2985\n",
      "Epoch 3/10, Loss: 1.1640\n",
      "Epoch 4/10, Loss: 1.0213\n",
      "Epoch 5/10, Loss: 0.8929\n",
      "Epoch 6/10, Loss: 0.8886\n",
      "Epoch 7/10, Loss: 0.8762\n",
      "Epoch 8/10, Loss: 0.7103\n",
      "Epoch 9/10, Loss: 0.7208\n",
      "Epoch 10/10, Loss: 0.6745\n",
      "Loss: 0.6243, Accuracy: 0.4794, Precision: 0.2890, Recall: 0.7330, F1: 0.2368\n",
      "Loss: 0.7285, Accuracy: 0.4514, Precision: 0.2805, Recall: 0.6719, F1: 0.2169\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "reading_comp_dot, criterion_dot = train(attention_method=\"dot_product\")\n",
    "train_report, test_report = evaluate(reading_comp_dot, as_doc_train, as_qn_train, train_labels, criterion_dot), evaluate(reading_comp_dot, as_doc_test, as_qn_test, test_labels, criterion_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.46      0.63    260691\n",
      "           1       0.03      0.90      0.05       826\n",
      "           2       0.12      0.67      0.21     19947\n",
      "           3       0.03      0.90      0.06       812\n",
      "\n",
      "    accuracy                           0.48    282276\n",
      "   macro avg       0.29      0.73      0.24    282276\n",
      "weighted avg       0.91      0.48      0.59    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.44      0.60     80177\n",
      "           1       0.02      0.85      0.05       230\n",
      "           2       0.10      0.65      0.17      5295\n",
      "           3       0.02      0.75      0.04       229\n",
      "\n",
      "    accuracy                           0.45     85931\n",
      "   macro avg       0.28      0.67      0.22     85931\n",
      "weighted avg       0.92      0.45      0.57     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation for train and test set\n",
    "print(\"Evaluation on train set\")\n",
    "print(train_report)\n",
    "print('----------------------------------------------------------')\n",
    "print(\"Evaluation on test set\")\n",
    "print(test_report)\n",
    "print('----------------------------------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Ablation Study - Scaled Dot Product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3930\n",
      "Epoch 2/10, Loss: 1.2962\n",
      "Epoch 3/10, Loss: 1.1624\n",
      "Epoch 4/10, Loss: 1.0126\n",
      "Epoch 5/10, Loss: 0.8761\n",
      "Epoch 6/10, Loss: 0.8303\n",
      "Epoch 7/10, Loss: 0.8849\n",
      "Epoch 8/10, Loss: 0.7509\n",
      "Epoch 9/10, Loss: 0.7617\n",
      "Epoch 10/10, Loss: 0.6999\n"
     ]
    }
   ],
   "source": [
    "# testing with scaled dot product attention\n",
    "reading_comp_scaled, criterion_scaled = train(attention_method=\"scale_dot_product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6344, Accuracy: 0.4300, Precision: 0.2885, Recall: 0.7360, F1: 0.2194\n",
      "Loss: 0.7066, Accuracy: 0.4040, Precision: 0.2813, Recall: 0.6986, F1: 0.2018\n",
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.41      0.58    260691\n",
      "           1       0.03      0.93      0.05       826\n",
      "           2       0.13      0.66      0.21     19947\n",
      "           3       0.02      0.95      0.04       812\n",
      "\n",
      "    accuracy                           0.43    282276\n",
      "   macro avg       0.29      0.74      0.22    282276\n",
      "weighted avg       0.92      0.43      0.55    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.39      0.55     80177\n",
      "           1       0.02      0.89      0.04       230\n",
      "           2       0.10      0.64      0.18      5295\n",
      "           3       0.02      0.88      0.03       229\n",
      "\n",
      "    accuracy                           0.40     85931\n",
      "   macro avg       0.28      0.70      0.20     85931\n",
      "weighted avg       0.92      0.40      0.53     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "scaled_train_report, scaled_test_report = evaluate(reading_comp_scaled, as_doc_train, as_qn_train, train_labels, criterion_scaled), evaluate(reading_comp_scaled, as_doc_test, as_qn_test, test_labels, criterion_scaled)\n",
    "\n",
    "# model evaluation for train and test set\n",
    "print(\"Evaluation on train set\")\n",
    "print(scaled_train_report)\n",
    "print('----------------------------------------------------------')\n",
    "print(\"Evaluation on test set\")\n",
    "print(scaled_test_report)\n",
    "print('----------------------------------------------------------')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Ablation Study - Cosine Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3923\n",
      "Epoch 2/10, Loss: 1.2907\n",
      "Epoch 3/10, Loss: 1.1422\n",
      "Epoch 4/10, Loss: 0.9780\n",
      "Epoch 5/10, Loss: 0.8600\n",
      "Epoch 6/10, Loss: 0.9419\n",
      "Epoch 7/10, Loss: 0.8626\n",
      "Epoch 8/10, Loss: 0.8444\n",
      "Epoch 9/10, Loss: 0.7202\n",
      "Epoch 10/10, Loss: 0.7252\n"
     ]
    }
   ],
   "source": [
    "# testing with cosine similarity attention\n",
    "reading_comp_cosine, criterion_cosine = train(attention_method=\"cosine_similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7232, Accuracy: 0.3062, Precision: 0.2883, Recall: 0.7168, F1: 0.1751\n",
      "Loss: 0.8248, Accuracy: 0.2832, Precision: 0.2822, Recall: 0.6633, F1: 0.1586\n",
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.26      0.42    260691\n",
      "           1       0.02      0.91      0.05       826\n",
      "           2       0.10      0.81      0.19     19947\n",
      "           3       0.03      0.89      0.05       812\n",
      "\n",
      "    accuracy                           0.31    282276\n",
      "   macro avg       0.29      0.72      0.18    282276\n",
      "weighted avg       0.93      0.31      0.40    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.39     80177\n",
      "           1       0.02      0.84      0.04       230\n",
      "           2       0.09      0.81      0.16      5295\n",
      "           3       0.02      0.76      0.04       229\n",
      "\n",
      "    accuracy                           0.28     85931\n",
      "   macro avg       0.28      0.66      0.16     85931\n",
      "weighted avg       0.94      0.28      0.38     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# model evaluation for train and test set\n",
    "cosine_train_report, cosine_test_report = evaluate(reading_comp_cosine, as_doc_train, as_qn_train, train_labels, criterion_cosine), evaluate(reading_comp_cosine, as_doc_test, as_qn_test, test_labels, criterion_cosine)\n",
    "\n",
    "print(\"Evaluation on train set\")\n",
    "print(cosine_train_report)\n",
    "print('----------------------------------------------------------')\n",
    "print(\"Evaluation on test set\")\n",
    "print(cosine_test_report)\n",
    "print('----------------------------------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embeddings Ablation Study\n",
    "\n",
    "The above model used the full context vector with all word embeddings taken (Word2Vec, POS, NER, TF-IDF). In this section, we want to study the results of:\n",
    "1. Word2Vec Word embeddings only\n",
    "2. Word2Vec + TF-IDF\n",
    "3. Full vector, which we have ran the results above\n",
    "\n",
    "As we have seen from the above study that the best attention was the dot product, we will standardise across the 3 experiments using this. Additionally, we use the same hyperparameters as above in the Attention Study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2117, 200, 100]) torch.Size([630, 200, 100]) torch.Size([2117, 23, 100]) torch.Size([630, 23, 100])\n"
     ]
    }
   ],
   "source": [
    "# Word embeds only, important to run this as it also makes sure that the input size is correct\n",
    "as_doc_train, as_doc_test, as_qn_train, as_qn_test = convert_tensors(final_doc_train, final_doc_test, final_qn_train, final_qn_test, 1)\n",
    "# check the tensor sizes\n",
    "print(as_doc_train.shape, as_doc_test.shape, as_qn_train.shape, as_qn_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec Word Embeddings Ablation Study**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3862\n",
      "Epoch 2/10, Loss: 1.2843\n",
      "Epoch 3/10, Loss: 1.1349\n",
      "Epoch 4/10, Loss: 0.9795\n",
      "Epoch 5/10, Loss: 0.8905\n",
      "Epoch 6/10, Loss: 0.9311\n",
      "Epoch 7/10, Loss: 0.7805\n",
      "Epoch 8/10, Loss: 0.7982\n",
      "Epoch 9/10, Loss: 0.6880\n",
      "Epoch 10/10, Loss: 0.6777\n",
      "Loss: 0.6712, Accuracy: 0.5077, Precision: 0.2971, Recall: 0.7111, F1: 0.2562\n",
      "Loss: 0.7897, Accuracy: 0.4777, Precision: 0.2871, Recall: 0.6466, F1: 0.2326\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation for Word Embeds only\n",
    "reading_comp_word, criterion_word = train(attention_method=\"dot_product\")\n",
    "word_train_report, word_test_report = evaluate(reading_comp_word, as_doc_train, as_qn_train, train_labels, criterion_word), evaluate(reading_comp_word, as_doc_test, as_qn_test, test_labels, criterion_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.49      0.65    260691\n",
      "           1       0.03      0.89      0.06       826\n",
      "           2       0.13      0.75      0.22     19947\n",
      "           3       0.05      0.72      0.10       812\n",
      "\n",
      "    accuracy                           0.51    282276\n",
      "   macro avg       0.30      0.71      0.26    282276\n",
      "weighted avg       0.91      0.51      0.62    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.46      0.62     80177\n",
      "           1       0.03      0.81      0.05       230\n",
      "           2       0.10      0.75      0.18      5295\n",
      "           3       0.04      0.56      0.07       229\n",
      "\n",
      "    accuracy                           0.48     85931\n",
      "   macro avg       0.29      0.65      0.23     85931\n",
      "weighted avg       0.92      0.48      0.59     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation for train and test set\n",
    "print(\"Evaluation on train set\")\n",
    "print(word_train_report)\n",
    "print('----------------------------------------------------------')\n",
    "print(\"Evaluation on test set\")\n",
    "print(word_test_report)\n",
    "print('----------------------------------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec + TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2117, 200, 101]) torch.Size([630, 200, 101]) torch.Size([2117, 23, 101]) torch.Size([630, 23, 101])\n"
     ]
    }
   ],
   "source": [
    "# Word embeds + TF-IDF, important to run this as it also makes sure that the input size is correct\n",
    "as_doc_train, as_doc_test, as_qn_train, as_qn_test = convert_tensors(final_doc_train, final_doc_test, final_qn_train, final_qn_test, 2)\n",
    "# check the tensor sizes\n",
    "print(as_doc_train.shape, as_doc_test.shape, as_qn_train.shape, as_qn_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3894\n",
      "Epoch 2/10, Loss: 1.2855\n",
      "Epoch 3/10, Loss: 1.1432\n",
      "Epoch 4/10, Loss: 0.9871\n",
      "Epoch 5/10, Loss: 0.8744\n",
      "Epoch 6/10, Loss: 0.9173\n",
      "Epoch 7/10, Loss: 0.9402\n",
      "Epoch 8/10, Loss: 0.9034\n",
      "Epoch 9/10, Loss: 0.8007\n",
      "Epoch 10/10, Loss: 0.7634\n",
      "Loss: 0.7210, Accuracy: 0.4227, Precision: 0.2837, Recall: 0.6919, F1: 0.2123\n",
      "Loss: 0.7827, Accuracy: 0.4018, Precision: 0.2775, Recall: 0.6506, F1: 0.1973\n"
     ]
    }
   ],
   "source": [
    "reading_comp_tfidf, criterion_tfidf = train(attention_method=\"dot_product\")\n",
    "tfidf_train_report, tfidf_test_report = evaluate(reading_comp_tfidf, as_doc_train, as_qn_train, train_labels, criterion_tfidf), evaluate(reading_comp_tfidf, as_doc_test, as_qn_test, test_labels, criterion_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.41      0.58    260691\n",
      "           1       0.01      0.94      0.03       826\n",
      "           2       0.12      0.51      0.20     19947\n",
      "           3       0.02      0.91      0.05       812\n",
      "\n",
      "    accuracy                           0.42    282276\n",
      "   macro avg       0.28      0.69      0.21    282276\n",
      "weighted avg       0.91      0.42      0.55    282276\n",
      "\n",
      "----------------------------------------------------------\n",
      "Evaluation on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.39      0.56     80177\n",
      "           1       0.01      0.91      0.02       230\n",
      "           2       0.10      0.51      0.17      5295\n",
      "           3       0.02      0.79      0.04       229\n",
      "\n",
      "    accuracy                           0.40     85931\n",
      "   macro avg       0.28      0.65      0.20     85931\n",
      "weighted avg       0.92      0.40      0.53     85931\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation for train and test set\n",
    "print(\"Evaluation on train set\")\n",
    "print(tfidf_train_report)\n",
    "print('----------------------------------------------------------')\n",
    "print(\"Evaluation on test set\")\n",
    "print(tfidf_test_report)\n",
    "print('----------------------------------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
