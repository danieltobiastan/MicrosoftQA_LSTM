{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528971ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3cb696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data\n",
    "train_data = pd.read_csv(\"WikiQA-train.tsv\", sep=\"\\t\")\n",
    "test_data = pd.read_csv(\"WikiQA-test.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fad5982",
   "metadata": {},
   "source": [
    "Extract the unique questions from the train and test data frames, including the documentID and the DocumentTitle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034c4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_documenttag(data):\n",
    "    qd = data[\n",
    "        [\"Question\", \"QuestionID\", \"DocumentID\", \"DocumentTitle\"]\n",
    "    ].drop_duplicates()\n",
    "    return qd\n",
    "\n",
    "\n",
    "train_question_doctag = get_questions_documenttag(train_data)\n",
    "test_question_doctag = get_questions_documenttag(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "368895a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique questions\n",
    "train_questions = train_question_doctag[\"Question\"]\n",
    "test_questions = test_question_doctag[\"Question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f8f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique document ids\n",
    "train_docid = train_question_doctag[\"DocumentID\"]\n",
    "test_docid = test_question_doctag[\"DocumentID\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fbf4306",
   "metadata": {},
   "source": [
    "Extract the answers to those questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d6a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(data, questions, documentids):\n",
    "    answers = []  # list of answers\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q]  # add the document id\n",
    "        df = data[data[\"Question\"] == question]\n",
    "        index = df.loc[df[\"Label\"] == 1][\"Sentence\"].index.values\n",
    "        if len(index) == 0:  # if no answer found\n",
    "            answers.append([question, doc_id, \"No answer\"])\n",
    "        else:  # if 1 answer found\n",
    "            answers.append([question, doc_id, df.loc[index[0], \"Sentence\"]])\n",
    "    return answers\n",
    "\n",
    "\n",
    "train_answers = pd.DataFrame(get_answers(train_data, train_questions, train_docid))\n",
    "test_answers = pd.DataFrame(get_answers(test_data, test_questions, test_docid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12e5527b",
   "metadata": {},
   "source": [
    "The above get_answers returns train_answers and test_answers which, gives us in the following columns\n",
    "\n",
    "-   Question\n",
    "-   Related Document ID\n",
    "-   Answer (if no answer to that question, return no answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac4567fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(data, questions, documentids):  # (done by Finn, tweaked by Dan)\n",
    "    documents = []\n",
    "    for q in range(len(questions)):\n",
    "        question = questions.iloc[q]\n",
    "        doc_id = documentids.iloc[q]  # add the document id\n",
    "        df = data[data[\"Question\"] == question]\n",
    "        sentences = df[\"Sentence\"].tolist()\n",
    "        for i in range(0, len(sentences) - 1):\n",
    "            sentences[i] = sentences[i] + \" \"\n",
    "        documents.append([doc_id, \"\".join(sentences)])\n",
    "    return documents\n",
    "\n",
    "\n",
    "train_documents = pd.DataFrame(\n",
    "    get_documents(train_data, train_questions, train_docid)\n",
    ")  # return the individual document in list\n",
    "test_documents = pd.DataFrame(\n",
    "    get_documents(test_data, test_questions, test_docid)\n",
    ")  # return the individual document in list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0aa7704",
   "metadata": {},
   "source": [
    "The above train_documents and test_documents called from the get_documents gives us in the following columns\n",
    "\n",
    "-   Document ID\n",
    "-   Full Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0432bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming all the columns for more standardised access\n",
    "train_answers.columns = [\"Question\", \"DocumentID\", \"Answer\"]\n",
    "test_answers.columns = [\"Question\", \"DocumentID\", \"Answer\"]\n",
    "train_documents.columns = [\"DocumentID\", \"Document\"]\n",
    "test_documents.columns = [\"DocumentID\", \"Document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "763141d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2117, 2117, 630, 630)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result is 2117, 2117, 630, 630\n",
    "\n",
    "len(train_answers), len(train_documents), len(test_answers), len(test_documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5edcdf6",
   "metadata": {},
   "source": [
    "**Prior to tagging, we should maybe clean the document and answers first:** (stopped here)\n",
    "\n",
    "Maybe?\n",
    "\n",
    "-   lowercase (might lose context, but we can use on questions)\n",
    "-   removing any punctuation or weird symbols (do)\n",
    "-   removal of stop words? (probably not)\n",
    "\n",
    "Make sure that the pre-processing is standardised to be the same throughout doc and ans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "830b5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# These are just common English contractions. We used it in Lab 5 before!\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
    "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
    "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
    "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
    "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
    "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
    "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
    "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
    "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5fddce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lower(text):\n",
    "    # Lowercase the text for question, answer and documents\n",
    "    text = text.lower()\n",
    "    for word, new_word in contraction_dict.items():\n",
    "            text = text.replace(word, new_word) #dealing with contractions\n",
    "    pattern = r\"[^a-zA-Z0-9\\s]\"\n",
    "    cleaned_text = re.sub(pattern, \" \", text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "train_answers[[\"Question\", \"Answer\"]] = train_answers[[\"Question\", \"Answer\"]].applymap(\n",
    "    preprocess_lower\n",
    ")\n",
    "train_documents[\"Document\"] = train_documents[\"Document\"].apply(preprocess_lower)\n",
    "test_answers[[\"Question\", \"Answer\"]] = test_answers[[\"Question\", \"Answer\"]].applymap(\n",
    "    preprocess_lower\n",
    ")\n",
    "test_documents[\"Document\"] = test_documents[\"Document\"].apply(preprocess_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9785d51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>a partly submerged glacier cave on perito more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>in physics   circular motion is a movement of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D5</td>\n",
       "      <td>apollo creed is a fictional character from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D6</td>\n",
       "      <td>in the united states  the title of federal jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D7</td>\n",
       "      <td>the beretta 21a bobcat is a small pocket sized...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>D2805</td>\n",
       "      <td>blue mountain state is an american comedy seri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>D2806</td>\n",
       "      <td>apple inc   formerly apple computer  inc   is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>D2807</td>\n",
       "      <td>section 8 housing in the south bronx section 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>D2808</td>\n",
       "      <td>restaurants categorized by type and informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>D2810</td>\n",
       "      <td>u s  federal reserve notes in the mid 1990s th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2117 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DocumentID                                           Document\n",
       "0            D1  a partly submerged glacier cave on perito more...\n",
       "1            D2  in physics   circular motion is a movement of ...\n",
       "2            D5  apollo creed is a fictional character from the...\n",
       "3            D6  in the united states  the title of federal jud...\n",
       "4            D7  the beretta 21a bobcat is a small pocket sized...\n",
       "...         ...                                                ...\n",
       "2112      D2805  blue mountain state is an american comedy seri...\n",
       "2113      D2806  apple inc   formerly apple computer  inc   is ...\n",
       "2114      D2807  section 8 housing in the south bronx section 8...\n",
       "2115      D2808  restaurants categorized by type and informatio...\n",
       "2116      D2810  u s  federal reserve notes in the mid 1990s th...\n",
       "\n",
       "[2117 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3328fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelling(documents, answers):\n",
    "    tagged_documents = []\n",
    "    for q in range(len(answers)):\n",
    "        tagged_document = []\n",
    "        qn = answers[\"Question\"].loc[q]\n",
    "        doc_id = answers[\"DocumentID\"].loc[q]\n",
    "        content = documents.loc[documents[\"DocumentID\"] == doc_id, \"Document\"].values[0]\n",
    "        answer = answers[\"Answer\"].loc[q]\n",
    "\n",
    "        if answer == \"no answer\":\n",
    "            tokens = word_tokenize(content)\n",
    "            for j in range(len(tokens)):\n",
    "                tagged_document.append(\"N\")  # none\n",
    "        else:\n",
    "            parts = content.partition(answer)\n",
    "            for j in range(len(parts)):\n",
    "                tokens = word_tokenize(parts[j])\n",
    "                if j == 1:\n",
    "                    tagged_document.append(\"S\")  # start of answer\n",
    "                    for k in range(len(tokens) - 2):\n",
    "                        tagged_document.append(\"I\")  # inside of answer\n",
    "                    tagged_document.append(\"E\")  # end of answer\n",
    "                else:\n",
    "                    for k in range(len(tokens)):\n",
    "                        tagged_document.append(\"N\")  # outside answer\n",
    "        tagged_documents.append(tagged_document)\n",
    "    return tagged_documents\n",
    "\n",
    "\n",
    "train_doc_ans_labels = labelling(train_documents, train_answers)\n",
    "test_doc_ans_labels = labelling(test_documents, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fc6f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'egg']\n",
      "['N', 'roll']\n",
      "['N', 'is']\n",
      "['N', 'a']\n",
      "['N', 'term']\n",
      "['N', 'used']\n",
      "['N', 'for']\n",
      "['N', 'many']\n",
      "['N', 'different']\n",
      "['N', 'foods']\n",
      "['N', 'around']\n",
      "['N', 'the']\n",
      "['N', 'world']\n",
      "['S', '2']\n",
      "['I', 'egg']\n",
      "['I', 'roll']\n",
      "['I', 'varieties']\n",
      "['I', 'of']\n",
      "['I', 'egg']\n",
      "['I', 'rolls']\n",
      "['I', 'are']\n",
      "['I', 'found']\n",
      "['I', 'in']\n",
      "['I', 'mainland']\n",
      "['I', 'china']\n",
      "['I', 'many']\n",
      "['I', 'chinese']\n",
      "['I', 'speaking']\n",
      "['I', 'regions']\n",
      "['I', 'of']\n",
      "['I', 'asia']\n",
      "['I', 'and']\n",
      "['I', 'chinese']\n",
      "['I', 'immigrant']\n",
      "['I', 'communities']\n",
      "['I', 'around']\n",
      "['I', 'the']\n",
      "['E', 'world']\n",
      "['N', 'egg']\n",
      "['N', 'rolls']\n",
      "['N', 'as']\n",
      "['N', 'referred']\n",
      "['N', 'to']\n",
      "['N', 'in']\n",
      "['N', 'china']\n",
      "['N', 'in']\n",
      "['N', 'guangdong']\n",
      "['N', 'and']\n",
      "['N', 'hong']\n",
      "['N', 'kong']\n",
      "['N', 'egg']\n",
      "['N', 'roll']\n",
      "['N', 'usually']\n",
      "['N', 'refers']\n",
      "['N', 'to']\n",
      "['N', 'biscuit']\n",
      "['N', 'roll']\n",
      "['N', 'this']\n",
      "['N', 'is']\n",
      "['N', 'a']\n",
      "['N', 'type']\n",
      "['N', 'of']\n",
      "['N', 'biscuit']\n",
      "['N', 'the']\n",
      "['N', 'ingredient']\n",
      "['N', 'included']\n",
      "['N', 'egg']\n",
      "['N', 'flour']\n",
      "['N', 'and']\n",
      "['N', 'sugar']\n",
      "['N', 'egg']\n",
      "['N', 'roll']\n",
      "['N', 'also']\n",
      "['N', 'is']\n",
      "['N', 'a']\n",
      "['N', 'dish']\n",
      "['N', 'in']\n",
      "['N', 'canton']\n",
      "['N', 'and']\n",
      "['N', 'hong']\n",
      "['N', 'kong']\n",
      "['N', 'this']\n",
      "['N', 'is']\n",
      "['N', 'usually']\n",
      "['N', 'made']\n",
      "['N', 'with']\n",
      "['N', 'vegetable']\n",
      "['N', 'within']\n",
      "['N', 'china']\n",
      "['N', 'egg']\n",
      "['N', 'rolls']\n",
      "['N', 'are']\n",
      "['N', 'eaten']\n",
      "['N', 'predominantly']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'southeast']\n",
      "['N', 'and']\n",
      "['N', 'are']\n",
      "['N', 'not']\n",
      "['N', 'as']\n",
      "['N', 'commonly']\n",
      "['N', 'consumed']\n",
      "['N', 'in']\n",
      "['N', 'the']\n",
      "['N', 'north']\n",
      "['N', 'and']\n",
      "['N', 'western']\n",
      "['N', 'parts']\n",
      "['N', 'of']\n",
      "['N', 'china']\n",
      "['N', 'in']\n",
      "['N', 'american']\n",
      "['N', 'chinese']\n",
      "['N', 'cuisine']\n",
      "['N', 'an']\n",
      "['N', 'egg']\n",
      "['N', 'roll']\n",
      "['N', 'is']\n",
      "['N', 'a']\n",
      "['N', 'savory']\n",
      "['N', 'dish']\n",
      "['N', 'typically']\n",
      "['N', 'served']\n",
      "['N', 'as']\n",
      "['N', 'an']\n",
      "['N', 'appetizer']\n",
      "['N', 'it']\n",
      "['N', 'is']\n",
      "['N', 'usually']\n",
      "['N', 'stuffed']\n",
      "['N', 'with']\n",
      "['N', 'chicken']\n",
      "['N', 'pork']\n",
      "['N', 'or']\n",
      "['N', 'shrimp']\n",
      "['N', 'cabbage']\n",
      "['N', 'carrots']\n",
      "['N', 'tomatoes']\n",
      "['N', 'bean']\n",
      "['N', 'sprouts']\n",
      "['N', 'and']\n",
      "['N', 'other']\n",
      "['N', 'vegetables']\n",
      "['N', 'and']\n",
      "['N', 'then']\n",
      "['N', 'deep']\n",
      "['N', 'fried']\n",
      "['N', 'this']\n",
      "['N', 'variety']\n",
      "['N', 'of']\n",
      "['N', 'the']\n",
      "['N', 'egg']\n",
      "['N', 'roll']\n",
      "['N', 'is']\n",
      "['N', 'very']\n",
      "['N', 'common']\n",
      "['N', 'and']\n",
      "['N', 'popular']\n",
      "['N', 'across']\n",
      "['N', 'even']\n",
      "['N', 'regional']\n",
      "['N', 'varieties']\n",
      "['N', 'of']\n",
      "['N', 'american']\n",
      "['N', 'chinese']\n",
      "['N', 'food']\n",
      "['N', 'and']\n",
      "['N', 'is']\n",
      "['N', 'often']\n",
      "['N', 'included']\n",
      "['N', 'as']\n",
      "['N', 'part']\n",
      "['N', 'of']\n",
      "['N', 'a']\n",
      "['N', 'combination']\n",
      "['N', 'platter']\n",
      "['N', 'in']\n",
      "['N', 'montreal']\n",
      "['N', 'canada']\n",
      "['N', 'open']\n",
      "['N', 'ended']\n",
      "['N', 'eggrolls']\n",
      "['N', 'are']\n",
      "['N', 'a']\n",
      "['N', 'jewish']\n",
      "['N', 'chinese']\n",
      "['N', 'fusion']\n",
      "['N', 'dish']\n",
      "['N', 'they']\n",
      "['N', 'can']\n",
      "['N', 'be']\n",
      "['N', 'either']\n",
      "['N', 'kosher']\n",
      "['N', 'certified']\n",
      "['N', 'or']\n",
      "['N', 'merely']\n",
      "['N', 'kosher']\n",
      "['N', 'style']\n",
      "2  egg roll    varieties of egg rolls are found in mainland china   many chinese speaking regions of asia  and chinese immigrant communities around the world \n"
     ]
    }
   ],
   "source": [
    "# check if tags are good\n",
    "def testing_tokens(ind, labels, documents, answers):\n",
    "    for i, j in zip(labels[ind], word_tokenize(documents[\"Document\"][ind])):\n",
    "        print([i, j])\n",
    "    print(answers[\"Answer\"][ind])\n",
    "\n",
    "testing_tokens(1000, train_doc_ans_labels, train_documents, train_answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a294cdb",
   "metadata": {},
   "source": [
    "Cleaned Documents: train and test\n",
    "\n",
    "train_answers - contains the ['Question','DocumentID','Answer']\n",
    "\n",
    "train_documents - contains the ['DocumentID','Document']\n",
    "\n",
    "train_doc_ans_labels - contains a list of list of answer tags for each document,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ed3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare the document for word embeddings:\n",
    "train_doc_ques = pd.DataFrame(\n",
    "    {\"Document\": train_documents[\"Document\"], \"Question\": train_answers[\"Question\"]}\n",
    ")\n",
    "test_doc_ques = pd.DataFrame(\n",
    "    {\"Document\": test_documents[\"Document\"], \"Question\": test_answers[\"Question\"]}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "453a01e5",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "To use the CBOW model, we need the data in sentences. Extract this from the original dataset, don't use sent_tokenise, will mess with some of the fullstops, we want to maintain structure from above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99ba3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokens(data):\n",
    "    sentence_list = []\n",
    "    for i in range(len(data)):\n",
    "        sentence_list.append(word_tokenize(data[i]))\n",
    "    return sentence_list\n",
    "\n",
    "\n",
    "train_doc_list = word_tokens(train_doc_ques[\"Document\"])\n",
    "train_ques_list = word_tokens(train_doc_ques[\"Question\"])\n",
    "test_doc_list = word_tokens(test_doc_ques[\"Document\"])\n",
    "test_ques_list = word_tokens(test_doc_ques[\"Question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c491320",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = train_doc_list + train_ques_list + test_doc_list + test_ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acb5de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trained, don't have to run this multiple times\n",
    "wc_cbow_model = Word2Vec(\n",
    "    sentences=combined_text,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=2,\n",
    "    epochs=30,\n",
    ")\n",
    "#wc_cbow_model.save(\"cbow.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f8cdd66",
   "metadata": {},
   "source": [
    "To implement QA\n",
    "\n",
    "1. Word Embeddings, using CBOW\n",
    "2. Feature Extraction 1 - POS tags\n",
    "3. Feature Extraction 2 - TF-IDF\n",
    "4. Feature Extraction 3 - NER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae9d019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this if model in directory \n",
    "wc_cbow_model = Word2Vec.load(\"./cbow.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee6b4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(doc):\n",
    "    tokenized_doc = word_tokenize(doc)\n",
    "    embeddings = [wc_cbow_model.wv[word] for word in tokenized_doc]\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "train_doc_ques[\"Doc_Embeddings\"] = train_doc_ques[\"Document\"].apply(get_word_embeddings)\n",
    "train_doc_ques[\"Q_Embeddings\"] = train_doc_ques[\"Question\"].apply(get_word_embeddings)\n",
    "test_doc_ques[\"Doc_Embeddings\"] = test_doc_ques[\"Document\"].apply(get_word_embeddings)\n",
    "test_doc_ques[\"Q_Embeddings\"] = test_doc_ques[\"Question\"].apply(get_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7da9a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_ques[\"Doc_Tokens\"] = train_doc_ques[\"Document\"].apply(word_tokenize)\n",
    "train_doc_ques[\"Q_Tokens\"] = train_doc_ques[\"Question\"].apply(word_tokenize)\n",
    "test_doc_ques[\"Doc_Tokens\"] = test_doc_ques[\"Document\"].apply(word_tokenize)\n",
    "test_doc_ques[\"Q_Tokens\"] = test_doc_ques[\"Question\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00f3a5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Max Seq Length is 924, Median is 182.0, Number of lines is 2117)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_length(column):\n",
    "    \n",
    "    max_length = 0\n",
    "    lns = []\n",
    "    for i in range(len(column)):\n",
    "        lns.append(len(column[i]))\n",
    "        if len(column[i]) > max_length:\n",
    "            max_length = len(column[i])\n",
    "    return (\"Max Seq Length is {}, Median is {}, Number of lines is {})\".format(max_length, np.median(lns), len(lns)))\n",
    "\n",
    "find_max_length(test_doc_ques[\"Doc_Embeddings\"])\n",
    "find_max_length(train_doc_ques[\"Doc_Embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdf6d45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_count(doc):\n",
    "    count = 0\n",
    "    for i in range(len(doc)):\n",
    "        if len(doc[\"Doc_Embeddings\"][i]) != len(doc[\"Doc_Tokens\"][i]):\n",
    "            count += 1\n",
    "        elif len(doc[\"Q_Embeddings\"][i]) != len(doc[\"Q_Tokens\"][i]):\n",
    "            count += 1\n",
    "        else:\n",
    "            continue\n",
    "    return count\n",
    "\n",
    "\n",
    "check_count(train_doc_ques)  # looks good"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46810415",
   "metadata": {},
   "source": [
    "Note, need to convert the POS tags, NER tags into embeddings. After this, pad the questions and answers to the max question/document length in the combined training and test set.\n",
    "\n",
    "### PoS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea21daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Apply the pos tags to the tokens\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# download the dependency and resource as required\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "train_doc_ques[\"Doc_POS\"] = train_doc_ques[\"Doc_Tokens\"].apply(pos_tag)\n",
    "train_doc_ques[\"Q_POS\"] = train_doc_ques[\"Q_Tokens\"].apply(pos_tag)\n",
    "test_doc_ques[\"Doc_POS\"] = test_doc_ques[\"Doc_Tokens\"].apply(pos_tag)\n",
    "test_doc_ques[\"Q_POS\"] = test_doc_ques[\"Q_Tokens\"].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e62df735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how', 'WRB'),\n",
       " ('many', 'JJ'),\n",
       " ('schools', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('ten', 'NN')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the POS tags: # looks ok\n",
    "train_doc_ques[\"Q_POS\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfdef90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$': 0,\n",
       " 'CC': 1,\n",
       " 'CD': 2,\n",
       " 'DT': 3,\n",
       " 'EX': 4,\n",
       " 'FW': 5,\n",
       " 'IN': 6,\n",
       " 'JJ': 7,\n",
       " 'JJR': 8,\n",
       " 'JJS': 9,\n",
       " 'MD': 10,\n",
       " 'NN': 11,\n",
       " 'NNP': 12,\n",
       " 'NNPS': 13,\n",
       " 'NNS': 14,\n",
       " 'PDT': 15,\n",
       " 'POS': 16,\n",
       " 'PRP': 17,\n",
       " 'PRP$': 18,\n",
       " 'RB': 19,\n",
       " 'RBR': 20,\n",
       " 'RBS': 21,\n",
       " 'RP': 22,\n",
       " 'SYM': 23,\n",
       " 'TO': 24,\n",
       " 'UH': 25,\n",
       " 'VB': 26,\n",
       " 'VBD': 27,\n",
       " 'VBG': 28,\n",
       " 'VBN': 29,\n",
       " 'VBP': 30,\n",
       " 'VBZ': 31,\n",
       " 'WDT': 32,\n",
       " 'WP': 33,\n",
       " 'WP$': 34,\n",
       " 'WRB': 35}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all unique POS Tags\n",
    "all_pos_tags = (\n",
    "    train_doc_ques[\"Doc_POS\"].tolist()\n",
    "    + test_doc_ques[\"Doc_POS\"].tolist()\n",
    "    + train_doc_ques[\"Q_POS\"].tolist()\n",
    "    + test_doc_ques[\"Q_POS\"].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "def get_unique_pos(data):\n",
    "    pos_tags = set()\n",
    "    for item in data:\n",
    "        for _, pos_tag in item:\n",
    "            pos_tags.add(pos_tag)\n",
    "\n",
    "    pos_tag_index = {tag: i for i, tag in enumerate(sorted(pos_tags))}\n",
    "    return pos_tag_index\n",
    "\n",
    "\n",
    "pos_iden = get_unique_pos(all_pos_tags)  # list of tags\n",
    "pos_iden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00c06ede",
   "metadata": {},
   "source": [
    "### NER Tagging\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d9b9a90",
   "metadata": {},
   "source": [
    "### Steps to run this:\n",
    "\n",
    "-   pip install spacy\n",
    "-   python -m spacy download en_core_web_sm\n",
    "\n",
    "If loaded for the first time, restart kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89121aaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 18:14:02.660149: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# nltk using Spacy\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "# loading pre-trained model of NER\n",
    "#nlp = en_core_web_sm.load()\n",
    "#nlp.to_disk('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32e50019",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('./en_core_web_sm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab395e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tagging(texts):\n",
    "    tagged_texts = []\n",
    "    for text in texts:\n",
    "        doc = spacy.tokens.Doc(nlp.vocab, words=text)\n",
    "        nlp.get_pipe(\"ner\")(doc)\n",
    "        tagged_texts.append([(token.text, token.ent_type_) for token in doc])\n",
    "    return tagged_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "946fb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will take a while...\n",
    "train_doc_ques[\"Doc_NER\"] = ner_tagging(train_doc_ques[\"Doc_Tokens\"])\n",
    "train_doc_ques[\"Q_NER\"] = ner_tagging(train_doc_ques[\"Q_Tokens\"])\n",
    "test_doc_ques[\"Doc_NER\"] = ner_tagging(test_doc_ques[\"Doc_Tokens\"])\n",
    "test_doc_ques[\"Q_NER\"] = ner_tagging(test_doc_ques[\"Q_Tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5ab4ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how', ''),\n",
       " ('much', ''),\n",
       " ('are', ''),\n",
       " ('the', ''),\n",
       " ('harry', 'WORK_OF_ART'),\n",
       " ('potter', 'WORK_OF_ART'),\n",
       " ('movies', 'WORK_OF_ART'),\n",
       " ('worth', 'WORK_OF_ART')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc_ques[\"Q_NER\"][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25cba08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'CARDINAL': 1,\n",
       " 'DATE': 2,\n",
       " 'EVENT': 3,\n",
       " 'FAC': 4,\n",
       " 'GPE': 5,\n",
       " 'LANGUAGE': 6,\n",
       " 'LAW': 7,\n",
       " 'LOC': 8,\n",
       " 'MONEY': 9,\n",
       " 'NORP': 10,\n",
       " 'ORDINAL': 11,\n",
       " 'ORG': 12,\n",
       " 'PERCENT': 13,\n",
       " 'PERSON': 14,\n",
       " 'PRODUCT': 15,\n",
       " 'QUANTITY': 16,\n",
       " 'TIME': 17,\n",
       " 'WORK_OF_ART': 18}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar approach to the POS\n",
    "\n",
    "# Extract all unique POS Tags\n",
    "all_ner_tags = (\n",
    "    train_doc_ques[\"Doc_NER\"].tolist()\n",
    "    + test_doc_ques[\"Doc_NER\"].tolist()\n",
    "    + train_doc_ques[\"Q_NER\"].tolist()\n",
    "    + test_doc_ques[\"Q_NER\"].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "def get_unique_ner(data):\n",
    "    ner_tags = set()\n",
    "    for item in data:\n",
    "        for _, ner_tag in item:\n",
    "            ner_tags.add(ner_tag)\n",
    "\n",
    "    ner_tag_index = {tag: i for i, tag in enumerate(sorted(ner_tags))}\n",
    "    return ner_tag_index\n",
    "\n",
    "\n",
    "ner_iden = get_unique_pos(all_ner_tags)  # list of tags\n",
    "ner_iden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d28ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ohv dims\n",
    "ner_idx = ner_iden.values()\n",
    "aa = np.eye(max(ner_idx) + 1)\n",
    "#aa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd6fafd8",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "First, calculate the document frequency of each token in the entire corpus (training documents + testing documents). The result is a dictionary where each token is a key and its value is the document frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85be9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_frequency(corpus):\n",
    "    \"\"\"\n",
    "    Computes the document frequency for every token in the corpus.\n",
    "    Returns a dictionary {token: doc_freq, ...}\n",
    "    \"\"\"\n",
    "    document_frequency = {}\n",
    "    for document in corpus:\n",
    "        for token in np.unique(document):\n",
    "            try:\n",
    "                document_frequency[token] += 1\n",
    "            except:\n",
    "                document_frequency[token] = 1\n",
    "    return document_frequency\n",
    "\n",
    "\n",
    "train_corpus = (\n",
    "    train_doc_ques[\"Doc_Tokens\"].tolist() + train_doc_ques[\"Q_Tokens\"].tolist()\n",
    ")\n",
    "test_corpus = test_doc_ques[\"Doc_Tokens\"].tolist() + test_doc_ques[\"Q_Tokens\"].tolist()\n",
    "train_doc_freq = document_frequency(train_corpus)\n",
    "test_doc_freq = document_frequency(test_corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0adc55c",
   "metadata": {},
   "source": [
    "Now calculate TF-IDF using the document frequency from above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10f840c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "def compute_tf_idf(corpus, doc_frequency):\n",
    "    \"\"\"\n",
    "    Computes the term frequency inverse document frequency for every token in every document in the corpus.\n",
    "    Returns a list the same shape as the list of tokenized documents except every token is replaced with the tf-idf\n",
    "    for that token.\n",
    "    \"\"\"\n",
    "    tf_idf = {}\n",
    "    tf_idf_list = []\n",
    "    N = len(doc_frequency)\n",
    "    doc_id = 0\n",
    "    for document in corpus:\n",
    "        tf_idf_doc = []\n",
    "        counter = Counter(document)\n",
    "        total_num_words = len(document)\n",
    "        for token in np.unique(document):\n",
    "            tf = counter[token] / total_num_words\n",
    "            df = doc_frequency[token]\n",
    "            idf = math.log(N / (df + 1)) + 1\n",
    "            tf_idf[doc_id, token] = tf * idf\n",
    "        for token in document:\n",
    "            tf_idf_doc.append(tf_idf[doc_id, token])\n",
    "        tf_idf_list.append(tf_idf_doc)\n",
    "        doc_id += 1\n",
    "    return tf_idf_list\n",
    "\n",
    "\n",
    "train_doc_ques[\"Doc_TFIDF\"] = compute_tf_idf(\n",
    "    train_doc_ques[\"Doc_Tokens\"].tolist(), train_doc_freq\n",
    ")\n",
    "train_doc_ques[\"Q_TFIDF\"] = compute_tf_idf(\n",
    "    train_doc_ques[\"Q_Tokens\"].tolist(), train_doc_freq\n",
    ")\n",
    "test_doc_ques[\"Doc_TFIDF\"] = compute_tf_idf(\n",
    "    test_doc_ques[\"Doc_Tokens\"].tolist(), test_doc_freq\n",
    ")\n",
    "test_doc_ques[\"Q_TFIDF\"] = compute_tf_idf(\n",
    "    test_doc_ques[\"Q_Tokens\"].tolist(), test_doc_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8565260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Question</th>\n",
       "      <th>Doc_Embeddings</th>\n",
       "      <th>Q_Embeddings</th>\n",
       "      <th>Doc_Tokens</th>\n",
       "      <th>Q_Tokens</th>\n",
       "      <th>Doc_POS</th>\n",
       "      <th>Q_POS</th>\n",
       "      <th>Doc_NER</th>\n",
       "      <th>Q_NER</th>\n",
       "      <th>Doc_TFIDF</th>\n",
       "      <th>Q_TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>african immigration to the united states refer...</td>\n",
       "      <td>how african americans were immigrated to the us</td>\n",
       "      <td>[[-0.16284928, -0.5099784, -2.502221, 0.793150...</td>\n",
       "      <td>[[0.08589837, 2.4839535, -2.2136166, 3.3471255...</td>\n",
       "      <td>[african, immigration, to, the, united, states...</td>\n",
       "      <td>[how, african, americans, were, immigrated, to...</td>\n",
       "      <td>[(african, JJ), (immigration, NN), (to, TO), (...</td>\n",
       "      <td>[(how, WRB), (african, JJ), (americans, NNS), ...</td>\n",
       "      <td>[(african, ORG), (immigration, ORG), (to, ), (...</td>\n",
       "      <td>[(how, ), (african, NORP), (americans, NORP), ...</td>\n",
       "      <td>[0.2444338691504048, 0.1690021701802108, 0.283...</td>\n",
       "      <td>[0.708514287434769, 0.9349595495002982, 0.9907...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a prison  from old french prisoun   also known...</td>\n",
       "      <td>how large were early jails</td>\n",
       "      <td>[[0.6947981, -0.006612428, -0.9311606, 0.93038...</td>\n",
       "      <td>[[0.08589837, 2.4839535, -2.2136166, 3.3471255...</td>\n",
       "      <td>[a, prison, from, old, french, prisoun, also, ...</td>\n",
       "      <td>[how, large, were, early, jails]</td>\n",
       "      <td>[(a, DT), (prison, NN), (from, IN), (old, JJ),...</td>\n",
       "      <td>[(how, WRB), (large, JJ), (were, VBD), (early,...</td>\n",
       "      <td>[(a, ), (prison, ), (from, ), (old, ), (french...</td>\n",
       "      <td>[(how, ), (large, ), (were, ), (early, ), (jai...</td>\n",
       "      <td>[0.23734258656531904, 0.15055215160158517, 0.0...</td>\n",
       "      <td>[1.1336228598956304, 1.2987719202959243, 1.145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a small  electrically powered pump a large  el...</td>\n",
       "      <td>how a water pump works</td>\n",
       "      <td>[[0.6947981, -0.006612428, -0.9311606, 0.93038...</td>\n",
       "      <td>[[0.08589837, 2.4839535, -2.2136166, 3.3471255...</td>\n",
       "      <td>[a, small, electrically, powered, pump, a, lar...</td>\n",
       "      <td>[how, a, water, pump, works]</td>\n",
       "      <td>[(a, DT), (small, JJ), (electrically, RB), (po...</td>\n",
       "      <td>[(how, WRB), (a, DT), (water, NN), (pump, NN),...</td>\n",
       "      <td>[(a, ), (small, ), (electrically, ), (powered,...</td>\n",
       "      <td>[(how, ), (a, ), (water, ), (pump, ), (works, )]</td>\n",
       "      <td>[0.1817774325199266, 0.07355137595811154, 0.19...</td>\n",
       "      <td>[1.1336228598956304, 0.8361761895916624, 1.412...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lolita is a 1962 comedy drama film by stanley ...</td>\n",
       "      <td>how old was sue lyon when she made lolita</td>\n",
       "      <td>[[-0.11471885, 0.26024267, 0.2678885, 0.086137...</td>\n",
       "      <td>[[0.08589837, 2.4839535, -2.2136166, 3.3471255...</td>\n",
       "      <td>[lolita, is, a, 1962, comedy, drama, film, by,...</td>\n",
       "      <td>[how, old, was, sue, lyon, when, she, made, lo...</td>\n",
       "      <td>[(lolita, NN), (is, VBZ), (a, DT), (1962, CD),...</td>\n",
       "      <td>[(how, WRB), (old, JJ), (was, VBD), (sue, NN),...</td>\n",
       "      <td>[(lolita, ), (is, ), (a, ), (1962, DATE), (com...</td>\n",
       "      <td>[(how, ), (old, ), (was, ), (sue, PERSON), (ly...</td>\n",
       "      <td>[0.22677023461895618, 0.03157635832223484, 0.0...</td>\n",
       "      <td>[0.6297904777197946, 0.7657655256778074, 0.528...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>each antibody binds to a specific antigen   an...</td>\n",
       "      <td>how are antibodies used in</td>\n",
       "      <td>[[1.1073288, 2.4011667, -1.4736867, 2.3350687,...</td>\n",
       "      <td>[[0.08589837, 2.4839535, -2.2136166, 3.3471255...</td>\n",
       "      <td>[each, antibody, binds, to, a, specific, antig...</td>\n",
       "      <td>[how, are, antibodies, used, in]</td>\n",
       "      <td>[(each, DT), (antibody, NN), (binds, VBZ), (to...</td>\n",
       "      <td>[(how, WRB), (are, VBP), (antibodies, NNS), (u...</td>\n",
       "      <td>[(each, ), (antibody, ), (binds, ), (to, ), (a...</td>\n",
       "      <td>[(how, ), (are, ), (antibodies, ), (used, ), (...</td>\n",
       "      <td>[0.05405354788321593, 0.24067984633049655, 0.0...</td>\n",
       "      <td>[1.1336228598956304, 0.9669842090544979, 1.862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>american cuts of beef including the brisket br...</td>\n",
       "      <td>where is the brisket from</td>\n",
       "      <td>[[1.6384165, 0.26733622, -0.88549775, 2.357230...</td>\n",
       "      <td>[[-1.611623, 1.7158104, 0.45376357, 1.2674458,...</td>\n",
       "      <td>[american, cuts, of, beef, including, the, bri...</td>\n",
       "      <td>[where, is, the, brisket, from]</td>\n",
       "      <td>[(american, JJ), (cuts, NNS), (of, IN), (beef,...</td>\n",
       "      <td>[(where, WRB), (is, VBZ), (the, DT), (brisket,...</td>\n",
       "      <td>[(american, NORP), (cuts, ), (of, ), (beef, ),...</td>\n",
       "      <td>[(where, ), (is, ), (the, ), (brisket, ), (fro...</td>\n",
       "      <td>[0.04467110190076254, 0.15609658426345493, 0.2...</td>\n",
       "      <td>[1.1855735192809844, 0.8020395013847651, 0.792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>the arm architecture describes a family of ris...</td>\n",
       "      <td>what is arm chipset</td>\n",
       "      <td>[[-1.8290923, -0.30621898, 0.3005725, 0.443662...</td>\n",
       "      <td>[[1.906984, 3.2368956, -2.5320153, -2.3044674,...</td>\n",
       "      <td>[the, arm, architecture, describes, a, family,...</td>\n",
       "      <td>[what, is, arm, chipset]</td>\n",
       "      <td>[(the, DT), (arm, NN), (architecture, NN), (de...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (arm, JJ), (chipset, NN)]</td>\n",
       "      <td>[(the, ), (arm, ), (architecture, ), (describe...</td>\n",
       "      <td>[(what, ), (is, ), (arm, ), (chipset, )]</td>\n",
       "      <td>[0.13239230955602443, 0.2925047479012816, 0.06...</td>\n",
       "      <td>[1.2003495883084616, 1.0025493767309563, 2.188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>june bug or junebug may refer to  beetles  phy...</td>\n",
       "      <td>what is the life span of june bugs</td>\n",
       "      <td>[[-2.4841905, -0.36323845, 1.1136703, 0.393528...</td>\n",
       "      <td>[[1.906984, 3.2368956, -2.5320153, -2.3044674,...</td>\n",
       "      <td>[june, bug, or, junebug, may, refer, to, beetl...</td>\n",
       "      <td>[what, is, the, life, span, of, june, bugs]</td>\n",
       "      <td>[(june, NN), (bug, NN), (or, CC), (junebug, NN...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (the, DT), (life, NN),...</td>\n",
       "      <td>[(june, DATE), (bug, ), (or, ), (junebug, ), (...</td>\n",
       "      <td>[(what, ), (is, ), (the, ), (life, ), (span, )...</td>\n",
       "      <td>[0.3625692810391371, 0.3450139669072635, 0.056...</td>\n",
       "      <td>[0.6001747941542308, 0.5012746883654782, 0.495...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>this is a list of known biological mothers und...</td>\n",
       "      <td>who is the youngest female to give birth world...</td>\n",
       "      <td>[[-0.45228565, -1.4249868, -4.851625, -0.42707...</td>\n",
       "      <td>[[-2.3819268, 1.8224666, -2.229628, -0.3044447...</td>\n",
       "      <td>[this, is, a, list, of, known, biological, mot...</td>\n",
       "      <td>[who, is, the, youngest, female, to, give, bir...</td>\n",
       "      <td>[(this, DT), (is, VBZ), (a, DT), (list, NN), (...</td>\n",
       "      <td>[(who, WP), (is, VBZ), (the, DT), (youngest, J...</td>\n",
       "      <td>[(this, ), (is, ), (a, ), (list, ), (of, ), (k...</td>\n",
       "      <td>[(who, ), (is, ), (the, ), (youngest, ), (fema...</td>\n",
       "      <td>[0.4279041372169481, 0.30847673130183273, 0.32...</td>\n",
       "      <td>[0.5327914192742925, 0.40101975069238255, 0.39...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>a broodmare and foal a mare is an adult female...</td>\n",
       "      <td>what is an open mare</td>\n",
       "      <td>[[0.6947981, -0.006612428, -0.9311606, 0.93038...</td>\n",
       "      <td>[[1.906984, 3.2368956, -2.5320153, -2.3044674,...</td>\n",
       "      <td>[a, broodmare, and, foal, a, mare, is, an, adu...</td>\n",
       "      <td>[what, is, an, open, mare]</td>\n",
       "      <td>[(a, DT), (broodmare, NN), (and, CC), (foal, V...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (an, DT), (open, JJ), ...</td>\n",
       "      <td>[(a, ), (broodmare, ), (and, ), (foal, ), (a, ...</td>\n",
       "      <td>[(what, ), (is, ), (an, ), (open, ), (mare, )]</td>\n",
       "      <td>[0.5552732509007133, 0.15633445375485377, 0.13...</td>\n",
       "      <td>[0.9602796706467693, 0.8020395013847651, 0.964...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Document  \\\n",
       "0    african immigration to the united states refer...   \n",
       "1    a prison  from old french prisoun   also known...   \n",
       "2    a small  electrically powered pump a large  el...   \n",
       "3    lolita is a 1962 comedy drama film by stanley ...   \n",
       "4    each antibody binds to a specific antigen   an...   \n",
       "..                                                 ...   \n",
       "625  american cuts of beef including the brisket br...   \n",
       "626  the arm architecture describes a family of ris...   \n",
       "627  june bug or junebug may refer to  beetles  phy...   \n",
       "628  this is a list of known biological mothers und...   \n",
       "629  a broodmare and foal a mare is an adult female...   \n",
       "\n",
       "                                              Question  \\\n",
       "0      how african americans were immigrated to the us   \n",
       "1                           how large were early jails   \n",
       "2                               how a water pump works   \n",
       "3            how old was sue lyon when she made lolita   \n",
       "4                           how are antibodies used in   \n",
       "..                                                 ...   \n",
       "625                          where is the brisket from   \n",
       "626                                what is arm chipset   \n",
       "627                 what is the life span of june bugs   \n",
       "628  who is the youngest female to give birth world...   \n",
       "629                              what is an open mare    \n",
       "\n",
       "                                        Doc_Embeddings  \\\n",
       "0    [[-0.16284928, -0.5099784, -2.502221, 0.793150...   \n",
       "1    [[0.6947981, -0.006612428, -0.9311606, 0.93038...   \n",
       "2    [[0.6947981, -0.006612428, -0.9311606, 0.93038...   \n",
       "3    [[-0.11471885, 0.26024267, 0.2678885, 0.086137...   \n",
       "4    [[1.1073288, 2.4011667, -1.4736867, 2.3350687,...   \n",
       "..                                                 ...   \n",
       "625  [[1.6384165, 0.26733622, -0.88549775, 2.357230...   \n",
       "626  [[-1.8290923, -0.30621898, 0.3005725, 0.443662...   \n",
       "627  [[-2.4841905, -0.36323845, 1.1136703, 0.393528...   \n",
       "628  [[-0.45228565, -1.4249868, -4.851625, -0.42707...   \n",
       "629  [[0.6947981, -0.006612428, -0.9311606, 0.93038...   \n",
       "\n",
       "                                          Q_Embeddings  \\\n",
       "0    [[0.08589837, 2.4839535, -2.2136166, 3.3471255...   \n",
       "1    [[0.08589837, 2.4839535, -2.2136166, 3.3471255...   \n",
       "2    [[0.08589837, 2.4839535, -2.2136166, 3.3471255...   \n",
       "3    [[0.08589837, 2.4839535, -2.2136166, 3.3471255...   \n",
       "4    [[0.08589837, 2.4839535, -2.2136166, 3.3471255...   \n",
       "..                                                 ...   \n",
       "625  [[-1.611623, 1.7158104, 0.45376357, 1.2674458,...   \n",
       "626  [[1.906984, 3.2368956, -2.5320153, -2.3044674,...   \n",
       "627  [[1.906984, 3.2368956, -2.5320153, -2.3044674,...   \n",
       "628  [[-2.3819268, 1.8224666, -2.229628, -0.3044447...   \n",
       "629  [[1.906984, 3.2368956, -2.5320153, -2.3044674,...   \n",
       "\n",
       "                                            Doc_Tokens  \\\n",
       "0    [african, immigration, to, the, united, states...   \n",
       "1    [a, prison, from, old, french, prisoun, also, ...   \n",
       "2    [a, small, electrically, powered, pump, a, lar...   \n",
       "3    [lolita, is, a, 1962, comedy, drama, film, by,...   \n",
       "4    [each, antibody, binds, to, a, specific, antig...   \n",
       "..                                                 ...   \n",
       "625  [american, cuts, of, beef, including, the, bri...   \n",
       "626  [the, arm, architecture, describes, a, family,...   \n",
       "627  [june, bug, or, junebug, may, refer, to, beetl...   \n",
       "628  [this, is, a, list, of, known, biological, mot...   \n",
       "629  [a, broodmare, and, foal, a, mare, is, an, adu...   \n",
       "\n",
       "                                              Q_Tokens  \\\n",
       "0    [how, african, americans, were, immigrated, to...   \n",
       "1                     [how, large, were, early, jails]   \n",
       "2                         [how, a, water, pump, works]   \n",
       "3    [how, old, was, sue, lyon, when, she, made, lo...   \n",
       "4                     [how, are, antibodies, used, in]   \n",
       "..                                                 ...   \n",
       "625                    [where, is, the, brisket, from]   \n",
       "626                           [what, is, arm, chipset]   \n",
       "627        [what, is, the, life, span, of, june, bugs]   \n",
       "628  [who, is, the, youngest, female, to, give, bir...   \n",
       "629                         [what, is, an, open, mare]   \n",
       "\n",
       "                                               Doc_POS  \\\n",
       "0    [(african, JJ), (immigration, NN), (to, TO), (...   \n",
       "1    [(a, DT), (prison, NN), (from, IN), (old, JJ),...   \n",
       "2    [(a, DT), (small, JJ), (electrically, RB), (po...   \n",
       "3    [(lolita, NN), (is, VBZ), (a, DT), (1962, CD),...   \n",
       "4    [(each, DT), (antibody, NN), (binds, VBZ), (to...   \n",
       "..                                                 ...   \n",
       "625  [(american, JJ), (cuts, NNS), (of, IN), (beef,...   \n",
       "626  [(the, DT), (arm, NN), (architecture, NN), (de...   \n",
       "627  [(june, NN), (bug, NN), (or, CC), (junebug, NN...   \n",
       "628  [(this, DT), (is, VBZ), (a, DT), (list, NN), (...   \n",
       "629  [(a, DT), (broodmare, NN), (and, CC), (foal, V...   \n",
       "\n",
       "                                                 Q_POS  \\\n",
       "0    [(how, WRB), (african, JJ), (americans, NNS), ...   \n",
       "1    [(how, WRB), (large, JJ), (were, VBD), (early,...   \n",
       "2    [(how, WRB), (a, DT), (water, NN), (pump, NN),...   \n",
       "3    [(how, WRB), (old, JJ), (was, VBD), (sue, NN),...   \n",
       "4    [(how, WRB), (are, VBP), (antibodies, NNS), (u...   \n",
       "..                                                 ...   \n",
       "625  [(where, WRB), (is, VBZ), (the, DT), (brisket,...   \n",
       "626  [(what, WP), (is, VBZ), (arm, JJ), (chipset, NN)]   \n",
       "627  [(what, WP), (is, VBZ), (the, DT), (life, NN),...   \n",
       "628  [(who, WP), (is, VBZ), (the, DT), (youngest, J...   \n",
       "629  [(what, WP), (is, VBZ), (an, DT), (open, JJ), ...   \n",
       "\n",
       "                                               Doc_NER  \\\n",
       "0    [(african, ORG), (immigration, ORG), (to, ), (...   \n",
       "1    [(a, ), (prison, ), (from, ), (old, ), (french...   \n",
       "2    [(a, ), (small, ), (electrically, ), (powered,...   \n",
       "3    [(lolita, ), (is, ), (a, ), (1962, DATE), (com...   \n",
       "4    [(each, ), (antibody, ), (binds, ), (to, ), (a...   \n",
       "..                                                 ...   \n",
       "625  [(american, NORP), (cuts, ), (of, ), (beef, ),...   \n",
       "626  [(the, ), (arm, ), (architecture, ), (describe...   \n",
       "627  [(june, DATE), (bug, ), (or, ), (junebug, ), (...   \n",
       "628  [(this, ), (is, ), (a, ), (list, ), (of, ), (k...   \n",
       "629  [(a, ), (broodmare, ), (and, ), (foal, ), (a, ...   \n",
       "\n",
       "                                                 Q_NER  \\\n",
       "0    [(how, ), (african, NORP), (americans, NORP), ...   \n",
       "1    [(how, ), (large, ), (were, ), (early, ), (jai...   \n",
       "2     [(how, ), (a, ), (water, ), (pump, ), (works, )]   \n",
       "3    [(how, ), (old, ), (was, ), (sue, PERSON), (ly...   \n",
       "4    [(how, ), (are, ), (antibodies, ), (used, ), (...   \n",
       "..                                                 ...   \n",
       "625  [(where, ), (is, ), (the, ), (brisket, ), (fro...   \n",
       "626           [(what, ), (is, ), (arm, ), (chipset, )]   \n",
       "627  [(what, ), (is, ), (the, ), (life, ), (span, )...   \n",
       "628  [(who, ), (is, ), (the, ), (youngest, ), (fema...   \n",
       "629     [(what, ), (is, ), (an, ), (open, ), (mare, )]   \n",
       "\n",
       "                                             Doc_TFIDF  \\\n",
       "0    [0.2444338691504048, 0.1690021701802108, 0.283...   \n",
       "1    [0.23734258656531904, 0.15055215160158517, 0.0...   \n",
       "2    [0.1817774325199266, 0.07355137595811154, 0.19...   \n",
       "3    [0.22677023461895618, 0.03157635832223484, 0.0...   \n",
       "4    [0.05405354788321593, 0.24067984633049655, 0.0...   \n",
       "..                                                 ...   \n",
       "625  [0.04467110190076254, 0.15609658426345493, 0.2...   \n",
       "626  [0.13239230955602443, 0.2925047479012816, 0.06...   \n",
       "627  [0.3625692810391371, 0.3450139669072635, 0.056...   \n",
       "628  [0.4279041372169481, 0.30847673130183273, 0.32...   \n",
       "629  [0.5552732509007133, 0.15633445375485377, 0.13...   \n",
       "\n",
       "                                               Q_TFIDF  \n",
       "0    [0.708514287434769, 0.9349595495002982, 0.9907...  \n",
       "1    [1.1336228598956304, 1.2987719202959243, 1.145...  \n",
       "2    [1.1336228598956304, 0.8361761895916624, 1.412...  \n",
       "3    [0.6297904777197946, 0.7657655256778074, 0.528...  \n",
       "4    [1.1336228598956304, 0.9669842090544979, 1.862...  \n",
       "..                                                 ...  \n",
       "625  [1.1855735192809844, 0.8020395013847651, 0.792...  \n",
       "626  [1.2003495883084616, 1.0025493767309563, 2.188...  \n",
       "627  [0.6001747941542308, 0.5012746883654782, 0.495...  \n",
       "628  [0.5327914192742925, 0.40101975069238255, 0.39...  \n",
       "629  [0.9602796706467693, 0.8020395013847651, 0.964...  \n",
       "\n",
       "[630 rows x 12 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc_ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d807d2d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Question</th>\n",
       "      <th>Doc_Embeddings</th>\n",
       "      <th>Q_Embeddings</th>\n",
       "      <th>Doc_Tokens</th>\n",
       "      <th>Q_Tokens</th>\n",
       "      <th>Doc_POS</th>\n",
       "      <th>Q_POS</th>\n",
       "      <th>Doc_NER</th>\n",
       "      <th>Q_NER</th>\n",
       "      <th>Doc_TFIDF</th>\n",
       "      <th>Q_TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a partly submerged glacier cave on perito more...</td>\n",
       "      <td>how are glacier caves formed</td>\n",
       "      <td>[[0.6947981, -0.006612428, -0.9311606, 0.93038...</td>\n",
       "      <td>[[0.08589837, 2.4839535, -2.2136166, 3.3471255...</td>\n",
       "      <td>[a, partly, submerged, glacier, cave, on, peri...</td>\n",
       "      <td>[how, are, glacier, caves, formed]</td>\n",
       "      <td>[(a, DT), (partly, RB), (submerged, VBN), (gla...</td>\n",
       "      <td>[(how, WRB), (are, VBP), (glacier, JJ), (caves...</td>\n",
       "      <td>[(a, ), (partly, ), (submerged, ), (glacier, )...</td>\n",
       "      <td>[(how, ), (are, ), (glacier, ), (caves, ), (fo...</td>\n",
       "      <td>[0.24677746860673927, 0.1451147752938821, 0.16...</td>\n",
       "      <td>[1.034209132108837, 0.8495245060136498, 1.9896...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in physics   circular motion is a movement of ...</td>\n",
       "      <td>how are the directions of the velocity and for...</td>\n",
       "      <td>[[-0.3025114, 0.803383, 0.88826, 0.36154857, 1...</td>\n",
       "      <td>[[0.08589837, 2.4839535, -2.2136166, 3.3471255...</td>\n",
       "      <td>[in, physics, circular, motion, is, a, movemen...</td>\n",
       "      <td>[how, are, the, directions, of, the, velocity,...</td>\n",
       "      <td>[(in, IN), (physics, NNS), (circular, JJ), (mo...</td>\n",
       "      <td>[(how, WRB), (are, VBP), (the, DT), (direction...</td>\n",
       "      <td>[(in, ), (physics, ), (circular, ), (motion, )...</td>\n",
       "      <td>[(how, ), (are, ), (the, ), (directions, ), (o...</td>\n",
       "      <td>[0.10157954319378303, 0.047977208560696934, 0....</td>\n",
       "      <td>[0.34473637736961227, 0.28317483533788324, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apollo creed is a fictional character from the...</td>\n",
       "      <td>how did apollo creed die</td>\n",
       "      <td>[[-0.08092001, 0.23716491, 0.7449493, 0.191466...</td>\n",
       "      <td>[[0.08589837, 2.4839535, -2.2136166, 3.3471255...</td>\n",
       "      <td>[apollo, creed, is, a, fictional, character, f...</td>\n",
       "      <td>[how, did, apollo, creed, die]</td>\n",
       "      <td>[(apollo, NNS), (creed, VBP), (is, VBZ), (a, D...</td>\n",
       "      <td>[(how, WRB), (did, VBD), (apollo, VB), (creed,...</td>\n",
       "      <td>[(apollo, ORG), (creed, ), (is, ), (a, ), (fic...</td>\n",
       "      <td>[(how, ), (did, ), (apollo, ORG), (creed, ), (...</td>\n",
       "      <td>[0.15061458188394025, 0.28752101617726733, 0.0...</td>\n",
       "      <td>[1.034209132108837, 1.1532031921635457, 1.7872...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in the united states  the title of federal jud...</td>\n",
       "      <td>how long is the term for federal judges</td>\n",
       "      <td>[[-0.3025114, 0.803383, 0.88826, 0.36154857, 1...</td>\n",
       "      <td>[[0.08589837, 2.4839535, -2.2136166, 3.3471255...</td>\n",
       "      <td>[in, the, united, states, the, title, of, fede...</td>\n",
       "      <td>[how, long, is, the, term, for, federal, judges]</td>\n",
       "      <td>[(in, IN), (the, DT), (united, JJ), (states, V...</td>\n",
       "      <td>[(how, WRB), (long, JJ), (is, VBZ), (the, DT),...</td>\n",
       "      <td>[(in, ), (the, GPE), (united, GPE), (states, G...</td>\n",
       "      <td>[(how, ), (long, ), (is, ), (the, ), (term, ),...</td>\n",
       "      <td>[0.08828641778817685, 0.32044981985432075, 0.1...</td>\n",
       "      <td>[0.646380707568023, 0.7777965845277852, 0.4296...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the beretta 21a bobcat is a small pocket sized...</td>\n",
       "      <td>how a beretta model 21 pistols magazines works</td>\n",
       "      <td>[[-1.8290923, -0.30621898, 0.3005725, 0.443662...</td>\n",
       "      <td>[[0.08589837, 2.4839535, -2.2136166, 3.3471255...</td>\n",
       "      <td>[the, beretta, 21a, bobcat, is, a, small, pock...</td>\n",
       "      <td>[how, a, beretta, model, 21, pistols, magazine...</td>\n",
       "      <td>[(the, DT), (beretta, NN), (21a, CD), (bobcat,...</td>\n",
       "      <td>[(how, WRB), (a, DT), (beretta, NN), (model, N...</td>\n",
       "      <td>[(the, ), (beretta, ), (21a, ), (bobcat, ), (i...</td>\n",
       "      <td>[(how, ), (a, ), (beretta, PRODUCT), (model, )...</td>\n",
       "      <td>[0.21160137561032594, 0.8290055086215778, 0.22...</td>\n",
       "      <td>[0.646380707568023, 0.4549959577436755, 1.2435...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>blue mountain state is an american comedy seri...</td>\n",
       "      <td>where was blue mountain state filmed at</td>\n",
       "      <td>[[-0.2716396, 2.832203, 2.2265713, 1.0802964, ...</td>\n",
       "      <td>[[-1.611623, 1.7158104, 0.45376357, 1.2674458,...</td>\n",
       "      <td>[blue, mountain, state, is, an, american, come...</td>\n",
       "      <td>[where, was, blue, mountain, state, filmed, at]</td>\n",
       "      <td>[(blue, JJ), (mountain, NN), (state, NN), (is,...</td>\n",
       "      <td>[(where, WRB), (was, VBD), (blue, JJ), (mounta...</td>\n",
       "      <td>[(blue, LOC), (mountain, LOC), (state, ), (is,...</td>\n",
       "      <td>[(where, ), (was, ), (blue, ), (mountain, ), (...</td>\n",
       "      <td>[0.29475406599874054, 0.3823486411848812, 0.22...</td>\n",
       "      <td>[0.7503059363146255, 0.5906688741268076, 1.063...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>apple inc   formerly apple computer  inc   is ...</td>\n",
       "      <td>when was apple computer founded</td>\n",
       "      <td>[[0.24397807, 0.11627979, 0.98766154, -1.33965...</td>\n",
       "      <td>[[-3.38445, 1.818213, -0.19103132, -0.26652685...</td>\n",
       "      <td>[apple, inc, formerly, apple, computer, inc, i...</td>\n",
       "      <td>[when, was, apple, computer, founded]</td>\n",
       "      <td>[(apple, NN), (inc, VBP), (formerly, RB), (app...</td>\n",
       "      <td>[(when, WRB), (was, VBD), (apple, NN), (comput...</td>\n",
       "      <td>[(apple, ORG), (inc, ORG), (formerly, ORG), (a...</td>\n",
       "      <td>[(when, ), (was, ), (apple, ), (computer, ), (...</td>\n",
       "      <td>[0.2615737575545419, 0.08943278683987749, 0.02...</td>\n",
       "      <td>[0.9915267033360398, 0.8269364237775307, 1.700...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>section 8 housing in the south bronx section 8...</td>\n",
       "      <td>what is section eight housing</td>\n",
       "      <td>[[0.83727205, 0.588537, -0.8883795, 1.2417848,...</td>\n",
       "      <td>[[1.906984, 3.2368956, -2.5320153, -2.3044674,...</td>\n",
       "      <td>[section, 8, housing, in, the, south, bronx, s...</td>\n",
       "      <td>[what, is, section, eight, housing]</td>\n",
       "      <td>[(section, NN), (8, CD), (housing, NN), (in, I...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (section, NN), (eight,...</td>\n",
       "      <td>[(section, LAW), (8, LAW), (housing, ), (in, )...</td>\n",
       "      <td>[(what, ), (is, ), (section, ), (eight, CARDIN...</td>\n",
       "      <td>[0.19207290739926391, 0.16089672153840806, 0.2...</td>\n",
       "      <td>[0.8426216022668568, 0.6874871616619922, 1.514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>restaurants categorized by type and informatio...</td>\n",
       "      <td>what is the main type of restaurant</td>\n",
       "      <td>[[0.62694025, 0.13019273, -0.10285008, 1.67548...</td>\n",
       "      <td>[[1.906984, 3.2368956, -2.5320153, -2.3044674,...</td>\n",
       "      <td>[restaurants, categorized, by, type, and, info...</td>\n",
       "      <td>[what, is, the, main, type, of, restaurant]</td>\n",
       "      <td>[(restaurants, NNS), (categorized, VBN), (by, ...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (the, DT), (main, JJ),...</td>\n",
       "      <td>[(restaurants, ), (categorized, ), (by, ), (ty...</td>\n",
       "      <td>[(what, ), (is, ), (the, ), (main, ), (type, )...</td>\n",
       "      <td>[0.876941110711729, 0.9254918922898989, 0.4042...</td>\n",
       "      <td>[0.6018725730477548, 0.49106225832999434, 0.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>u s  federal reserve notes in the mid 1990s th...</td>\n",
       "      <td>what is us dollar worth based on</td>\n",
       "      <td>[[1.8772068, -1.9619582, 0.12009179, 0.1666371...</td>\n",
       "      <td>[[1.906984, 3.2368956, -2.5320153, -2.3044674,...</td>\n",
       "      <td>[u, s, federal, reserve, notes, in, the, mid, ...</td>\n",
       "      <td>[what, is, us, dollar, worth, based, on]</td>\n",
       "      <td>[(u, JJ), (s, JJ), (federal, JJ), (reserve, NN...</td>\n",
       "      <td>[(what, WP), (is, VBZ), (us, PRP), (dollar, NN...</td>\n",
       "      <td>[(u, ), (s, ORG), (federal, ORG), (reserve, OR...</td>\n",
       "      <td>[(what, ), (is, ), (us, ), (dollar, ), (worth,...</td>\n",
       "      <td>[0.2686013488464822, 0.19929661266768658, 0.30...</td>\n",
       "      <td>[0.6018725730477548, 0.49106225832999434, 0.88...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2117 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Document  \\\n",
       "0     a partly submerged glacier cave on perito more...   \n",
       "1     in physics   circular motion is a movement of ...   \n",
       "2     apollo creed is a fictional character from the...   \n",
       "3     in the united states  the title of federal jud...   \n",
       "4     the beretta 21a bobcat is a small pocket sized...   \n",
       "...                                                 ...   \n",
       "2112  blue mountain state is an american comedy seri...   \n",
       "2113  apple inc   formerly apple computer  inc   is ...   \n",
       "2114  section 8 housing in the south bronx section 8...   \n",
       "2115  restaurants categorized by type and informatio...   \n",
       "2116  u s  federal reserve notes in the mid 1990s th...   \n",
       "\n",
       "                                               Question  \\\n",
       "0                         how are glacier caves formed    \n",
       "1     how are the directions of the velocity and for...   \n",
       "2                              how did apollo creed die   \n",
       "3               how long is the term for federal judges   \n",
       "4        how a beretta model 21 pistols magazines works   \n",
       "...                                                 ...   \n",
       "2112            where was blue mountain state filmed at   \n",
       "2113                    when was apple computer founded   \n",
       "2114                      what is section eight housing   \n",
       "2115                what is the main type of restaurant   \n",
       "2116                   what is us dollar worth based on   \n",
       "\n",
       "                                         Doc_Embeddings  \\\n",
       "0     [[0.6947981, -0.006612428, -0.9311606, 0.93038...   \n",
       "1     [[-0.3025114, 0.803383, 0.88826, 0.36154857, 1...   \n",
       "2     [[-0.08092001, 0.23716491, 0.7449493, 0.191466...   \n",
       "3     [[-0.3025114, 0.803383, 0.88826, 0.36154857, 1...   \n",
       "4     [[-1.8290923, -0.30621898, 0.3005725, 0.443662...   \n",
       "...                                                 ...   \n",
       "2112  [[-0.2716396, 2.832203, 2.2265713, 1.0802964, ...   \n",
       "2113  [[0.24397807, 0.11627979, 0.98766154, -1.33965...   \n",
       "2114  [[0.83727205, 0.588537, -0.8883795, 1.2417848,...   \n",
       "2115  [[0.62694025, 0.13019273, -0.10285008, 1.67548...   \n",
       "2116  [[1.8772068, -1.9619582, 0.12009179, 0.1666371...   \n",
       "\n",
       "                                           Q_Embeddings  \\\n",
       "0     [[0.08589837, 2.4839535, -2.2136166, 3.3471255...   \n",
       "1     [[0.08589837, 2.4839535, -2.2136166, 3.3471255...   \n",
       "2     [[0.08589837, 2.4839535, -2.2136166, 3.3471255...   \n",
       "3     [[0.08589837, 2.4839535, -2.2136166, 3.3471255...   \n",
       "4     [[0.08589837, 2.4839535, -2.2136166, 3.3471255...   \n",
       "...                                                 ...   \n",
       "2112  [[-1.611623, 1.7158104, 0.45376357, 1.2674458,...   \n",
       "2113  [[-3.38445, 1.818213, -0.19103132, -0.26652685...   \n",
       "2114  [[1.906984, 3.2368956, -2.5320153, -2.3044674,...   \n",
       "2115  [[1.906984, 3.2368956, -2.5320153, -2.3044674,...   \n",
       "2116  [[1.906984, 3.2368956, -2.5320153, -2.3044674,...   \n",
       "\n",
       "                                             Doc_Tokens  \\\n",
       "0     [a, partly, submerged, glacier, cave, on, peri...   \n",
       "1     [in, physics, circular, motion, is, a, movemen...   \n",
       "2     [apollo, creed, is, a, fictional, character, f...   \n",
       "3     [in, the, united, states, the, title, of, fede...   \n",
       "4     [the, beretta, 21a, bobcat, is, a, small, pock...   \n",
       "...                                                 ...   \n",
       "2112  [blue, mountain, state, is, an, american, come...   \n",
       "2113  [apple, inc, formerly, apple, computer, inc, i...   \n",
       "2114  [section, 8, housing, in, the, south, bronx, s...   \n",
       "2115  [restaurants, categorized, by, type, and, info...   \n",
       "2116  [u, s, federal, reserve, notes, in, the, mid, ...   \n",
       "\n",
       "                                               Q_Tokens  \\\n",
       "0                    [how, are, glacier, caves, formed]   \n",
       "1     [how, are, the, directions, of, the, velocity,...   \n",
       "2                        [how, did, apollo, creed, die]   \n",
       "3      [how, long, is, the, term, for, federal, judges]   \n",
       "4     [how, a, beretta, model, 21, pistols, magazine...   \n",
       "...                                                 ...   \n",
       "2112    [where, was, blue, mountain, state, filmed, at]   \n",
       "2113              [when, was, apple, computer, founded]   \n",
       "2114                [what, is, section, eight, housing]   \n",
       "2115        [what, is, the, main, type, of, restaurant]   \n",
       "2116           [what, is, us, dollar, worth, based, on]   \n",
       "\n",
       "                                                Doc_POS  \\\n",
       "0     [(a, DT), (partly, RB), (submerged, VBN), (gla...   \n",
       "1     [(in, IN), (physics, NNS), (circular, JJ), (mo...   \n",
       "2     [(apollo, NNS), (creed, VBP), (is, VBZ), (a, D...   \n",
       "3     [(in, IN), (the, DT), (united, JJ), (states, V...   \n",
       "4     [(the, DT), (beretta, NN), (21a, CD), (bobcat,...   \n",
       "...                                                 ...   \n",
       "2112  [(blue, JJ), (mountain, NN), (state, NN), (is,...   \n",
       "2113  [(apple, NN), (inc, VBP), (formerly, RB), (app...   \n",
       "2114  [(section, NN), (8, CD), (housing, NN), (in, I...   \n",
       "2115  [(restaurants, NNS), (categorized, VBN), (by, ...   \n",
       "2116  [(u, JJ), (s, JJ), (federal, JJ), (reserve, NN...   \n",
       "\n",
       "                                                  Q_POS  \\\n",
       "0     [(how, WRB), (are, VBP), (glacier, JJ), (caves...   \n",
       "1     [(how, WRB), (are, VBP), (the, DT), (direction...   \n",
       "2     [(how, WRB), (did, VBD), (apollo, VB), (creed,...   \n",
       "3     [(how, WRB), (long, JJ), (is, VBZ), (the, DT),...   \n",
       "4     [(how, WRB), (a, DT), (beretta, NN), (model, N...   \n",
       "...                                                 ...   \n",
       "2112  [(where, WRB), (was, VBD), (blue, JJ), (mounta...   \n",
       "2113  [(when, WRB), (was, VBD), (apple, NN), (comput...   \n",
       "2114  [(what, WP), (is, VBZ), (section, NN), (eight,...   \n",
       "2115  [(what, WP), (is, VBZ), (the, DT), (main, JJ),...   \n",
       "2116  [(what, WP), (is, VBZ), (us, PRP), (dollar, NN...   \n",
       "\n",
       "                                                Doc_NER  \\\n",
       "0     [(a, ), (partly, ), (submerged, ), (glacier, )...   \n",
       "1     [(in, ), (physics, ), (circular, ), (motion, )...   \n",
       "2     [(apollo, ORG), (creed, ), (is, ), (a, ), (fic...   \n",
       "3     [(in, ), (the, GPE), (united, GPE), (states, G...   \n",
       "4     [(the, ), (beretta, ), (21a, ), (bobcat, ), (i...   \n",
       "...                                                 ...   \n",
       "2112  [(blue, LOC), (mountain, LOC), (state, ), (is,...   \n",
       "2113  [(apple, ORG), (inc, ORG), (formerly, ORG), (a...   \n",
       "2114  [(section, LAW), (8, LAW), (housing, ), (in, )...   \n",
       "2115  [(restaurants, ), (categorized, ), (by, ), (ty...   \n",
       "2116  [(u, ), (s, ORG), (federal, ORG), (reserve, OR...   \n",
       "\n",
       "                                                  Q_NER  \\\n",
       "0     [(how, ), (are, ), (glacier, ), (caves, ), (fo...   \n",
       "1     [(how, ), (are, ), (the, ), (directions, ), (o...   \n",
       "2     [(how, ), (did, ), (apollo, ORG), (creed, ), (...   \n",
       "3     [(how, ), (long, ), (is, ), (the, ), (term, ),...   \n",
       "4     [(how, ), (a, ), (beretta, PRODUCT), (model, )...   \n",
       "...                                                 ...   \n",
       "2112  [(where, ), (was, ), (blue, ), (mountain, ), (...   \n",
       "2113  [(when, ), (was, ), (apple, ), (computer, ), (...   \n",
       "2114  [(what, ), (is, ), (section, ), (eight, CARDIN...   \n",
       "2115  [(what, ), (is, ), (the, ), (main, ), (type, )...   \n",
       "2116  [(what, ), (is, ), (us, ), (dollar, ), (worth,...   \n",
       "\n",
       "                                              Doc_TFIDF  \\\n",
       "0     [0.24677746860673927, 0.1451147752938821, 0.16...   \n",
       "1     [0.10157954319378303, 0.047977208560696934, 0....   \n",
       "2     [0.15061458188394025, 0.28752101617726733, 0.0...   \n",
       "3     [0.08828641778817685, 0.32044981985432075, 0.1...   \n",
       "4     [0.21160137561032594, 0.8290055086215778, 0.22...   \n",
       "...                                                 ...   \n",
       "2112  [0.29475406599874054, 0.3823486411848812, 0.22...   \n",
       "2113  [0.2615737575545419, 0.08943278683987749, 0.02...   \n",
       "2114  [0.19207290739926391, 0.16089672153840806, 0.2...   \n",
       "2115  [0.876941110711729, 0.9254918922898989, 0.4042...   \n",
       "2116  [0.2686013488464822, 0.19929661266768658, 0.30...   \n",
       "\n",
       "                                                Q_TFIDF  \n",
       "0     [1.034209132108837, 0.8495245060136498, 1.9896...  \n",
       "1     [0.34473637736961227, 0.28317483533788324, 0.4...  \n",
       "2     [1.034209132108837, 1.1532031921635457, 1.7872...  \n",
       "3     [0.646380707568023, 0.7777965845277852, 0.4296...  \n",
       "4     [0.646380707568023, 0.4549959577436755, 1.2435...  \n",
       "...                                                 ...  \n",
       "2112  [0.7503059363146255, 0.5906688741268076, 1.063...  \n",
       "2113  [0.9915267033360398, 0.8269364237775307, 1.700...  \n",
       "2114  [0.8426216022668568, 0.6874871616619922, 1.514...  \n",
       "2115  [0.6018725730477548, 0.49106225832999434, 0.48...  \n",
       "2116  [0.6018725730477548, 0.49106225832999434, 0.88...  \n",
       "\n",
       "[2117 rows x 12 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc_ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60b20141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vectorize(\n",
    "    pos_tagger, ner_tagger, data\n",
    "):  # pass in the unique dict for ner or pos\n",
    "    pos_idx = pos_tagger.values()\n",
    "    pos_ohv = np.eye(max(pos_idx) + 1)  # create the ohv\n",
    "    ner_idx = ner_tagger.values()\n",
    "    ner_ohv = np.eye(max(ner_idx) + 1)\n",
    "\n",
    "    dpos_full_ohv, dner_full_ohv = [], []  # lists to append to\n",
    "    qpos_full_ohv, qner_full_ohv = [], []  # lists to append to\n",
    "\n",
    "    for item in data[\"Doc_POS\"]:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            pos_index_iden = pos_tagger[tag]\n",
    "            sent_ohv.append(pos_ohv[pos_index_iden])\n",
    "        dpos_full_ohv.append(sent_ohv)\n",
    "\n",
    "    for item in data[\"Q_POS\"]:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            pos_index_iden = pos_tagger[tag]\n",
    "            sent_ohv.append(pos_ohv[pos_index_iden])\n",
    "        qpos_full_ohv.append(sent_ohv)\n",
    "\n",
    "    for item in data[\"Doc_NER\"]:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            ner_index_iden = ner_tagger[tag]\n",
    "            sent_ohv.append(ner_ohv[ner_index_iden])\n",
    "        dner_full_ohv.append(sent_ohv)\n",
    "\n",
    "    for item in data[\"Q_NER\"]:\n",
    "        sent_ohv = []\n",
    "        for word in item:\n",
    "            tag = word[1]\n",
    "            ner_index_iden = ner_tagger[tag]\n",
    "            sent_ohv.append(ner_ohv[ner_index_iden])\n",
    "        qner_full_ohv.append(sent_ohv)\n",
    "\n",
    "    return (dpos_full_ohv, qpos_full_ohv, dner_full_ohv, qner_full_ohv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c766e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ohv for doc\n",
    "(\n",
    "    train_doc_pos_ohv,\n",
    "    train_q_pos_ohv,\n",
    "    train_doc_ner_ohv,\n",
    "    train_q_ner_ohv,\n",
    ") = one_hot_vectorize(pos_iden, ner_iden, train_doc_ques)\n",
    "test_doc_pos_ohv, test_q_pos_ohv, test_doc_ner_ohv, test_q_ner_ohv = one_hot_vectorize(\n",
    "    pos_iden, ner_iden, test_doc_ques\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68e412d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dataframe to just tokens and embeddings:\n",
    "doc_emb_train = train_doc_ques[[\"Doc_Tokens\", \"Doc_Embeddings\", \"Doc_TFIDF\"]]\n",
    "doc_pos_ner = pd.DataFrame({\"Doc_POS\": train_doc_pos_ohv, \"Doc_NER\": train_doc_ner_ohv})\n",
    "doc_emb_train = pd.concat([doc_emb_train, doc_pos_ner], axis=1)\n",
    "\n",
    "q_emb_train = train_doc_ques[[\"Q_Tokens\", \"Q_Embeddings\", \"Q_TFIDF\"]]\n",
    "q_pos_ner = pd.DataFrame({\"Q_POS\": train_q_pos_ohv, \"Q_NER\": train_q_ner_ohv})\n",
    "q_emb_train = pd.concat([q_emb_train, q_pos_ner], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d10ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb_test = test_doc_ques[[\"Doc_Tokens\", \"Doc_Embeddings\", \"Doc_TFIDF\"]]\n",
    "doc_pos_ner = pd.DataFrame({\"Doc_POS\": test_doc_pos_ohv, \"Doc_NER\": test_doc_ner_ohv})\n",
    "doc_emb_test = pd.concat([doc_emb_test, doc_pos_ner], axis=1)\n",
    "\n",
    "q_emb_test = test_doc_ques[[\"Q_Tokens\", \"Q_Embeddings\", \"Q_TFIDF\"]]\n",
    "q_pos_ner = pd.DataFrame({\"Q_POS\": test_q_pos_ohv, \"Q_NER\": test_q_ner_ohv})\n",
    "q_emb_test = pd.concat([q_emb_test, q_pos_ner], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10ad89b3",
   "metadata": {},
   "source": [
    "### Word Embeddings (Doc and Qn)\n",
    "\n",
    "The embeddings of the questions and answers of the train and test set can be found here:\n",
    "\n",
    "-   Train Document - doc_emb_train\n",
    "-   Train Q - q_emb_train\n",
    "-   Test Document - doc_emb_test\n",
    "-   Test Q - q_emb_test\n",
    "\n",
    "The max_document size is 1675 and max_question size is 23.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c75c753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_Tokens</th>\n",
       "      <th>Doc_Embeddings</th>\n",
       "      <th>Doc_TFIDF</th>\n",
       "      <th>Doc_POS</th>\n",
       "      <th>Doc_NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[a, partly, submerged, glacier, cave, on, peri...</td>\n",
       "      <td>[[0.6947981, -0.006612428, -0.9311606, 0.93038...</td>\n",
       "      <td>[0.24677746860673927, 0.1451147752938821, 0.16...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[in, physics, circular, motion, is, a, movemen...</td>\n",
       "      <td>[[-0.3025114, 0.803383, 0.88826, 0.36154857, 1...</td>\n",
       "      <td>[0.10157954319378303, 0.047977208560696934, 0....</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[apollo, creed, is, a, fictional, character, f...</td>\n",
       "      <td>[[-0.08092001, 0.23716491, 0.7449493, 0.191466...</td>\n",
       "      <td>[0.15061458188394025, 0.28752101617726733, 0.0...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[in, the, united, states, the, title, of, fede...</td>\n",
       "      <td>[[-0.3025114, 0.803383, 0.88826, 0.36154857, 1...</td>\n",
       "      <td>[0.08828641778817685, 0.32044981985432075, 0.1...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[the, beretta, 21a, bobcat, is, a, small, pock...</td>\n",
       "      <td>[[-1.8290923, -0.30621898, 0.3005725, 0.443662...</td>\n",
       "      <td>[0.21160137561032594, 0.8290055086215778, 0.22...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>[blue, mountain, state, is, an, american, come...</td>\n",
       "      <td>[[-0.2716396, 2.832203, 2.2265713, 1.0802964, ...</td>\n",
       "      <td>[0.29475406599874054, 0.3823486411848812, 0.22...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>[apple, inc, formerly, apple, computer, inc, i...</td>\n",
       "      <td>[[0.24397807, 0.11627979, 0.98766154, -1.33965...</td>\n",
       "      <td>[0.2615737575545419, 0.08943278683987749, 0.02...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>[section, 8, housing, in, the, south, bronx, s...</td>\n",
       "      <td>[[0.83727205, 0.588537, -0.8883795, 1.2417848,...</td>\n",
       "      <td>[0.19207290739926391, 0.16089672153840806, 0.2...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>[restaurants, categorized, by, type, and, info...</td>\n",
       "      <td>[[0.62694025, 0.13019273, -0.10285008, 1.67548...</td>\n",
       "      <td>[0.876941110711729, 0.9254918922898989, 0.4042...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>[u, s, federal, reserve, notes, in, the, mid, ...</td>\n",
       "      <td>[[1.8772068, -1.9619582, 0.12009179, 0.1666371...</td>\n",
       "      <td>[0.2686013488464822, 0.19929661266768658, 0.30...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2117 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Doc_Tokens  \\\n",
       "0     [a, partly, submerged, glacier, cave, on, peri...   \n",
       "1     [in, physics, circular, motion, is, a, movemen...   \n",
       "2     [apollo, creed, is, a, fictional, character, f...   \n",
       "3     [in, the, united, states, the, title, of, fede...   \n",
       "4     [the, beretta, 21a, bobcat, is, a, small, pock...   \n",
       "...                                                 ...   \n",
       "2112  [blue, mountain, state, is, an, american, come...   \n",
       "2113  [apple, inc, formerly, apple, computer, inc, i...   \n",
       "2114  [section, 8, housing, in, the, south, bronx, s...   \n",
       "2115  [restaurants, categorized, by, type, and, info...   \n",
       "2116  [u, s, federal, reserve, notes, in, the, mid, ...   \n",
       "\n",
       "                                         Doc_Embeddings  \\\n",
       "0     [[0.6947981, -0.006612428, -0.9311606, 0.93038...   \n",
       "1     [[-0.3025114, 0.803383, 0.88826, 0.36154857, 1...   \n",
       "2     [[-0.08092001, 0.23716491, 0.7449493, 0.191466...   \n",
       "3     [[-0.3025114, 0.803383, 0.88826, 0.36154857, 1...   \n",
       "4     [[-1.8290923, -0.30621898, 0.3005725, 0.443662...   \n",
       "...                                                 ...   \n",
       "2112  [[-0.2716396, 2.832203, 2.2265713, 1.0802964, ...   \n",
       "2113  [[0.24397807, 0.11627979, 0.98766154, -1.33965...   \n",
       "2114  [[0.83727205, 0.588537, -0.8883795, 1.2417848,...   \n",
       "2115  [[0.62694025, 0.13019273, -0.10285008, 1.67548...   \n",
       "2116  [[1.8772068, -1.9619582, 0.12009179, 0.1666371...   \n",
       "\n",
       "                                              Doc_TFIDF  \\\n",
       "0     [0.24677746860673927, 0.1451147752938821, 0.16...   \n",
       "1     [0.10157954319378303, 0.047977208560696934, 0....   \n",
       "2     [0.15061458188394025, 0.28752101617726733, 0.0...   \n",
       "3     [0.08828641778817685, 0.32044981985432075, 0.1...   \n",
       "4     [0.21160137561032594, 0.8290055086215778, 0.22...   \n",
       "...                                                 ...   \n",
       "2112  [0.29475406599874054, 0.3823486411848812, 0.22...   \n",
       "2113  [0.2615737575545419, 0.08943278683987749, 0.02...   \n",
       "2114  [0.19207290739926391, 0.16089672153840806, 0.2...   \n",
       "2115  [0.876941110711729, 0.9254918922898989, 0.4042...   \n",
       "2116  [0.2686013488464822, 0.19929661266768658, 0.30...   \n",
       "\n",
       "                                                Doc_POS  \\\n",
       "0     [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...   \n",
       "2     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...   \n",
       "4     [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "...                                                 ...   \n",
       "2112  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...   \n",
       "2113  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2114  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2115  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2116  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...   \n",
       "\n",
       "                                                Doc_NER  \n",
       "0     [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1     [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3     [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4     [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "...                                                 ...  \n",
       "2112  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...  \n",
       "2113  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2114  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...  \n",
       "2115  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2116  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "\n",
       "[2117 rows x 5 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "350a724c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_emb_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bc/5zby_td12xvblch941x43zl40000gn/T/ipykernel_49488/3632861856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc_emb_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Doc_NER'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_emb_train' is not defined"
     ]
    }
   ],
   "source": [
    "doc_emb_train['Doc_NER'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c7aa57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_array(data, data_type=\"Document\"):\n",
    "    num_vec_length = 156\n",
    "    max_doc = 1675\n",
    "    max_qn = 23\n",
    "    zero_vec = np.zeros(156)\n",
    "\n",
    "    if data_type == \"Document\":\n",
    "        full_vec = []  # create a list for list of list for document\n",
    "        for dat in range(len(data)):  # go through each line\n",
    "            doc_ques = data.loc[dat]  # document data\n",
    "            v = []  # create list to each word\n",
    "            for j in range(len(doc_ques.iloc[0])):\n",
    "                vn = []  # list of concat word embeddings\n",
    "                vn.append(doc_ques.iloc[1][j].tolist()) #Word2Vec\n",
    "                vn.append(doc_ques.iloc[2][j]) # TF-IDF\n",
    "                vn.append(doc_ques.iloc[3][j].tolist()) # POS\n",
    "                vn.append(doc_ques.iloc[4][j].tolist()) # NER\n",
    "                flatten = [\n",
    "                    item\n",
    "                    for sublist in vn\n",
    "                    for item in (sublist if isinstance(sublist, list) else [sublist])\n",
    "                ]\n",
    "                v.append(flatten)\n",
    "            while len(v) < max_doc:\n",
    "                v.append(zero_vec)\n",
    "            full_vec.append(v)\n",
    "\n",
    "    if data_type == \"Question\":\n",
    "        full_vec = []  # create a list for list of list for document\n",
    "        for dat in range(len(data)):  # go through each line\n",
    "            doc_ques = data.loc[dat]  # document data\n",
    "            v = []  # create list to each word\n",
    "            for j in range(len(doc_ques.iloc[0])):\n",
    "                vn = []  # list of concat word embeddings\n",
    "                vn.append(doc_ques.iloc[1][j].tolist()) #Word2Vec\n",
    "                vn.append(doc_ques.iloc[2][j]) # TF-IDF\n",
    "                vn.append(doc_ques.iloc[3][j].tolist()) #POS\n",
    "                vn.append(doc_ques.iloc[4][j].tolist()) #NER\n",
    "                flatten = [\n",
    "                    item\n",
    "                    for sublist in vn\n",
    "                    for item in (sublist if isinstance(sublist, list) else [sublist])\n",
    "                ]\n",
    "                v.append(flatten)\n",
    "            while len(v) < max_qn:\n",
    "                v.append(zero_vec)\n",
    "            full_vec.append(v)\n",
    "    return full_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0088de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Test Documents to pass in, takes about a min\n",
    "final_doc_train = np.stack(full_array(doc_emb_train, data_type=\"Document\"))\n",
    "final_doc_test = np.stack(full_array(doc_emb_test, data_type=\"Document\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dbb830b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Test Questions to pass in, takes about a few seconds\n",
    "final_qn_train = np.stack(full_array(q_emb_train, data_type=\"Question\"))\n",
    "final_qn_test = np.stack(full_array(q_emb_test, data_type=\"Question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cdf9ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(labels):\n",
    "    check = []\n",
    "    for i in labels:\n",
    "        if len(i) < 1675:\n",
    "            while len(i) < 1675:\n",
    "                i.append('N')\n",
    "            check.append(i)\n",
    "        else:\n",
    "            check.append(i)\n",
    "    return check\n",
    "tr_labels = np.array(convert_labels(train_doc_ans_labels))\n",
    "ts_labels = np.array(convert_labels(test_doc_ans_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "654e151a",
   "metadata": {},
   "source": [
    "Earlier, we found that the sequence length of the documents can get quite large, which might introduce a lot of noise into the model with paddings. One alternative to reduce this noise is to perhaps truncate the sequences down to just over the median for the documents. We also need to do this for the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55f86b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Max Seq Length is 1675, Median is 181.0, Number of lines is 630)',\n",
       " 'Max Seq Length is 924, Median is 182.0, Number of lines is 2117)',\n",
       " 'Max Seq Length is 19, Median is 7.0, Number of lines is 630)',\n",
       " 'Max Seq Length is 23, Median is 7.0, Number of lines is 2117)')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_max_length(test_doc_ques[\"Doc_Embeddings\"]),find_max_length(train_doc_ques[\"Doc_Embeddings\"]), find_max_length(test_doc_ques[\"Q_Embeddings\"]), find_max_length(train_doc_ques[\"Q_Embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7c5e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the embeddings and the labels to 200\n",
    "\n",
    "def truncate(data, labels, max_seq=200):\n",
    "    data = data[:, :max_seq, :]\n",
    "    labels = labels[:, :max_seq]\n",
    "    return data, labels\n",
    "\n",
    "final_doc_train, tr_labels = truncate(final_doc_train, tr_labels)\n",
    "final_doc_test, ts_labels = truncate(final_doc_test, ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d926af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final prepared documents are found here:\n",
    "\n",
    "#final_doc_train, tr_labels - doc embeddings and output labels for training\n",
    "#final_doc_test, ts_labels - doc embeddings and output labels for testing\n",
    "#final_qn_train - question embeddings for training\n",
    "#final_qn_test - question embeddings for testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e27e9e3d",
   "metadata": {},
   "source": [
    "### Converting into Tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db6375d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df61114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a min\n",
    "tf_final_doc_train = torch.tensor(final_doc_train, device=device).to(dtype=torch.float32)\n",
    "tf_final_doc_test = torch.tensor(final_doc_test, device=device).to(dtype=torch.float32)\n",
    "tf_final_qn_train = torch.tensor(final_qn_train, device=device).to(dtype=torch.float32)\n",
    "tf_final_qn_test = torch.tensor(final_qn_test, device=device).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9dba3fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2117, 200, 156])\n",
      "torch.Size([630, 200, 156])\n",
      "torch.Size([2117, 23, 156])\n",
      "torch.Size([630, 23, 156])\n"
     ]
    }
   ],
   "source": [
    "# check dimensions\n",
    "print(tf_final_doc_train.shape)\n",
    "print(tf_final_doc_test.shape)\n",
    "print(tf_final_qn_train.shape)\n",
    "print(tf_final_qn_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a96e0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tensors into wd\n",
    "torch.save(tf_final_doc_train, 'tensor.doc_train')\n",
    "torch.save(tf_final_doc_test, 'tensor.doc_test')\n",
    "torch.save(tf_final_qn_train, 'tensor.qn_train')\n",
    "torch.save(tf_final_qn_test, 'tensor.qn_test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "123bb0ea",
   "metadata": {},
   "source": [
    "**Input Embedding Ablation Study**\n",
    "\n",
    "In the model input embedding Ablation study, we are given 3 variations of input embeddings to test. We will test 3 options:\n",
    "\n",
    "1. Word2Vec only # 100 dims\n",
    "2. Word2Vec + Tf-IDF # 101 dims\n",
    "3. Word2Vec + all features (TF-IDF, POS, NER) # 156 dims\n",
    "\n",
    "Since we are using tensors, we can use tensor slicing to take out the relevant features.\n",
    "Our tensor of embeddings are built as follows (w2v, TF-IDF, POS, NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e23b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensors(tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test, option=3):\n",
    "    if option == 3:\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test\n",
    "    elif option == 1:\n",
    "        tf_doc_train = tf_doc_train[:, :, :100]\n",
    "        tf_doc_test = tf_doc_test[:, :, :100]\n",
    "        tf_qn_train = tf_qn_train[:, :, :100]\n",
    "        tf_qn_test = tf_qn_test[:, :, :100]\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test\n",
    "    elif option == 2:\n",
    "        tf_doc_train = tf_doc_train[:, :, :101]\n",
    "        tf_doc_test = tf_doc_test[:, :, :101]\n",
    "        tf_qn_train = tf_qn_train[:, :, :101]\n",
    "        tf_qn_test = tf_qn_test[:, :, :101]\n",
    "        return tf_doc_train, tf_doc_test, tf_qn_train, tf_qn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0a70458a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2117, 200, 101])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change option to see size\n",
    "(convert_tensors(tf_final_doc_train, tf_final_doc_test, tf_final_qn_train, tf_final_qn_test, 2)[0]).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac2f6e40",
   "metadata": {},
   "source": [
    "Our answer should perhaps also be in the form of (1x1675) list containing ['N','S','I','E']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8538510",
   "metadata": {},
   "source": [
    "Additionally, the labels should be one hot vectorised, as they are cateogorical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a0a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tr_labels.npy', tr_labels)\n",
    "np.save('ts_labels.npy', ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee50025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_labels(labels):\n",
    "    # Create a dictionary that maps each label to a unique integer\n",
    "    label_to_int = {'N': 0, 'S': 1, 'I': 2, 'E': 3}\n",
    "\n",
    "    # Map the labels to integers\n",
    "    int_labels = [[label_to_int[label] for label in sequence] for sequence in labels]\n",
    "\n",
    "    # Create an identity matrix of size 4 (since there are 4 labels)\n",
    "    identity = np.eye(4)\n",
    "\n",
    "    # Use the integer labels as indices to select rows from the identity matrix\n",
    "    one_hot_labels = [identity[sequence] for sequence in int_labels]\n",
    "    return one_hot_labels\n",
    "\n",
    "tr_encoded = one_hot_encode_labels(tr_labels)\n",
    "tst_encoded = one_hot_encode_labels(ts_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebb0b7a7",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just testing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Bi-LSTM for Document Portion\n",
    "\n",
    "class Document_LSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, ) \n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.lstm(embedded, hidden) \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)    \n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
    "    ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
    "\n",
    "    def __init__(self, hidden_size, output_size, embedding, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "\n",
    "    def cal_attention(self, hidden, encoder_hiddens, method):\n",
    "        # Dot Product Attention\n",
    "        if method == AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT:\n",
    "            attn_weights = F.softmax(torch.bmm(hidden, encoder_hiddens.T.unsqueeze(0)),dim=-1)\n",
    "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0))\n",
    "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
    "        # Scaled Dot Product Attention\n",
    "        elif method == AttnDecoderRNN.ATTN_TYPE_SCALE_DOT_PRODUCT:\n",
    "            attn_weights = F.softmax(torch.bmm(hidden, encoder_hiddens.T.unsqueeze(0))/np.sqrt(hidden_size),dim=-1)\n",
    "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0))\n",
    "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from typing import Literal, Dict, Type, Union\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class NNType(Enum):\n",
    "    RNN = \"rnn\"\n",
    "    LSTM = \"lstm\"\n",
    "    GRU = \"gru\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "NN_MAP: Dict[NNType, Type[Union[nn.RNN, nn.LSTM, nn.GRU]]] = {\n",
    "    NNType.RNN: nn.RNN,\n",
    "    NNType.LSTM: nn.LSTM,\n",
    "    NNType.GRU: nn.GRU,\n",
    "}\n",
    "\n",
    "\n",
    "class EncoderBiRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        embedding: nn.Embedding,\n",
    "        nn_type: Literal[\"rnn\", \"lstm\", \"gru\"] = \"rnn\",\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super(EncoderBiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.nn_type = NNType(nn_type)\n",
    "        self.nn = NN_MAP[self.nn_type](\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: nn.Embedding, hidden: Tensor):\n",
    "        embedded: Tensor = self.embedding(input).view(1, 1, -1)\n",
    "        output: Tensor\n",
    "        output, hidden = self.nn(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (\n",
    "            torch.zeros(2, 1, self.hidden_size, device=device)\n",
    "            if self.nn_type != NNType.LSTM\n",
    "            else (\n",
    "                torch.zeros(2, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(2, 1, self.hidden_size, device=device),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AttentionMethod(Enum):\n",
    "    DOT_PRODUCT = \"dot_product\"\n",
    "    SCALE_DOT_PRODUCT = \"scale_dot_product\"\n",
    "    COSINE_SIMILARITY = \"cosine_similarity\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class DecoderBiRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        embedding: nn.Embedding,\n",
    "        max_length: int,\n",
    "        nn_type: Literal[\"rnn\", \"lstm\", \"gru\"] = \"rnn\",\n",
    "        num_layers=1,\n",
    "        dropout_p=0.1,\n",
    "        attention_method: Literal[\n",
    "            \"dot_product\",\n",
    "            \"scale_dot_product\",\n",
    "            \"cosine_similarity\",\n",
    "        ] = \"dot_product\",\n",
    "    ):\n",
    "        super(DecoderBiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.embedding = embedding\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.nn_type = NNType(nn_type)\n",
    "        self.attention_method = AttentionMethod(attention_method)\n",
    "        self.nn = NN_MAP[self.nn_type](\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.out = nn.Linear(self.hidden_size * 4, self.output_size)\n",
    "\n",
    "    def cal_attention(self, hidden: Tensor, encoder_hiddens: Tensor):\n",
    "        if self.attention_method == AttentionMethod.DOT_PRODUCT:\n",
    "            if self.nn_type == NNType.LSTM:  # For BiLSTM\n",
    "                energy = torch.bmm(hidden[0], encoder_hiddens.T.repeat(2, 1, 1))\n",
    "                attn_weights = F.softmax(energy, dim=-1)\n",
    "                attn_output = torch.bmm(attn_weights, encoder_hiddens.repeat(2, 1, 1))\n",
    "                concat_output = torch.cat(\n",
    "                    (attn_output[0], hidden[0][0], attn_output[1], hidden[0][1]), 1\n",
    "                )\n",
    "            else:  # For BiRNN & BiGRU\n",
    "                energy = torch.bmm(hidden, encoder_hiddens.T.repeat(2, 1, 1))\n",
    "                attn_weights = F.softmax(energy, dim=-1)\n",
    "                attn_output = torch.bmm(attn_weights, encoder_hiddens.repeat(2, 1, 1))\n",
    "                concat_output = torch.cat(\n",
    "                    (attn_output[0], hidden[0], attn_output[1], hidden[1]), 1\n",
    "                )\n",
    "\n",
    "        elif self.attention_method == AttentionMethod.COSINE_SIMILARITY:\n",
    "            if self.nn_type == NNType.LSTM:  # For LSTM\n",
    "                cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
    "                h_n, c_n = hidden\n",
    "                # h_n_reshaped = h_n.mean(dim=0, keepdim=True)\n",
    "                attn_weights_f = F.softmax(\n",
    "                    cosine_similarity(h_n[0].unsqueeze(0), encoder_hiddens), dim=-1\n",
    "                )\n",
    "                attn_output_f = torch.bmm(\n",
    "                    attn_weights_f.unsqueeze(0), encoder_hiddens.unsqueeze(0)\n",
    "                )\n",
    "                attn_weights_b = F.softmax(\n",
    "                    cosine_similarity(h_n[1].unsqueeze(0), encoder_hiddens), dim=-1\n",
    "                )\n",
    "                attn_output_b = torch.bmm(\n",
    "                    attn_weights_b.unsqueeze(0), encoder_hiddens.unsqueeze(0)\n",
    "                )\n",
    "                concat_output = torch.cat(\n",
    "                    (\n",
    "                        attn_output_f[0],\n",
    "                        h_n[0],\n",
    "                        attn_output_b[0],\n",
    "                        h_n[1],\n",
    "                    ),\n",
    "                    1,\n",
    "                )\n",
    "\n",
    "            else:  # For RNN & GRU\n",
    "                cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
    "                # hidden_reshaped = hidden.mean(dim=0, keepdim=True)\n",
    "                # print(hidden_reshaped.shape)\n",
    "                attn_weights_f = F.softmax(\n",
    "                    cosine_similarity(hidden[0].unsqueeze(0), encoder_hiddens), dim=-1\n",
    "                )\n",
    "                attn_output_f = torch.bmm(\n",
    "                    attn_weights_f.unsqueeze(0), encoder_hiddens.unsqueeze(0)\n",
    "                )\n",
    "                attn_weights_b = F.softmax(\n",
    "                    cosine_similarity(hidden[1].unsqueeze(0), encoder_hiddens), dim=-1\n",
    "                )\n",
    "                attn_output_b = torch.bmm(\n",
    "                    attn_weights_b.unsqueeze(0), encoder_hiddens.unsqueeze(0)\n",
    "                )\n",
    "                concat_output = torch.cat(\n",
    "                    (\n",
    "                        attn_output_f[0],\n",
    "                        hidden[0],\n",
    "                        attn_output_b[0],\n",
    "                        hidden[1],\n",
    "                    ),\n",
    "                    1,\n",
    "                )\n",
    "        else:\n",
    "            if self.nn_type == NNType.LSTM:  # For LSTM\n",
    "                energy = torch.bmm(\n",
    "                    hidden[0], encoder_hiddens.T.repeat(2, 1, 1)\n",
    "                ) / np.sqrt(self.hidden_size)\n",
    "                attn_weights = F.softmax(energy, dim=-1)\n",
    "                attn_output = torch.bmm(attn_weights, encoder_hiddens.repeat(2, 1, 1))\n",
    "                concat_output = torch.cat(\n",
    "                    (attn_output[0], hidden[0][0], attn_output[1], hidden[0][1]), 1\n",
    "                )\n",
    "            else:  # For RNN & GRU\n",
    "                energy = torch.bmm(hidden, encoder_hiddens.T.repeat(2, 1, 1)) / np.sqrt(\n",
    "                    self.hidden_size\n",
    "                )\n",
    "                attn_weights = F.softmax(energy, dim=-1)\n",
    "                attn_output = torch.bmm(attn_weights, encoder_hiddens.repeat(2, 1, 1))\n",
    "                concat_output = torch.cat(\n",
    "                    (attn_output[0], hidden[0], attn_output[1], hidden[1]), 1\n",
    "                )\n",
    "        return concat_output\n",
    "\n",
    "    def forward(self, input: nn.Embedding, hidden: Tensor, encoder_hiddens: Tensor):\n",
    "        embedded: Tensor = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        _, hidden = self.nn(embedded, hidden)\n",
    "        concat_output = self.cal_attention(hidden, encoder_hiddens)\n",
    "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (\n",
    "            torch.zeros(2, 1, self.hidden_size, device=device)\n",
    "            if self.nn_type != NNType.LSTM\n",
    "            else (\n",
    "                torch.zeros(2, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(2, 1, self.hidden_size, device=device),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e53da45d",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9274dd",
   "metadata": {
    "tags": [
     "parameters",
     "Max doc length"
    ]
   },
   "outputs": [],
   "source": [
    "MAX_DOC_LENGTH = 1675  # Max doc length\n",
    "MAX_QN_LENGTH = 23  # Max question length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e43c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(\n",
    "    input_tensor,\n",
    "    target_tensor,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    criterion,\n",
    "):\n",
    "    # Initialize the hidden state of the encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # Set the gradients of the optimizers to zero\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Get the length of the input and target tensors\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # Initialize the loss to zero\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over the length of the input tensor\n",
    "    for ei in range(input_length):\n",
    "        # Pass each element of the input tensor through the encoder\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    # Set the initial input to the decoder as the first element of the target tensor\n",
    "    decoder_input = target_tensor[0]\n",
    "    # Set the initial hidden state of the decoder as the final hidden state of the encoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Iterate over the length of the target tensor\n",
    "    for di in range(target_length):\n",
    "        # Pass each element of the target tensor through the decoder\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        # Calculate and accumulate the loss by comparing the output of the decoder to the target tensor\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        # Set the next input to the decoder as the current element of the target tensor\n",
    "        decoder_input = target_tensor[di]\n",
    "\n",
    "    # Compute gradients using backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # Return average loss per element in target sequence\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4152800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    input_tensor,\n",
    "    target_tensor,\n",
    "    documentRNN,\n",
    "    questionRNN,\n",
    "    documentRNN_optimizer,\n",
    "    questionRNN_optimizer,\n",
    "    criterion,\n",
    "    max_doc_length=MAX_DOC_LENGTH,\n",
    "    max_qn_length=MAX_QN_LENGTH,\n",
    "    nn_type=\"rnn\",\n",
    "):\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # it is for storing the hidden states of input sequence later, which will be used for calculating the attention during the decoding process\n",
    "    documentRNN_hiddens = torch.zeros(\n",
    "        max_doc_length, documentRNN.hidden_size * 2, device=device\n",
    "    )\n",
    "\n",
    "    # zero-initialize an initial hidden state\n",
    "    documentRNN_hidden = documentRNN.initHidden()\n",
    "    questionRNN_hidden = questionRNN.initHidden()\n",
    "    loss = 0\n",
    "    documentRNN_optimizer.zero_grad()\n",
    "    questionRNN_optimizer.zero_grad()\n",
    "\n",
    "    # Feed the input_tensor into the encoder we defined\n",
    "    for i in range(input_length):\n",
    "        documentRNN_output, documentRNN_hidden = documentRNN(\n",
    "            input_tensor[i], documentRNN_hidden\n",
    "        )\n",
    "        documentRNN_hiddens[i] = (\n",
    "            documentRNN_hidden[0][0, 0]\n",
    "            if nn_type == \"lstm\"\n",
    "            else documentRNN_hidden[0, 0]\n",
    "        )\n",
    "\n",
    "    # Set the initial input to the decoder as the first element of the target tensor\n",
    "    questionRNN_input = target_tensor[0]\n",
    "\n",
    "    # Teacher forcing: Feed the target as the next input\n",
    "    for i in range(target_length):\n",
    "        questionRNN_output, questionRNN_hidden = questionRNN(\n",
    "            questionRNN_input, questionRNN_hidden, documentRNN_hiddens\n",
    "        )\n",
    "        loss += criterion(questionRNN_output, target_tensor[i])\n",
    "        questionRNN_input = target_tensor[i]\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    documentRNN_optimizer.step()\n",
    "    questionRNN_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82256bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "# Helper functions for training\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (- %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "def trainIters(\n",
    "    documentRNN,\n",
    "    questionRNN,\n",
    "    n_iters,\n",
    "    print_every=1000,\n",
    "    plot_every=100,\n",
    "    learning_rate=0.01,\n",
    "    nn_type=\"rnn\",\n",
    "):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    documentRNN_optimizer = optim.AdamW(documentRNN.parameters(), lr=learning_rate)\n",
    "    questionRNN_optimizer = optim.AdamW(questionRNN.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        random_choice_ix = random.choice(\n",
    "            range(n_data)\n",
    "        )  # Get a random index within the scope of input data\n",
    "        input_index_r = [[ind] for ind in input_index[random_choice_ix]]\n",
    "        target_index_r = [[ind] for ind in target_index[random_choice_ix]]\n",
    "\n",
    "        input_tensor = torch.LongTensor(input_index_r).to(device)\n",
    "        target_tensor = torch.LongTensor(target_index_r).to(device)\n",
    "\n",
    "        loss = train(\n",
    "            input_tensor,\n",
    "            target_tensor,\n",
    "            documentRNN,\n",
    "            questionRNN,\n",
    "            documentRNN_optimizer,\n",
    "            questionRNN_optimizer,\n",
    "            criterion,\n",
    "            nn_type=nn_type,\n",
    "        )\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(\n",
    "                \"%s (%d %d%%) %.4f\"\n",
    "                % (\n",
    "                    timeSince(start, iter / n_iters),\n",
    "                    iter,\n",
    "                    iter / n_iters * 100,\n",
    "                    print_loss_avg,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0452a65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bc/5zby_td12xvblch941x43zl40000gn/T/ipykernel_43699/1269377478.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0minput_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mintput_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_sent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintput_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_sent = pre_process([sentence])[0]\n",
    "        intput_index = [word_to_ix[word] for word in input_sent]\n",
    "        input_tensor = torch.LongTensor([[ind] for ind in intput_index]).to(device)\n",
    "\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_hiddens = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            # encoder_hiddens[ei] += encoder_hidden[0, 0]\n",
    "            encoder_hiddens[ei] = encoder_hidden[0][0, 0]  # LSTM\n",
    "\n",
    "        decoder_input = torch.tensor([[word_to_ix[\"<BOS>\"]]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_hiddens\n",
    "            )\n",
    "            topv, topi = decoder_output.data.topk(\n",
    "                1\n",
    "            )  # simply adopt the predicted tag with the highest probabiity\n",
    "            if (\n",
    "                topi.item() == word_to_ix[\"<EOS>\"]\n",
    "            ):  # if <EOS> is generated, stop the generation\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(\n",
    "                    word_list[topi.item()]\n",
    "                )  # get the predicted word based on the index\n",
    "            # use the predicted output as the input for the next time step generation\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
